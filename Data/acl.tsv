context	label	split
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( <CITED HERE> ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	train
This was done by MERT optimization ( <CITED HERE> ) towards post-edits under the TER target metric .	Uses	train
She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( <CITED HERE> ) .	Background	train
The following four components have been identified as the key elements of a question related to patient care ( <CITED HERE> ) :	Background	train
<CITED HERE> report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .	CompareOrContrast	train
This is roughly an 11 % relative reduction in error rate over <CITED HERE> and Bods PCFG-reduction reported in Table 1 .	CompareOrContrast	train
Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; <CITED HERE> ; Morency et al. , 2007 ; Morency et al. , 2009 ) .	Background	train
We use the agreement checker code developed by <CITED HERE> and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .	Uses	train
The diagnoser , based on <CITED HERE>b ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .	Extends	train
The formalization of DLRs provided by <CITED HERE> defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .	Background	train
Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( <CITED HERE> ) for our experiments , using the naive Bayes algorithm as our classifier .	Uses	train
Similar to ( <CITED HERE>a ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .	CompareOrContrast	train
The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of <CITED HERE> and Collins ( 1997 ) .	Background	train
transition-based dependency parsing framework ( <CITED HERE> ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .	Uses	train
But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( <CITED HERE> ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .	Motivation	train
each relevant document is retrieved ( <CITED HERE> ) .	Uses	train
<CITED HERE> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .	Background	train
This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( <CITED HERE> ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .	Background	train
The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( <CITED HERE> ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .	Background	train
We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( <CITED HERE> ) .	Background	train
<CITED HERE> showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ .	Background	train
Tateisi et al. also translated LTAG into HPSG ( <CITED HERE> ) .	CompareOrContrast	train
The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq <CITED HERE> , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and	Uses	train
The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( <CITED HERE> ) .	Uses	train
There has also been work focused upon determining the political leaning ( e.g. , `` liberal '' vs. `` conservative '' ) of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified ( the `` unlabeled '' texts ) ( <CITED HERE> ; Efron , 2004 ; Mullen and Malouf , 2006 ) .	Background	train
A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , <CITED HERE> , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .	CompareOrContrast	train
Secondly , the cooperative principle of <CITED HERE> , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .	Motivation	train
In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ <CITED HERE> ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	Background	train
In this paper , inspired by KNN-SVM ( <CITED HERE> ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .	Motivation	train
1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; <CITED HERE> , for example ) .	CompareOrContrast	train
<CITED HERE> report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .	Background	train
Thus for instance , ( <CITED HERE> ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .	Background	train
While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , <CITED HERE> questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .	Background	train
We are going to make such a comparison with the theories proposed by J. <CITED HERE> , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .	CompareOrContrast	train
11 From ( <CITED HERE> ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .	Motivation	train
In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and <CITED HERE> describe further research in Cambridge utilising different types of information available in LDOCE .	Background	train
<CITED HERE> observed that some annotators were not familiar with the exact definition of semantic relatedness .	Motivation	train
All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( <CITED HERE> ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .	Background	train
Although originally developed as a tool to assist in query formulation , <CITED HERE> pointed out that PICO frames can be employed to structure IR results for improving precision .	Background	train
There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( <CITED HERE> ; Krovetz , 1993 ; Hull , 1996 ) .	Background	train
in history-based models ( <CITED HERE> ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .	Uses	train
The reader is referred to <CITED HERE> for a more detailed discussion of our use of constraint propagation .32 We illustrate the result of constraint propagation with our example grammar .	Background	train
Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( <CITED HERE> ) .	Background	train
Such technologies require significant human input , and are difficult to create and maintain ( <CITED HERE> ) .	Background	train
We can define PCAT using a probabilistic grammar ( <CITED HERE> ) .	Background	train
It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( <CITED HERE> ; Xu et al. , 2002 ) .	Background	train
The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim <CITED HERE> ) .	Uses	train
There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; <CITED HERE> ) , and ones on programming/grammar-development environ -	Background	train
For example , <CITED HERE> experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .	Background	train
The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( <CITED HERE> ) .	Uses	train
Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by <CITED HERE>b ) and shingling techniques described by Chakrabarti ( 2002 ) .	Future	train
Following the work of <CITED HERE> , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .	Uses	train
<CITED HERE> attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .	Background	train
From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; <CITED HERE> ) .	Background	train
Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( <CITED HERE> ; Abney , 1991 ; Greffenstette , 1993 ) .	Background	train
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; <CITED HERE>a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	train
Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; <CITED HERE> ; Miyao and Tsujii 2004 ) .	Motivation	train
They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( <CITED HERE> ) .	Background	train
Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , <CITED HERE> ) .	Background	train
<CITED HERE> , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .	Background	train
Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( <CITED HERE> ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .	Uses	train
Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( <CITED HERE> ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .	Motivation	train
This idea was inspired by <CITED HERE> , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .	Motivation	train
, `` domain circumscription '' ( cfXXX <CITED HERE> ) , and their kin .	CompareOrContrast	train
While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , <CITED HERE> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .	Background	train
A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( <CITED HERE> ) .	CompareOrContrast	train
Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by <CITED HERE> .	CompareOrContrast	train
Such questions are typically answered by designing appropriate priming experiments ( <CITED HERE> ) or other lexical decision tasks .	Background	train
The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( <CITED HERE> ; Harkema , 2000 ; Niyogi , 2001 ) .	CompareOrContrast	train
<CITED HERE> and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .	Background	train
The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; <CITED HERE> ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .	Background	train
The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; <CITED HERE> ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .	Background	train
Viewed in this way , gradable adjectives are an extreme example of the `` efficiency of language '' ( <CITED HERE> ) : Far from meaning something concrete like `` larger than 8 cm '' -- a concept that would have very limited applicability -- or even something more general like `` larger than the average N , '' a word like large is applicable across a wide range of different situations .	CompareOrContrast	train
However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; <CITED HERE> ; Ratnaparkhi , 1997 ) .	Background	train
It is defined on different kinds of textual units , e.g. documents , parts of a document ( e.g. words and their surrounding context ) , words or concepts ( <CITED HERE> ) .2 Linguistic distance between words is inverse to their semantic similarity or relatedness .	Background	train
However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data <CITED HERE> ) .	Background	train
For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( <CITED HERE> ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .	Background	train
<CITED HERE> introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :	Background	train
Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , <CITED HERE> , and Young ( 1989 ) .	Background	train
For compound splitting , we follow <CITED HERE> , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .	Uses	train
<CITED HERE> showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .	Background	train
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; <CITED HERE> ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	Background	train
We first identified the most informative unigrams and bigrams using the information gain measure ( <CITED HERE> ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .	Uses	train
Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; <CITED HERE> ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .	Background	train
It compares favorably to other stemming or root extraction algorithms ( <CITED HERE> ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .	Motivation	train
We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( <CITED HERE> ) and ( Chelba and Jelinek , 1998 ) .	Future	train
Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , <CITED HERE> ) .	Background	train
Many investigators ( e.g. <CITED HERE> ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .	Motivation	train
The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; <CITED HERE> ) .	CompareOrContrast	train
See <CITED HERE> for further discussion .	Background	train
We gather similar words using <CITED HERE>a ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .	Uses	train
Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in <CITED HERE> .	Background	train
ones , DIRT ( <CITED HERE> ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .	Background	train
There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of <CITED HERE> .	CompareOrContrast	train
The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( <CITED HERE> ) generation system to produce the appropriate text .	Uses	train
However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see <CITED HERE> for further discussion ) .	Background	train
Our results also confirm the insights gained by <CITED HERE> , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .	CompareOrContrast	train
We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( <CITED HERE> ) .	Uses	train
It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and <CITED HERE> ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .	Motivation	train
In a similar vein , <CITED HERE> showed that a different feature-topic model improved predictions on a fill-in-the-blank task .	Background	train
As ( <CITED HERE> ) show , lexical information improves on NP and VP chunking as well .	Future	train
But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to <CITED HERE> .	Background	train
In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( <CITED HERE> ) .	Background	train
Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( <CITED HERE>c ) .	Background	train
The work of <CITED HERE> and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .	Background	train
TNT refers to the HPSG parser ( <CITED HERE> ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .	CompareOrContrast	train
1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( <CITED HERE> , 203 ) .	Background	train
Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and <CITED HERE> .	Background	train
Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; <CITED HERE> ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .	CompareOrContrast	train
The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; <CITED HERE> ) .	CompareOrContrast	train
Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( <CITED HERE> ) , and Machine Translation ( Boas 2002 ) .	Background	train
Most approaches rely on VerbNet ( <CITED HERE> ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .	Background	train
In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( <CITED HERE> ) .	Background	train
Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; <CITED HERE> ) .7 As an example , consider the translation into French of the house collapsed .	Background	train
This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( <CITED HERE> , 31 ) .	CompareOrContrast	train
27 <CITED HERE> argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .	Background	train
Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( <CITED HERE> ) .	Future	train
These automatic transformations are based on linguistic rules ( <CITED HERE> ) .	Uses	train
â¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; <CITED HERE> ) .	CompareOrContrast	train
In comparison , the tag set of the Buckwalter Morphological Analyzer ( <CITED HERE> ) used in the PATB has a core POS set of 44 tags ( CORE44 ) before morphological extension .8 Cross-linguistically , a core set containing around 12 tags is often	CompareOrContrast	train
â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â¢ a `` conditioning '' facility as described by <CITED HERE> , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .	Future	train
<CITED HERE> present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .	Background	train
mers ( <CITED HERE> ; Porter , 1980 ) demonstrably improve retrieval performance .	Background	train
Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , <CITED HERE> ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that	Background	train
A formula for the test set perplexity ( <CITED HERE> ) is :13	Background	train
This method follows a traditional Information Retrieval paradigm ( <CITED HERE> ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .	Uses	train
Table 1 gives the interpretations of eight adjective-noun combinations discussed in <CITED HERE> and Vendler ( 1968 ) .	Uses	train
The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( <CITED HERE> ) .	CompareOrContrast	train
<CITED HERE> , by comparison , employ 163 distinct predefined frames .	Background	train
Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; <CITED HERE> ) .	Future	train
fÎ¸ on demand ( <CITED HERE> ) can pay off here , since only part of fÎ¸ may be needed subsequently . )	Background	train
An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in <CITED HERE> but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .	Background	train
This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in <CITED HERE> .	CompareOrContrast	train
Finally , we experiment with a method for combining phrase tables proposed in ( <CITED HERE> ; Nakov and Ng , 2012 ) .	Uses	train
Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( <CITED HERE> ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .	CompareOrContrast	train
For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; <CITED HERE> ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .	Uses	train
These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by <CITED HERE> , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .	CompareOrContrast	train
This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; <CITED HERE> ) .	Background	train
The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( <CITED HERE>a ) .	Motivation	train
Unlike our approach , those of <CITED HERE> and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .	CompareOrContrast	train
2The algorithm was implemented by the the authors , following the description in <CITED HERE> .	Uses	train
Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and <CITED HERE> , which deal with automatic generation of classic fill in the blank questions .	Motivation	train
In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( <CITED HERE> ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .	Background	train
Many researchers use the GIZA + + software package ( <CITED HERE> ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .	Background	train
Some works abstract perception via the usage of symbolic logic representations ( <CITED HERE> ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .	Background	train
Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; <CITED HERE> ; Brown et al. , 1993a ) .	Background	train
One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , <CITED HERE> ; Malouf 2000 ) .	Background	train
Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( <CITED HERE> ) .	Background	train
Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; <CITED HERE> ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .	Extends	train
For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in <CITED HERE> .	Uses	train
Compared to the reranking technique in <CITED HERE> , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .	CompareOrContrast	train
The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( <CITED HERE> ) .	CompareOrContrast	train
There has also been work focused upon determining the political leaning ( e.g. , `` liberal '' vs. `` conservative '' ) of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified ( the `` unlabeled '' texts ) ( Laver et al. , 2003 ; <CITED HERE> ; Mullen and Malouf , 2006 ) .	Background	train
It projects a functional head , voice ( <CITED HERE> ) , whose specifier is the external argument .	Background	train
Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( <CITED HERE> ) .	Background	train
This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( <CITED HERE> ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .	CompareOrContrast	train
3 The degree of precision of the measurement ( <CITED HERE> , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .	Background	train
There have already been several attempts to develop distributed NLP systems for dialogue systems ( <CITED HERE> ) and speech recognition ( Hacioglu and Pellom , 2003 ) .	Background	train
Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including `` crummy '' MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( <CITED HERE> ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( Catizone et al. , 1993 ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby .	Background	train
SWIZZLE is a multilingual enhancement of COCKTAIL ( <CITED HERE> ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information ' .	Extends	train
( Davis and Ogden , 1997 ; <CITED HERE> ; Hull and ( 3refenstette , 1996 ) .	CompareOrContrast	train
de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( <CITED HERE> ) or the Complement Cliticization Lexical Rule ( Miller and Sag 1993 ) to operate on those raised elements .	Background	train
<CITED HERE> attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .	Background	train
The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; <CITED HERE> ; Niyogi , 2001 ) .	CompareOrContrast	train
Each component will return a confidence measure of the reliability of its prediction , c.f. ( <CITED HERE> ) .	Motivation	train
In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( <CITED HERE> ) .	Background	train
The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , <CITED HERE> , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .	CompareOrContrast	train
In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; <CITED HERE> ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .	Background	train
This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( <CITED HERE> ) .	Background	train
Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and <CITED HERE> .	Background	train
<CITED HERE> describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .	Uses	train
For better comparison with work of others , we adopt the suggestion made by <CITED HERE> to evaluate the parsing quality on sentences up to 70 tokens long .	Uses	train
Some examples include text categorization ( <CITED HERE> ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .	Background	train
Differently , <CITED HERE> designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .	Motivation	train
They use a Bag of Visual Words ( BoVW ) model ( <CITED HERE> ) to create a bimodal vocabulary describing documents .	Background	train
W. <CITED HERE> discussed sentences of the form * This is a chair but you can sit on it .	Background	train
This imbalance foils thresholding strategies , clever as they might be ( <CITED HERE> ; Wu & Xia , 1994 ; Chen , 1996 ) .	Background	train
The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , <CITED HERE> , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .	CompareOrContrast	train
Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( <CITED HERE> ; Resnik and Elkiss , 2003 ) .	Background	train
In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; <CITED HERE> ) .	Background	train
More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( <CITED HERE> ) , act as excellent substitutes for feature	Background	train
In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( <CITED HERE> ) or the system ( Gomez , 1998 ) .	Background	train
Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; <CITED HERE> ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .	Background	train
The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( <CITED HERE> ) .	Uses	train
We use the non-projective k-best MST algorithm to generate k-best lists ( <CITED HERE> ) , where k = 8 for the experiments in this paper .	Uses	train
As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; <CITED HERE> ) and the CATiB ( Habash and Roth 2009 ) .	Background	train
For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also <CITED HERE> ) .	Background	train
â¢ Graph transformations for recovering nonprojective structures ( <CITED HERE> ) .	Uses	train
<CITED HERE> 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) .	Background	train
This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( <CITED HERE> ; Koo et al. , 2008 ; Miller et al. , 2004 ) .	Motivation	train
Opposition ( called `` adversative '' or `` contrary-to-expectation '' by Halliday and Hasan 1976 ; cfXXX also <CITED HERE> , p. 672 ) .	Background	train
Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( <CITED HERE> ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .	Background	train
1 The representation in <CITED HERE> is even more compact than ours for grammars that are not self-embedding .	CompareOrContrast	train
29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( <CITED HERE> ) .	CompareOrContrast	train
The reordering models we describe follow our previous work using function word models for translation ( <CITED HERE> ; Setiawan et al. , 2009 ) .	Extends	train
Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , <CITED HERE> , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .	CompareOrContrast	train
For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( <CITED HERE> ) .	Background	train
In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( <CITED HERE> ) .	Background	train
Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( <CITED HERE> ) .	Uses	train
See ( <CITED HERE> ) for a discussion .	Background	train
Figure 2 ( a ) shows the frame-based semantic representation for the utterance `` What time is Analyze This playing 2 See ( <CITED HERE> ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .	Background	train
For Berkeley system , we use the reported results from <CITED HERE> .	CompareOrContrast	train
Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( <CITED HERE> ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .	Uses	train
Such a component would serve as the first stage of a clinical question answering system ( <CITED HERE> ) or summarization system ( McKeown et al. , 2003 ) .	Future	train
In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( <CITED HERE> ; Hughes et al , 2004 ) .	Background	train
<CITED HERE> comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .	CompareOrContrast	train
To prove that our method is effective , we also make a comparison between the performances of our system and <CITED HERE> , Xue ( 2008 ) .	CompareOrContrast	train
The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( <CITED HERE> ) .	CompareOrContrast	train
32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; <CITED HERE> ) can be used to circumvent constraint propagation .	Background	train
<CITED HERE> , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .	CompareOrContrast	train
Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( <CITED HERE> ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .	Background	train
In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( <CITED HERE> ) .	CompareOrContrast	train
Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and <CITED HERE> .	Background	train
Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; <CITED HERE>c ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .	CompareOrContrast	train
As already mentioned in the literature , see for example ( <CITED HERE> ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .	Background	train
We use the Columbia Arabic Treebank ( CATiB ) ( <CITED HERE> ) .	Uses	train
This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; <CITED HERE> , p. 546 ) gives 10 definitions of a sentence .	Background	train
Another technique for making better use of unlabeled data is cotraining ( <CITED HERE> ) , in which two sufficiently different learners help each other learn by labeling training data for one another .	Background	train
For example , modeling CASE in Czech improves Czech parsing ( <CITED HERE> ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .	Motivation	train
Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including `` crummy '' MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( Catizone et al. , 1993 ; <CITED HERE> ) , computerassisted language learning , corpus linguistics ( Melby .	Background	train
Following our previous work ( <CITED HERE> ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .	Extends	train
There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; <CITED HERE> ) .	Background	train
This situation suggests a response-automation approach that follows the document retrieval paradigm ( <CITED HERE> ) , where a new request is matched with existing response documents ( e-mails ) .	Background	train
All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and <CITED HERE> ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .	Background	train
This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( <CITED HERE> ) .	CompareOrContrast	train
We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( <CITED HERE>b ) , but these features are also beyond the capabilities of current summarization systems .	CompareOrContrast	train
<CITED HERE>:472 ) , but these are the only ones which are explicit in the LDOCE coding system .	Background	train
This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( <CITED HERE> ) by a method of grammar conversion .	Background	train
These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ <CITED HERE> ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .	Background	train
Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; <CITED HERE> ) .	Background	train
Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including `` crummy '' MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; <CITED HERE>b ) ) , concordancing for bilingual lexicography ( Catizone et al. , 1993 ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby .	Background	train
An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , <CITED HERE> ) .	Background	train
Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; <CITED HERE> ; Roller et al. , 2012 ) .	Background	train
The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by <CITED HERE> , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .	CompareOrContrast	train
<CITED HERE> predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .	Background	train
Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( <CITED HERE> ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .	Background	train
<CITED HERE> give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .	Uses	train
FBLTAG ( Vijay-Shanker , 1987 ; <CITED HERE> ) is an extension of the LTAG formalism .	Background	train
There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; <CITED HERE> ) .	CompareOrContrast	train
12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in <CITED HERE> .	Background	train
Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; <CITED HERE> ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .	Motivation	train
This Principle of Finitism is also assumed by <CITED HERE> , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .	CompareOrContrast	train
We performed translation experiments with an implementation of the IBM-4 translation model ( <CITED HERE> ) .	Uses	train
In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( <CITED HERE> ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .	Background	train
For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( <CITED HERE> ) .	Background	train
For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; <CITED HERE> ) -- one of the most accurate full parsers around .	Uses	train
More sophisticated approaches have been proposed ( <CITED HERE> ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .	Background	train
<CITED HERE> have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .	Background	train
The one-sided t-test ( <CITED HERE> ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .	Uses	train
mers ( Lovins , 1968 ; <CITED HERE> ) demonstrably improve retrieval performance .	Background	train
It is therefore no surprise that early attempts at response automation were knowledge-driven ( <CITED HERE> ; Watson 1997 ; Delic and Lahaix 1998 ) .	Background	train
The framework was originally developed for the realization of deep-syntactic structures in NLG ( <CITED HERE> ) .	Background	train
These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( <CITED HERE> ) .	Background	train
Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; <CITED HERE> ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .	Background	train
5 The open source Moses ( <CITED HERE> ) toolkit from www.statmt.org/moses/ .	Uses	train
There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( <CITED HERE> ; Chu-Carroll , 1999 ) .	Future	train
The maximum entropy approach ( <CITED HERE> ) presents a powerful framework for the combination of several knowledge sources .	Uses	train
Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) <CITED HERE> , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .	Uses	train
We then use the program Snob ( Wallace and Boulton 1968 ; <CITED HERE> ) to cluster these experiences .	Uses	train
However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; <CITED HERE> ) .	Background	train
Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in <CITED HERE> .	Uses	train
For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( <CITED HERE> ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .	Uses	train
In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; <CITED HERE> ) .	Motivation	train
All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( <CITED HERE> ) .	Uses	train
Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by <CITED HERE> .	Motivation	train
The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in <CITED HERE> .	CompareOrContrast	train
More recently , an alignment selection approach was proposed in ( <CITED HERE> ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .	CompareOrContrast	train
<CITED HERE> for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .	Background	train
A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , <CITED HERE> , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .	Background	train
The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; <CITED HERE> ) ) .	CompareOrContrast	train
7A11 our results are computed with the evalb program following the now-standard criteria in ( <CITED HERE> ) .	Uses	train
We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ <CITED HERE> ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .	CompareOrContrast	train
results are based on a corpus of movie subtitles ( <CITED HERE> ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .	Uses	train
The system is implemented based on ( Galley et al. , 2006 ) and ( <CITED HERE> ) .	Uses	train
Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( <CITED HERE> ; Teufel and Moens , 2002 ) .	Background	train
For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( <CITED HERE> ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .	Future	train
Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; <CITED HERE> ] ) as well as for	Background	train
Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while <CITED HERE> examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .	Background	train
We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( <CITED HERE> ) and Penn Chinese Treebank ( Xue , Chiou , and Palmer 2002 ) , extracting wide-coverage , probabilistic LFG grammar	Uses	train
We use the structures previously used by <CITED HERE> , and propose one new structure .	Uses	train
Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; <CITED HERE> ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .	CompareOrContrast	train
A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; <CITED HERE> ; Mitkov 1996 , 1998b ) .	Background	train
Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in <CITED HERE> to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .	Background	train
We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( <CITED HERE> ) .	Uses	train
In this paper , I present a computational implementation of Distributed Morphology ( <CITED HERE> ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .	Uses	train
Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; <CITED HERE> ) .	CompareOrContrast	train
Subsequent processing by the natural language and response generation components was done automatically by the computer ( <CITED HERE> ) .	Uses	train
This is implemented as a cascade of simple strategies , which were briefly described in <CITED HERE> .	Uses	train
But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of <CITED HERE> , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora .	CompareOrContrast	train
Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; <CITED HERE> ) for self training .	Future	train
Intermedia is no more developed and nobody of us had the opportunity to try it ( <CITED HERE> ) .	Background	train
Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; <CITED HERE> ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .	Background	train
ment ( <CITED HERE> ; Doran et al. , 2000 ; Makino et al. , 1998 ) .	Background	train
Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( <CITED HERE> ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .	Background	train
Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( <CITED HERE> ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	Background	train
We also made use of the person-name/instance pairs automatically extracted by <CITED HERE> .2 This data provides counts for pairs such as `` Edwin Moses , hurdler '' and `` William Farley , industrialist . ''	Uses	train
<CITED HERE> presented an approach for constructing a BKB based on the S-SSTC .	Background	train
However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( <CITED HERE>b ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .	Background	train
Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( <CITED HERE> ) .	Background	train
<CITED HERE> furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .	Extends	train
It can be shown ( <CITED HERE> ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .	Motivation	train
Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in <CITED HERE> . ''	Background	train
18 In this article , we use a newer version of the corpus by <CITED HERE> than the one we used in Marton , Habash , and Rambow ( 2011 ) .	Uses	train
<CITED HERE>b ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .	Background	train
Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( <CITED HERE> ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .	Uses	train
The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( <CITED HERE> ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .	Background	train
This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; <CITED HERE> ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .	Background	train
Following the example of <CITED HERE> , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) .	Uses	train
( <CITED HERE> ) .	Background	train
We use the same set of binary features as in previous work on this dataset ( <CITED HERE> ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .	Uses	train
Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( <CITED HERE> ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .	Background	train
Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; <CITED HERE> ) .	Background	train
<CITED HERE> observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .	CompareOrContrast	train
<CITED HERE>a ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat .	Background	train
â¢ Learnability ( <CITED HERE> ) â¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )	Background	train
For example , a ` web page ' is more similar to an infinite canvas than a written page ( <CITED HERE> ) .	Background	train
Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) <CITED HERE> , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .	Uses	train
2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( <CITED HERE> ) .	Background	train
This approach resembles the work by Grishman et al. ( 1986 ) and <CITED HERE> on selectional restrictions .	CompareOrContrast	train
This is noticeable for German ( <CITED HERE> ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .	CompareOrContrast	train
LTAG ( <CITED HERE> ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera -	Background	train
Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( <CITED HERE> ) .	Uses	train
This work is a continuation of that initiated in ( <CITED HERE> ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .	Extends	train
Our previous work ( <CITED HERE> ) designed an EMbased method to construct unsupervised trees for tree-based translation models .	CompareOrContrast	train
Then , we binarize the English parse trees using the head binarization approach ( <CITED HERE> ) and use the resulting binary parse trees to build another s2t system .	Uses	train
This is the strongest version of the sorites paradox ( e.g. , <CITED HERE> ) .	Background	train
We used a standard implementation of IBM Model 4 ( <CITED HERE> ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .	Uses	train
Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( <CITED HERE> ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .	Background	train
Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( <CITED HERE> ) .	Background	train
For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; <CITED HERE> ) and history-based parsing ( Nivre and McDonald , 2008 ) .	Future	train
Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( <CITED HERE>a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .	Background	train
Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase <CITED HERE> , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .	Uses	train
Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( <CITED HERE> ) must play a role , too .	Background	train
Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( <CITED HERE> ) .	Background	train
Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; <CITED HERE> ) .	Background	train
Also , the <CITED HERE> approach will be undefined if the pair is unobserved on the web .	Uses	train
The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( <CITED HERE> ) and case-based reasoning ( Watson 1997 ) .	CompareOrContrast	train
We measure this association using pointwise Mutual Information ( MI ) ( <CITED HERE> ) .	Uses	train
Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( <CITED HERE> ) .	Background	train
There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( <CITED HERE> ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .	Uses	train
or quotation of messages in emails or postings ( see Mullen and Malouf ( 2006 ) but cfXXX <CITED HERE> ) .	Background	train
<CITED HERE> extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .	Background	train
SNoW ( <CITED HERE> ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .	Uses	train
The search algorithm is the standard Viterbi search ( <CITED HERE> ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .	Uses	train
Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( <CITED HERE> ) .	Background	train
7 We employed the LIBSVM package ( <CITED HERE> ) .	Uses	train
coreference performance on perfect mentions ( e.g. , Incorporate the two knowledge sources in a <CITED HERE> ) ; and for those that do report percoreference resolver .	CompareOrContrast	train
According to <CITED HERE> , p. 67 ) , these two sentences are incoherent .	CompareOrContrast	train
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( <CITED HERE> ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	train
Another dialogue acquisition system has been developed by <CITED HERE> .	CompareOrContrast	train
Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; <CITED HERE> ) is a member of the family of constraint-based grammars .	Background	train
1Our rules are similar to those from <CITED HERE> .	CompareOrContrast	train
In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im <CITED HERE> ) .	Background	train
In this article , we use an in-house system which provides functional gender , number , and rationality features ( <CITED HERE> ) .	Uses	train
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( <CITED HERE> ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	Background	train
The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; <CITED HERE> , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .	Background	train
Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see <CITED HERE> for an overview ) .	Background	train
Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( <CITED HERE> ; He et al. , 2010 ; Ma et al. , 2011 ) .	CompareOrContrast	train
Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( <CITED HERE> ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .	CompareOrContrast	train
A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and <CITED HERE> ) .	CompareOrContrast	train
As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( <CITED HERE> ) .	Background	train
Other representations use the link structure ( <CITED HERE> ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .	Background	train
From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; <CITED HERE> ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .	Background	train
There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; <CITED HERE> ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .	Background	train
This revalidates the observation of <CITED HERE> that phrase structure representations and dependency representations add complimentary value to the learning task .	CompareOrContrast	train
The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by <CITED HERE> .	Uses	train
The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( <CITED HERE> ) .	Uses	train
Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( <CITED HERE> ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) .	Uses	train
6 The Partial-VP Topicalization Lexical Rule proposed by <CITED HERE> , 10 ) is a linguistic example .	Background	train
We measure the inter annotator agreement using the Fleiss Kappa ( <CITED HERE> ) measure ( x ) where the agreement lies around 0.79 .	Uses	train
The task we used to compare different generalisation techniques is similar to that used by <CITED HERE> and Rooth et al. ( 1999 ) .	CompareOrContrast	train
Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; <CITED HERE> ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .	Background	train
It is not aimed at handling dependencies , which require heavy use of lexical information ( <CITED HERE> , for PP attachment ) .	CompareOrContrast	train
This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( <CITED HERE> ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .	Background	train
I A more detailed discussion of various aspects of the proposed parser can be found in ( <CITED HERE> ) .	Background	train
Our experimental design with professional bilingual translators follows our previous work <CITED HERE>a ) comparing scratch translation to post-edit .	Extends	train
Lexical functional grammar ( Kaplan and Bresnan 1982 ; <CITED HERE> ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .	Background	train
Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( <CITED HERE> ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .	Background	train
Previously , a user study ( <CITED HERE> ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .	CompareOrContrast	train
We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , <CITED HERE> ) .	Background	train
In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; <CITED HERE> ) .	Uses	train
The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( <CITED HERE> ) .	CompareOrContrast	train
Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( <CITED HERE> ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .	Background	train
The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( <CITED HERE> ) , and images are then quantized over the 5,000 codewords .	Uses	train
Another approach for partial parsing was presented by <CITED HERE> .	Background	train
In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , <CITED HERE> ) .	Background	train
The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , <CITED HERE> , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .	Background	train
This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; <CITED HERE> ) .	CompareOrContrast	train
Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( <CITED HERE> ) .	Background	train
The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( <CITED HERE> ) .	Motivation	train
Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; <CITED HERE> ) .	Background	train
based parsing algorithms with an arc-factored parameterization ( <CITED HERE> ) .	Uses	train
No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and <CITED HERE> ) .	Background	train
In this paper we focus on the exploitation of the LDOCE grammar coding system ; <CITED HERE> and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .	Background	train
Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( <CITED HERE> ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .	Background	train
We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ <CITED HERE>a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .	CompareOrContrast	train
Similarly , ( Barzilay and Lee , 2003 ) and ( <CITED HERE> ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .	Background	train
In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( <CITED HERE> ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .	Future	train
Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( <CITED HERE> ) ( Section 6 ) .	Uses	train
For instance , relating `` they '' to `` apples '' in the sentence ( cfXXX <CITED HERE> p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap	Background	train
We follow our previous work ( <CITED HERE> ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .	Extends	train
Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see <CITED HERE> and the references therein ) .	Background	train
According to <CITED HERE> , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .	Background	train
But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( <CITED HERE> ) .	Motivation	train
4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; <CITED HERE> ) .	Background	train
Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; <CITED HERE> ) .	CompareOrContrast	train
There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( <CITED HERE> ) .	Background	train
For instance , implementing an efficient version of the MXPOST POS tagger ( <CITED HERE> ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .	Future	train
cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , <CITED HERE> ) .	Background	train
<CITED HERE> and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .	Background	train
<CITED HERE> evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .	CompareOrContrast	train
We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; <CITED HERE> ) .	Uses	train
The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( <CITED HERE> ) .	CompareOrContrast	train
<CITED HERE> did very encouraging work on the feature calibration of semantic role labeling .	Background	train
This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( <CITED HERE> , for example ) , does not specify the relationship itself .	Background	train
And ( <CITED HERE> ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .	Background	train
Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( <CITED HERE> ) .	Background	train
There has also been work focused upon determining the political leaning ( e.g. , `` liberal '' vs. `` conservative '' ) of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified ( the `` unlabeled '' texts ) ( Laver et al. , 2003 ; Efron , 2004 ; <CITED HERE> ) .	Background	train
Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , <CITED HERE> , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .	Background	train
To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( <CITED HERE> ) .	CompareOrContrast	train
Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( <CITED HERE> ) .	Extends	train
CCGBank ( <CITED HERE> ) is used to train the model .	Uses	train
More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : <CITED HERE> uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .	Background	train
Some researchers ( Cucerzan , 2007 ; <CITED HERE> ) have explored the use of Wikipedia information to improve the disambiguation process .	Background	train
To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( <CITED HERE> ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .	Uses	train
One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; <CITED HERE> ) .	Background	train
Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( <CITED HERE> ) .	Background	train
McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; <CITED HERE> ) .	Background	train
According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( <CITED HERE> ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .	CompareOrContrast	train
Similar findings have been proposed by <CITED HERE> that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .	Background	train
The first lexical substitution method was proposed by <CITED HERE> .	Background	train
This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( <CITED HERE> ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .	Background	train
We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ <CITED HERE> ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .	CompareOrContrast	train
<CITED HERE> ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .	Background	train
Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( <CITED HERE> ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) .	Background	train
Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( <CITED HERE> ) .	Background	train
Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( <CITED HERE> ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .	Background	train
Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and <CITED HERE> .	Background	train
Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( <CITED HERE> ) .	Background	train
<CITED HERE> investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .	CompareOrContrast	train
This contrasts with one of the traditional approaches ( e.g. , <CITED HERE> ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .	CompareOrContrast	train
In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( <CITED HERE> ) .	Background	train
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; <CITED HERE> ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	train
For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( <CITED HERE> ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .	Background	train
While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; <CITED HERE> ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .	Background	train
It helps them build complex knowledge bases by combining components : events , entities and modifiers ( <CITED HERE> ) .	Background	train
Our group has developed a wide-coverage HPSG grammar for Japanese ( <CITED HERE> ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .	Background	train
Word frequency counts in internet search engines are inconsistent and unreliable ( <CITED HERE> ) .	Background	train
Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( <CITED HERE>a , 2001b ) .	Background	train
It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; <CITED HERE> ) .	Background	train
Actually , if we use LSH technique ( <CITED HERE> ) in retrieval process , the local method can be easily scaled to a larger training data .	Future	train
Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( <CITED HERE> ) .	Background	train
Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( <CITED HERE> ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .	Background	train
Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( <CITED HERE> ) 2 .	Uses	train
Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( <CITED HERE> ) extracts relations between the concepts .	Background	train
The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; <CITED HERE> ) .	Motivation	train
Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; <CITED HERE> ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .	Background	train
These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the <CITED HERE> Arabic data .	Uses	train
The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; <CITED HERE> ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .	Background	train
But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and <CITED HERE> , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .	Background	train
The implementation has been inspired by experience in extracting information from very large corpora ( <CITED HERE> ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .	Motivation	train
Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; <CITED HERE> ) .	Background	train
14We parse each sentence with the Collins parser ( <CITED HERE> ) .	Uses	train
Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( <CITED HERE> ) , we tried to adapt the same approach to the German-English language pair .	Motivation	train
We followed the same experimental procedure as discussed in ( <CITED HERE> ) for English polymorphemic words .	Uses	train
In our experiments , we employed the well-known classifier SVM `` ght to obtain individual-document classification scores , treating Y as the positive class and using plain unigrams as features .5 Following standard practice in sentiment analysis ( <CITED HERE> ) , the input to SVM `` ght consisted of normalized presence-of-feature ( rather than frequency-of-feature ) vectors .	Uses	train
The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , <CITED HERE> , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .	CompareOrContrast	train
To address this problem , we are currently working on developing a metagrammar in the sense of ( <CITED HERE> ) .	Future	train
Sentences like 12 , from <CITED HERE> , are frequently cited .	Background	train
After the PropBank ( <CITED HERE> ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .	Background	train
In order to estimate the parameters of our model , we develop a blocked sampler based on that of <CITED HERE> to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .	Uses	train
The extraction procedure utilizes a head percolation table as introduced by <CITED HERE> in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct .	Background	train
Many other such cases are described in Danlos 's book ( <CITED HERE> ) .	Background	train
â¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; <CITED HERE> ) , which performs lemmatization , and discovers proper names and technical terms .	Uses	train
Therefore , we repeated the experiments with POS tags predicted by the MADA toolkit ( <CITED HERE> ; Habash , Rambow , and Roth 2012 ) 15 ( see Table 2 , 14 Some parsers predict POS tags internally , instead of receiving them as input , but this is not the case in this article .	Uses	train
Experiments on Chinese SRL ( <CITED HERE> , Xue 2008 ) reassured these findings .	Motivation	train
An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by <CITED HERE> that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .	Background	train
According to <CITED HERE> , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .	Background	train
The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( <CITED HERE> ) .	Motivation	train
A good study comparing document categorization algorithms can be found in ( <CITED HERE> ) .	Background	train
For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( <CITED HERE> ) .	Background	train
We follow <CITED HERE> , for compound merging .	Uses	train
Semantic filters can also be used to prevent multiple versions of the same case frame ( <CITED HERE> ) showing up as complements .	Uses	train
The terms have been identified as the most specific to our corpus by a program developed by <CITED HERE> and called TER1vloSTAT .	Uses	train
For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and <CITED HERE> .	Uses	train
The availability of toolkits for this weighted case ( <CITED HERE> ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .	Background	train
tionally reconstructed by <CITED HERE> and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules .	CompareOrContrast	train
We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; <CITED HERE> ) .	Uses	train
<CITED HERE> improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .	CompareOrContrast	train
But typical OT grammars offer much richer finite-state models of left context ( <CITED HERE>a ) than provided by the traditional HMM finite-state topologies .	Background	train
Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; <CITED HERE> ) .	Background	train
This contrasts with the findings described in <CITED HERE> where significant improvements could be achieved by increasing the number of source languages .	CompareOrContrast	train
The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , <CITED HERE> ) .	Background	train
<CITED HERE> pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .	CompareOrContrast	train
In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( <CITED HERE> ) .	Background	train
The method is called targeted self-training as it is similar in vein to self-training ( <CITED HERE> ) , with the exception that the new parse data is targeted to produce accurate word reorderings .	CompareOrContrast	train
In our previous work ( <CITED HERE> ) , we started an initial investigation on conversation entailment .	Extends	train
<CITED HERE> tried to construct a semantic analysis based on `` prepared '' and `` unprepared mind '' .	Background	train
It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( <CITED HERE> ; Melamed , 1997 ; Lin , 99 ) .	Background	train
This is mainly due to the fact that Arabic is a non-concatenative language ( <CITED HERE> ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .	Background	train
ear regression adapted for classification ( <CITED HERE> ) , which can be described by the following equation :	Uses	train
For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( <CITED HERE> ) trains only stochastic edit distance .	Background	train
Using the tree-cut technique described above , our previous work ( <CITED HERE> ) extracted systematic polysemy from WordNet .	Extends	train
( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; <CITED HERE> ) employed an evaluation metric as a loss function and directly optimized it .	CompareOrContrast	train
5 Significant bigrams are obtained using the n-gram statistics package NSP ( <CITED HERE> ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .	Uses	train
Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , <CITED HERE> and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .	Background	train
2The WePS-1 corpus includes data from the Web03 testbed ( <CITED HERE> ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .	Uses	train
The local training method ( <CITED HERE> ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .	Background	train
Recent work ( Banko and Brill , 2001 ; <CITED HERE> ) has suggested that some tasks will benefit from using significantly more data .	Background	train
Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; <CITED HERE> ; Greffenstette , 1993 ) .	Background	train
The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by <CITED HERE> .	CompareOrContrast	train
The standard approach is to train two models independently and then intersect their predictions ( <CITED HERE> ) .	CompareOrContrast	train
We use the same method as <CITED HERE> for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair .	Uses	train
MI was also recently used for inference-rule SPs by <CITED HERE> .	Background	train
Based on this advise ( Moore and <CITED HERE> ) exclude the latent segmentation variables and opt for a heuristic training procedure .	CompareOrContrast	train
For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( <CITED HERE> ) , currently used for training POS taggers .	Background	train
The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( <CITED HERE> ) .	Background	train
4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( <CITED HERE> ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .	Uses	train
Second , software for utilizing this ontology already exists : MetaMap ( <CITED HERE> ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .	Background	train
Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( <CITED HERE> ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .	CompareOrContrast	train
Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , <CITED HERE> , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .	Background	train
The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( <CITED HERE> ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .	CompareOrContrast	train
The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following <CITED HERE> .	Background	train
This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; <CITED HERE> ; Russell , 1998 ) .	Background	train
The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in <CITED HERE> , or semantic nets as in Winston ( 1975 ) .	CompareOrContrast	train
Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( <CITED HERE> ) .	CompareOrContrast	train
To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( <CITED HERE> and references therein ) .	Uses	train
Other works ( Kasper et al. , 1995 ; <CITED HERE> ) convert HPSG grammars into LTAG grammars .	CompareOrContrast	train
The work of <CITED HERE> demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .	Background	train
Efficient hardware implementation is also possible via chip-level parallelism ( <CITED HERE> ) .	Future	train
2We could just as easily use other symmetric `` association '' measures , such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( <CITED HERE> ) .	CompareOrContrast	train
<CITED HERE> have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available .	Background	train
The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by <CITED HERE> where program flowcharts were constructed from traces of their behaviors .	CompareOrContrast	train
Character classes , such as punctuation , are defined according to the Unicode Standard ( <CITED HERE> ) .	Uses	train
Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; <CITED HERE> ) .	Background	train
Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by <CITED HERE> strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .	Motivation	train
In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( <CITED HERE> ) .	Background	train
The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( <CITED HERE> ) , a deep parse of Si , or some other representation .	Background	train
Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; <CITED HERE> ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .	Background	train
Similarly , ( <CITED HERE> ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .	Background	train
This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in <CITED HERE> .	Background	train
More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; <CITED HERE> use , as a starting point , a number of identical characters .	Background	train
Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( <CITED HERE> ) and find correlations between the various modalities both within and across speakers .	Background	train
Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( <CITED HERE> ) , and word sense disambiguation ( Fujii et al. 1998 ) .	Background	train
It is known that certain cue words and phrases ( <CITED HERE> ) can serve as explicit indicators of discourse structure .	Motivation	train
The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX <CITED HERE> ) .	CompareOrContrast	train
One approach to this problem is that taken by the ASCOT project ( <CITED HERE> ; Akkerman , 1986 ) .	Background	train
Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( <CITED HERE> ) and is simpler to measure .	Motivation	train
For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and <CITED HERE> ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers	Uses	train
We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( <CITED HERE> ) .19 The first part of Table 8 shows that the RAT ( rationality ) feature is very relevant ( in gold ) , but suffers from low accuracy ( no gains in machine-predicted input ) .	Uses	train
In our previous work ( <CITED HERE> ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .	CompareOrContrast	train
It has already been used to implement a framework for teaching NLP ( <CITED HERE> ) .	Extends	train
Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; <CITED HERE> ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .	Background	train
Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , <CITED HERE> , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .	Background	train
Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of <CITED HERE> , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .	CompareOrContrast	train
On the other side , wikis started as collective works where each entry is not owned by a single author e.g. <CITED HERE> .	Background	train
To address this inconsistency in the correspondence between inflectional features and morphemes , and inspired by SmrÅ¾ ( 2007 ) , we distinguish between two types of inflectional features : formbased ( a.k.a. surface , or illusory ) features and functional features .6 Most available Arabic NLP tools and resources model morphology using formbased ( `` surface '' ) inflectional features , and do not mark rationality ; this includes the Penn Arabic Treebank ( PATB ) ( <CITED HERE> ) , the Buckwalter morphological analyzer ( Buckwalter 2004 ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic ( MADA ) toolkit ( Habash and Rambow 2005 ; Habash , Rambow , and Roth 2012 ) .	CompareOrContrast	train
<CITED HERE> has built a semantic role classifier exploiting the interdependence of semantic roles .	Uses	train
Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; <CITED HERE> ) .	Background	train
8 It is based on the dataset of <CITED HERE> ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .	Extends	train
In previous work ( <CITED HERE> ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .	Extends	train
The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) <CITED HERE> .	Background	train
The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by <CITED HERE> .	Uses	train
As shown in <CITED HERE> this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .	Motivation	train
The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; <CITED HERE> ) to see what a formal interpretation of events in time might look like .	Background	train
Proceedings of EACL '99 example , the ALE parser ( <CITED HERE> ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown .	Background	train
Following Soon et al. ( 2001 ) , we represent use the ACE training data for acquiring our SC clasSCA as a binary value that indicates whether the insifier ; instead , we use the BBN Entity Type Corpus duced SCs of the two NPs involved are the same or ( <CITED HERE> ) , which consists of not .	Uses	train
Following <CITED HERE> , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .	Uses	train
<CITED HERE> use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .	Background	train
We do this with a first-order HMM part-ofspeech tagger ( Merialdo <CITED HERE> ) .	Uses	train
Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( <CITED HERE> ) , we can efficiently compute the probability of the sentence , P ( w | G ) .	Uses	train
As shown in ( <CITED HERE> ) â¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .	Background	train
According to <CITED HERE> , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .	Background	train
Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of <CITED HERE> , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .	CompareOrContrast	train
This choice is motivated by an observation we made previously ( <CITED HERE>a ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3 As our sequence learner , we employ a maximum entropy Markov model ( MEMM ) ( McCallum et al. , 2000 ) .	Extends	train
Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( <CITED HERE> ; Brown et al. , 1990 ; Brown et al. , 1993a ) .	Background	train
This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( <CITED HERE> ) .	Background	train
This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( <CITED HERE> ) .	Background	train
Two exceptions to this generalisation are the Linguistic String Project ( <CITED HERE> ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .	CompareOrContrast	train
They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of <CITED HERE> .	CompareOrContrast	train
The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( <CITED HERE> ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .	Background	train
Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; <CITED HERE> ) .	Background	train
To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.<CITED HERE> ) .	Uses	train
Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; <CITED HERE> ) where it has been claimed that words having low surface frequency tends to decompose .	Background	train
These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( <CITED HERE> ) .	Uses	train
â¢ The transition probability a is 0.7 using the EM algorithm ( <CITED HERE> ) on the TREC4 ad-hoc query set .	Uses	train
<CITED HERE> obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .	Background	train
( <CITED HERE> ) has found strong correlations between DF , IG and the X2 statistic for a term .	Background	train
Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; <CITED HERE> ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .	Background	train
To quantify the relative strengths of these transitive inferences , <CITED HERE> propose to assign a weight to each link .	Background	train
While these approaches have been reasonably successful ( see <CITED HERE> ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .	Background	train
Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , <CITED HERE> ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .	Background	train
We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( <CITED HERE> ) .	Uses	train
â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( <CITED HERE> ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )	Background	train
Details of the top performing heuristics of COCKTAIL were reported in ( <CITED HERE> ) .	Background	train
Representative systems are described in <CITED HERE> , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .	Background	train
In addition , <CITED HERE> note that our Object Raising rule would assign mean to this category incorrectly .	CompareOrContrast	train
The parallel corpus is word-aligned using GIZA + + ( <CITED HERE> ) .	Uses	train
tions for the remaining 20 % of the instances ; and ( 3 ) train an SVM classifier ( using the LIBSVM package ( <CITED HERE> ) ) on these 20 % of the instances , where each instance , i , is represented by a set of 31 binary features .	Uses	train
Our algorithm is similar to the approach taken by <CITED HERE> for inducing PCFG parsers .	CompareOrContrast	train
Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; <CITED HERE> ) .	Background	train
This experiment was again replicated by <CITED HERE> with 10 subjects .	Background	train
Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( <CITED HERE> ) .	Background	train
Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , <CITED HERE> ) .	CompareOrContrast	train
This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( <CITED HERE> ) , and many other tasks .	Background	train
( 4 ) NE : We use BBN 's IdentiFinder ( <CITED HERE> ) , a MUC-style NE recognizer to determine the NE type of NPZ .	Uses	train
<CITED HERE> has made the first attempt working on the single semantic role level to make further improvement .	CompareOrContrast	train
In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; <CITED HERE> ; Dave et al. , 2003 ) .	Background	train
raw length value as a feature , we follow our previous work ( <CITED HERE> ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .	Extends	train
<CITED HERE> used unification in an SMT system to model some of the	CompareOrContrast	train
Following Ruch et al. ( 2003 ) and <CITED HERE> , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .	Uses	train
The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; <CITED HERE> ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .	Background	train
Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; <CITED HERE> ; Zhang et al. , 2011b ) .	Background	train
Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; <CITED HERE> ; Chu-Carroll and Brown , 1998 ) .	Background	train
Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( <CITED HERE> ) ) as well as NLP tools and resources that can be manipulated from the GUI .	Background	train
A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , <CITED HERE> , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .	CompareOrContrast	train
It allows the construction of a non-TAL ( Shieber , 1994 ) , ( <CITED HERE> ) .	Background	train
In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; <CITED HERE> ) .	Background	train
In particular , the `` Semantic Information Retrieval '' project ( SIR <CITED HERE> ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .	Motivation	train
Our re-ranking approach , like the approach to parse re-ranking of <CITED HERE> , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .	CompareOrContrast	train
For instance , the Alembic workbench ( <CITED HERE> ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .	Background	train
In contrast , a single statistical model allows one to maintain a single table ( <CITED HERE> ) .	Background	train
Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by <CITED HERE> .	Uses	train
First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; <CITED HERE> ) .	Background	train
<CITED HERE> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .	CompareOrContrast	train
In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( <CITED HERE> : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .	Background	train
In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see <CITED HERE> for details ) .	Background	train
We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( <CITED HERE> ) .	Uses	train
Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( <CITED HERE> ) ) and parsers generally aim to produce a tree spanning each sentence .	Background	train
The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( <CITED HERE> ) .29 The unfolding transformation is also referred to as partial execution , for example , by Pereira and Shieber ( 1987 ) .	Uses	train
Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; <CITED HERE> ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .	Motivation	train
This seems to provide additional evidence of <CITED HERE>b ) 's suggestion that something like a distributional hypothesis of images is plausible .	CompareOrContrast	train
For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( <CITED HERE> ) .	Background	train
Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see <CITED HERE> on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .	Background	train
Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim <CITED HERE> ) to compare it to other shallow parsers .	Uses	train
Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( <CITED HERE> ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .	Extends	train
Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( <CITED HERE> ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .	Background	train
We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( <CITED HERE> ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .	Uses	train
It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( <CITED HERE> ; Artiles et al. , 2007 ) .	Background	train
This deficiency is rectified in the verb classification system employed by <CITED HERE> in the Brandeis verb catalogue .	CompareOrContrast	train
Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( <CITED HERE> ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .	Background	train
1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( <CITED HERE> ) or , more frequently , on a combination of the two ( Smadja , 1993 ; Kilgarriff and Tugwell , 2001 , for example ) .	CompareOrContrast	train
It is frequently used in tasks like scene identification , and <CITED HERE> shows that distance in GIST space correlates well with semantic distance in WordNet .	Motivation	train
Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow <CITED HERE> and decompose the prior probability P0 ( r | N ) into two factors as follows :	Uses	train
To address this inconsistency in the correspondence between inflectional features and morphemes , and inspired by SmrÅ¾ ( 2007 ) , we distinguish between two types of inflectional features : formbased ( a.k.a. surface , or illusory ) features and functional features .6 Most available Arabic NLP tools and resources model morphology using formbased ( `` surface '' ) inflectional features , and do not mark rationality ; this includes the Penn Arabic Treebank ( PATB ) ( Maamouri et al. 2004 ) , the Buckwalter morphological analyzer ( Buckwalter 2004 ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic ( MADA ) toolkit ( <CITED HERE> ; Habash , Rambow , and Roth 2012 ) .	CompareOrContrast	train
In our prior work ( <CITED HERE> ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .	Extends	train
Previous sentiment-analysis work in different domains has considered inter-document similarity ( <CITED HERE> ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit	Background	train
<CITED HERE>a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .	Background	train
The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( <CITED HERE> ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .	Uses	train
The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( <CITED HERE> ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .	Background	train
Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( <CITED HERE> ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 .	Uses	train
On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in <CITED HERE> ( 0.44 % vs. 0.5 % error rate ) .	CompareOrContrast	train
<CITED HERE> studied the issue of disambiguation for mono-lingual M.	Background	train
Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; <CITED HERE> ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .	Background	train
To address this inconsistency in the correspondence between inflectional features and morphemes , and inspired by SmrÅ¾ ( 2007 ) , we distinguish between two types of inflectional features : formbased ( a.k.a. surface , or illusory ) features and functional features .6 Most available Arabic NLP tools and resources model morphology using formbased ( `` surface '' ) inflectional features , and do not mark rationality ; this includes the Penn Arabic Treebank ( PATB ) ( Maamouri et al. 2004 ) , the Buckwalter morphological analyzer ( <CITED HERE> ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic ( MADA ) toolkit ( Habash and Rambow 2005 ; Habash , Rambow , and Roth 2012 ) .	CompareOrContrast	train
Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; <CITED HERE> ; Patel-Schneider 1985 ) .	CompareOrContrast	train
Riehemann 1993 ; Oliva 1994 ; Frank 1994 ; <CITED HERE> ; Sanfilippo 1995 ) .	CompareOrContrast	train
Berger et al. 2000 ; Jijkoun and de Rijke 2005 ; <CITED HERE> ) .	CompareOrContrast	train
We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( <CITED HERE> , Krahmer and Theune 2002 ) .	Background	train
A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; <CITED HERE> ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .	Background	train
From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( <CITED HERE> ) .	Background	train
This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX <CITED HERE> .	CompareOrContrast	train
The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( <CITED HERE> ; Butterworth , 1983 ) .	Background	train
As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( <CITED HERE> ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .	Background	train
However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , <CITED HERE> ) .	Background	train
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; <CITED HERE> ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	train
The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( <CITED HERE> ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .	Background	train
Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( <CITED HERE> ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .	Background	train
For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( <CITED HERE> ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .	Background	train
For instance , ( <CITED HERE> ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .	Background	train
Per-state joint normalization ( <CITED HERE>b , Â§ 8.2 ) is similar but drops the dependence on a .	CompareOrContrast	train
Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; <CITED HERE> .	Background	train
We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( <CITED HERE> ) .	Uses	train
These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; <CITED HERE> ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .	Motivation	train
Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( <CITED HERE> ; Popescu and Magnini , 2007 ) - .	Background	train
More recently , ( <CITED HERE> ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .	Background	train
McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( <CITED HERE> ; Panaget 1994 ; Wanner 1994 ) .	Background	train
It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; <CITED HERE> ) .	Background	train
A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , <CITED HERE> , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .	CompareOrContrast	train
It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; <CITED HERE> ) .	Motivation	train
Some efforts have tackled tasks such as automatic image caption generation ( <CITED HERE>a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .	Background	train
This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by <CITED HERE> .	Background	train
A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( <CITED HERE> ) .	CompareOrContrast	train
However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; <CITED HERE> ) .	Background	train
Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; <CITED HERE> ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .	Background	train
The algorithm we implemented is inspired by the work of <CITED HERE> on word sense disambiguation .	Motivation	train
Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( <CITED HERE> ) .	Background	train
EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( <CITED HERE> ) :	Uses	train
It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( <CITED HERE>a ) .	Background	train
â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( <CITED HERE> ; Roy and Subramaniam 2006 ) .	CompareOrContrast	train
A stops B from doing something ; A disagreees with B on something , 8 % and 12 % ) Note that in our original work ( <CITED HERE> ) , only development data were used to show some initial observations .	CompareOrContrast	train
We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( <CITED HERE> ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .	Uses	train
The EM algorithm ( <CITED HERE> ) can maximize these functions .	Uses	train
Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; <CITED HERE> ; Morency et al. , 2009 ) .	Background	train
For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; <CITED HERE> ) .	Future	train
For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( <CITED HERE> ) .	Future	train
The coreference system system is similar to the Bell tree algorithm as described by ( <CITED HERE> ) .	CompareOrContrast	train
Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( <CITED HERE> ) .	Background	train
The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; <CITED HERE> ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .	Background	train
We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in <CITED HERE> , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .	Uses	train
transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in <CITED HERE> with a beam size of 8 .	Uses	train
Using the basic solution proposed by ( <CITED HERE> ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions : ( 1 ) What is the potential of the existing multilingual lexical resources to approach CLTE ?	CompareOrContrast	train
Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( <CITED HERE> ) .	Uses	train
Automatic text categorization has been used in search engines , digital library systems , and document management systems ( <CITED HERE> ) .	Background	train
For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( <CITED HERE> ) containing approximately 1.7 B word tokens .	Uses	train
Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( <CITED HERE> ) ) one can not really recommend this method .	CompareOrContrast	train
For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( <CITED HERE> ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .	CompareOrContrast	train
Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as <CITED HERE> , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .	Background	train
A similar method is included in PATR-II ( <CITED HERE> ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .	CompareOrContrast	train
Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; <CITED HERE> ; Rooth et al. , 1999 ) .	CompareOrContrast	train
With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible <CITED HERE> .	Background	train
A formula q5 of L ( =-RRB- , the language with equality , is weakly R + M-abductible from an object theory T , denoted by T I-R + m 0 , iff there exists a partial theory T e PT ( T ) and a preferred model M E PM ( T ) such that M = 0 , i.e. 0 is true in at least one preferred model of the partial theory T. Note : The notions of strong provability and strong R + M-abduction can be introduced by replacing `` there exists '' by `` all '' in the above definitions ( cfXXX <CITED HERE>b ) .	CompareOrContrast	train
The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( <CITED HERE> ) .	Uses	train
A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by <CITED HERE> .	CompareOrContrast	train
In ( <CITED HERE> ) , I present evidence from Mandarin Chinese that this analysis is on the right track .	Extends	train
Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( <CITED HERE> ) .	Uses	train
Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) <CITED HERE> , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .	Uses	train
There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see <CITED HERE> , for further details ) .	Background	train
The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; <CITED HERE>a ) .	Background	train
<CITED HERE> ) .	Future	train
<CITED HERE> proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .	Background	train
financial news , we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see <CITED HERE> ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary .	Uses	train
In the field of machine learning research , incremental training has been employed in the work ( <CITED HERE> ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .	Background	train
A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; <CITED HERE> ) .	CompareOrContrast	train
In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( <CITED HERE> ) .	Uses	train
We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( <CITED HERE> ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .	Uses	train
A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( <CITED HERE> ) .	Background	train
This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ <CITED HERE>b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .	CompareOrContrast	train
Future research should apply the work of <CITED HERE> and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .	Future	train
In informal experiments described elsewhere ( <CITED HERE> ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .	Background	train
porating these two KSs into our resolver : they can Following <CITED HERE> , we select as the aneach be represented as a constraint or as a feature , tecedent of each NP , NPS , the closest preceding NP and they can be applied to the resolver in isolation that is classified as coreferent with NPS .	Motivation	train
Using the section labels , the HMM was trained using the HTK toolkit ( <CITED HERE> ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .	Uses	train
In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( <CITED HERE> ) .	CompareOrContrast	train
A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , <CITED HERE> , and Sekine ( 1998 ) ) .	Background	train
There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; <CITED HERE> ; Drews and Zwitserlood , 1995 ) .	Background	train
In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in <CITED HERE> .	CompareOrContrast	train
The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( <CITED HERE> ) .	Motivation	train
<CITED HERE> further labeled the SCFG rules with POS tags and unsupervised word classes .	CompareOrContrast	train
Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( <CITED HERE> ) .	Background	train
We offer a theorem that highlights the broad applicability of these modeling techniques .4 If f ( input , output ) is a weighted regular relation , then the following statements are equivalent : ( 1 ) f is a joint probabilistic relation ; ( 2 ) f can be computed by a Markovian FST that halts with probability 1 ; ( 3 ) f can be expressed as a probabilistic regexp , i.e. , a regexp built up from atomic expressions a : b ( for a E E U -LCB- E -RCB- , b E A U -LCB- E -RCB- ) using concatenation , probabilistic union + p , and probabilistic closure * p. For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( <CITED HERE> ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .	Background	train
Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( <CITED HERE> ) , the German equivalent to WordNet , as a sense inventory for each word .	Uses	train
Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( <CITED HERE> ) .	Uses	train
For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( <CITED HERE> ) .	Background	train
Our results are lower than those of full parsers , e.g. , <CITED HERE> as might be expected since much less structural data , and no lexical data are being used .	CompareOrContrast	train
Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan <CITED HERE> ) , where specified meter or rhyme schemes are enforced .	CompareOrContrast	train
Hermann and Deutsch ( 1976 ; also reported in <CITED HERE> ) show that greater differences are most likely to be chosen , presumably because they are more striking .	Background	train
The result holds for both the MaltParser ( <CITED HERE> ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .	Uses	train
This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( <CITED HERE> ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .	Background	train
In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ <CITED HERE> ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	Uses	train
We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( <CITED HERE> ) to identify their heads .	Uses	train
<CITED HERE> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .	Background	train
feature Cohen 's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2 : Inter-coder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures , see <CITED HERE> , it is usually assumed that Cohen 's kappa figures over 60 are good while those over 75 are excellent ( Fleiss , 1971 ) .	Background	train
Collins 1996 , Charniak 1997 , Collins 1999 and <CITED HERE> ) .	CompareOrContrast	train
Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; <CITED HERE> ) and the .	Background	train
We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( <CITED HERE> ) to align the tokenized corpora at the word level .	Uses	train
Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; <CITED HERE> ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .	Background	train
We then go on to compare the current approach with that of some other theories with similar aims : the `` standard '' version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by <CITED HERE> and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language '' approach of Dalrymple et al. ( 1996 ) .	CompareOrContrast	train
Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX <CITED HERE> ; Veale and Way 1997 ; Carl 1999 ) .7 As an example , consider the translation into French of the house collapsed .	Background	train
Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( <CITED HERE>b ) .	Background	train
One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; <CITED HERE> ) .	CompareOrContrast	train
Adding selectional restrictions ( semantic feature information , <CITED HERE> ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .	Background	train
Software engineering research on Generative Programming ( <CITED HERE> ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .	Background	train
Both use the evaluation software and triple encoding presented in <CITED HERE> .	Uses	train
In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. <CITED HERE> cases ) .	Background	train
Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( <CITED HERE> ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .	Background	train
Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( <CITED HERE> ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .	Background	train
Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( <CITED HERE> ) .	Uses	train
Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and <CITED HERE> : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .	Background	train
Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; <CITED HERE> ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .	Background	train
The reader may consult recent papers on this subject ( e.g. <CITED HERE> ; Webber 1987 ) to see what a formal interpretation of events in time might look like .	Background	train
Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX <CITED HERE>a , 1987b ) .	Background	train
For our experiments we used the standard division of the WSJ ( <CITED HERE> ) , with sections 2 through 21 for training ( approx .	Uses	train
With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( <CITED HERE> ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .	Uses	train
From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( <CITED HERE> ) .	Background	train
Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; <CITED HERE> ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .	Background	train
Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( <CITED HERE> ) .	Background	train
There are several variations of such a method ( Ballesteros and Croft , 1998 ; <CITED HERE> ; Hull 1997 ) .	CompareOrContrast	train
This evaluation set-up is an improvement versus the one we previously reported ( <CITED HERE> ) , in which fixed partitions were used for training , development , and testing .	Extends	train
<CITED HERE> compared the performace of NEs versus BoW features .	Background	train
The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of <CITED HERE> .	CompareOrContrast	train
The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in <CITED HERE> , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) .	CompareOrContrast	train
However , the method we are currently using in the ATIS domain ( <CITED HERE> ) represents our most promising approach to this problem .	Future	train
Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm <CITED HERE> such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .	Uses	train
<CITED HERE> , Charniak 1997 , Collins 1999 and Charniak 2000 ) .	CompareOrContrast	train
To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and <CITED HERE> show that verb clusters can be used to improve activity recognition in videos .	Background	train
The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and <CITED HERE> ; Gotz and Meurers 1997a ) .	Uses	train
Arabic has two kinds of plurals : broken plurals and sound plurals ( <CITED HERE> ; Chen and Gey , 2002 ) .	Background	train
Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , <CITED HERE> , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .	CompareOrContrast	train
To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , <CITED HERE> .	CompareOrContrast	train
But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to <CITED HERE> and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .	Background	train
Our plan is to implement a windowed or moving-average version of BLEU as in ( <CITED HERE> ) .	Future	train
As <CITED HERE> rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .	CompareOrContrast	train
The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; <CITED HERE> ) .	Background	train
For instance , part of the ACE Phase 2 also adopted a corpus-based approach to SC deterevaluation involves classifying an NP as PERSON , mination that is investigated as part of the mention ORGANIZATION , GPE ( a geographical-political redetection ( MD ) task ( e.g. , <CITED HERE> ) .	CompareOrContrast	train
In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; <CITED HERE> ; Petrov et al. , 2010 ) .	Background	train
Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in <CITED HERE> .	Motivation	train
TF is given by TFD , t , and it denotes frequency of term t in document D. IDF is given by IDFt = log ( N/dft ) , where N is the number of documents in the collection , and dft is the number of documents containing the term t. ( <CITED HERE> ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .	Motivation	train
An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( <CITED HERE> ) .	Background	train
More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( <CITED HERE> ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .	Background	train
In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( <CITED HERE> ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .	Background	train
For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( <CITED HERE> ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .	Background	train
This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ <CITED HERE> ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .	CompareOrContrast	train
Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; <CITED HERE> ; Calcagno and Pollard 1995 ) and the .	Background	train
In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( <CITED HERE> ) .	Uses	train
The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( <CITED HERE> ) .	Uses	train
Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( <CITED HERE> ) , and the X2 statistic .	Background	train
Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( <CITED HERE> ) .	Background	train
The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; <CITED HERE> ) .	Background	train
In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; <CITED HERE> ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .	Background	train
Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( <CITED HERE> ; Tar an , 1981a ) .	Background	train
<CITED HERE> take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .	Background	train
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; <CITED HERE> ; Steinhauser et al. , 2007 ) .	Background	train
Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( <CITED HERE> ) for a recent example with further references ) .	Background	train
We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( <CITED HERE> ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity ' 6 Separate equivalences might also make it easier to encode determiner-specific preferences , such as that of each for wide scope .	Background	train
Following <CITED HERE> , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .	Uses	train
Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; <CITED HERE> ) .	Background	train
The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; <CITED HERE> ; Charniak , 2000 ) .	CompareOrContrast	train
Following <CITED HERE> , we measure association norm prediction as an average of percentile ranks .	Uses	train
There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; <CITED HERE>b ; Rappaport Hovav and Levin , 1998 ) .	Background	train
Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) <CITED HERE> , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .	Uses	train
<CITED HERE> re-trained the linguistic parsers bilingually based on word alignment .	CompareOrContrast	train
de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( <CITED HERE> ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( Miller and Sag 1993 ) to operate on those raised elements .	Background	train
Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( <CITED HERE> ) , such as the relation between syntax and semantic .	Background	train
From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; <CITED HERE> ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .	Background	train
The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( <CITED HERE> ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .	Uses	train
A similar problem is discussed in the psycholinguistics of interpretation ( <CITED HERE> ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .	CompareOrContrast	train
In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of <CITED HERE> .	Uses	train
Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( <CITED HERE> ) .	Extends	train
And <CITED HERE> argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .	Background	train
<CITED HERE> use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .	Background	train
The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( <CITED HERE> ) .	Uses	train
A common computational treatment of lexical rules adopted , for example , in the ALE system ( <CITED HERE> ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .	CompareOrContrast	train
Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on <CITED HERE> and Collins ( 1997 ) ;	Uses	train
Authors may choose this right with the No-Deriv option of the Creative Commons licences ( <CITED HERE> ) .	Background	train
Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( <CITED HERE> ) .4 However , mathematical analysis can not tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application .	Background	train
The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( <CITED HERE> ) .	Extends	train
We also compute GIST vectors ( <CITED HERE> ) for every image using LearGIST ( Douze et al. , 2009 ) .	Uses	train
In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ <CITED HERE> ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	Background	train
The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( <CITED HERE> ) .	Future	train
A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by <CITED HERE> , 1994 ) .	Background	train
fields generally follow the pattern of `` introduction '' , `` methods '' , `` results '' , and `` conclusions '' ( SalangerMeyer , 1990 ; <CITED HERE> ; OrËasan , 2001 ) .	Background	train
<CITED HERE> report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .	Background	train
The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29 The unfolding transformation is also referred to as partial execution , for example , by <CITED HERE> .	Background	train
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; <CITED HERE> ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	Background	train
9 We do not relate to specific results in their study because it has been brought to our attention that <CITED HERE> are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .	CompareOrContrast	train
Inspired by ( <CITED HERE> ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent .	Motivation	train
The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; <CITED HERE> ; Walker et al. , 2000 ) .	Background	train
Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; <CITED HERE> ) , and Collins 's Model 2 parser ( 1997 ) .	Uses	train
Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; <CITED HERE> ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .	Background	train
We prepare the training data by splitting compounds in two steps , following the technique of <CITED HERE> .	Uses	train
Recent work by <CITED HERE> on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .	Background	train
Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; <CITED HERE> ) .	CompareOrContrast	train
Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; <CITED HERE> ) .	Background	train
Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( <CITED HERE> ; Munson et al. , 2005 ) .	Future	train
The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; <CITED HERE> ; Copestake et al. , 2001 ) .	CompareOrContrast	train
For example , <CITED HERE> and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .	Background	train
As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX <CITED HERE> ) .	Background	train
In a similar vain to Skut and Brants ( 1998 ) and <CITED HERE> , the method extends an existing flat shallow-parsing method to handle composite structures .	Future	train
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( <CITED HERE> ) .	CompareOrContrast	train
â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; <CITED HERE> ; Gough , Way , and Hearne 2002 )	Background	train
In Charniak ( 1996 ) and <CITED HERE> , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .	Background	train
We apply our system to the latest version of the XTAG English grammar ( The XTAG Research <CITED HERE> ) , which is a large-scale FB-LTAG grammar .	Uses	train
Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; <CITED HERE> , 2005 ) .	Background	train
Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( <CITED HERE> ) .	Background	train
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; <CITED HERE> ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	Background	train
Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ <CITED HERE> ] Chapter 8 ) .	Background	train
â¢ language learning ( <CITED HERE> ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )	Background	train
Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , <CITED HERE> , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .	CompareOrContrast	train
While corpus driven efforts along the PARSEVAL lines ( <CITED HERE> ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .	Background	train
Before using the DCA method , we applied a Russian morphological processor ( <CITED HERE> ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .	Uses	train
The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( <CITED HERE> ) .	Uses	train
Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( <CITED HERE> ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .	Background	train
The language chosen for semantic representation is a flat semantics along the line of ( <CITED HERE> ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .	CompareOrContrast	train
In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( <CITED HERE> ) .	Background	train
A common way to combine different models consists of selecting the model that is most confident regarding its decision ( <CITED HERE> ) .	CompareOrContrast	train
Common sense ( as well as the Gricean maxims ; <CITED HERE> ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .	Background	train
A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( <CITED HERE> ; Emele 1994 ) .	CompareOrContrast	train
For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in <CITED HERE> .	Future	train
This description can then be given the standard set-theoretical interpretation of <CITED HERE> , 1994 ) . '	Background	train
<CITED HERE> helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms .	Background	train
A more flexible approach is used by <CITED HERE> , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .	Background	train
Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( <CITED HERE> ) .	Background	train
The shallow parser used is the SNoW-based CSCL parser ( <CITED HERE> ; Munoz et al. , 1999 ) .	Uses	train
In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; <CITED HERE> ) .	CompareOrContrast	train
Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim <CITED HERE> ) terminology .	Uses	train
Unfortunately , as shown in ( <CITED HERE> ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .	Motivation	train
Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , <CITED HERE> .	Background	train
<CITED HERE> have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .	Background	train
â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( <CITED HERE> ) .	CompareOrContrast	train
For example , Jokinen and Ragni ( 2007 ) and <CITED HERE> find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .	Background	train
This approach resembles the work by <CITED HERE> and Hirschman et al. ( 1975 ) on selectional restrictions .	CompareOrContrast	train
Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( <CITED HERE> ) .	Background	train
feature Cohen 's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2 : Inter-coder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures , see Artstein and Poesio ( 2008 ) , it is usually assumed that Cohen 's kappa figures over 60 are good while those over 75 are excellent ( <CITED HERE> ) .	Background	train
To name a few examples , Rohrbach et al. ( 2010 ) and <CITED HERE> show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .	Background	train
Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; <CITED HERE> ; Tzoukermann et al. , 1997 ) .	Background	train
The XTAG group ( <CITED HERE> ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .	Background	train
HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( <CITED HERE> ) .	CompareOrContrast	train
The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( <CITED HERE> ) .	Background	train
ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( <CITED HERE> ) .	Uses	train
based parsing algorithms with an arc-factored parameterization ( <CITED HERE> ) .	Uses	train
Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; Shrestha and McKeown 2004 ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( <CITED HERE> ;	CompareOrContrast	train
The retrieval process relies on the vector space model ( <CITED HERE> ) , with the cosine measure expressing the similarity between a query and a document .	Uses	train
For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( <CITED HERE> ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) .	Background	train
A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; <CITED HERE> ; Goldberg and Zhu , 2006 ) .	Background	train
A few others incorporate various measures of inter-document similarity between the texts to be labeled ( <CITED HERE> ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .	Background	train
This is similar to `` one sense per collocation '' idea of <CITED HERE> .	CompareOrContrast	train
Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; <CITED HERE> ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .	Background	train
This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( <CITED HERE> ) .	Background	train
be found in figure 2 , which is similar with that in <CITED HERE> .	CompareOrContrast	train
It is inspired by the system described in <CITED HERE> .	Motivation	train
â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; <CITED HERE> ) .	CompareOrContrast	train
Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( <CITED HERE> ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :	CompareOrContrast	train
Riehemann 1993 ; <CITED HERE> ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .	CompareOrContrast	train
When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( <CITED HERE> ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .	Future	train
These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , <CITED HERE> , and Hogan and Frederking ( 1998 ) .	CompareOrContrast	train
A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , <CITED HERE> , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .	Background	train
Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( <CITED HERE> ; Ng and Jordan , 2001 ) .	Background	train
This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; <CITED HERE> ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .	CompareOrContrast	train
A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; <CITED HERE> ) .	Background	train
11 <CITED HERE> proposes to unify these two steps by including an update operator in the	Background	train
For instance , when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( <CITED HERE> ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ' extraction from parallel parsed corpus ( Menezes & Richardson , 2001 ) , ( Watanabe et al. ,	Background	train
<CITED HERE> introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .	Background	train
More details on how the structural divergences described in ( <CITED HERE> ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) .	Background	train
At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( <CITED HERE> ) .	Uses	train
More recently , <CITED HERE> show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature	Background	train
We found the same number using our previous approach ( <CITED HERE> ) , which is roughly equivalent to our core module .	Extends	train
Similar things hold for multifaceted properties like intelligence ( <CITED HERE> ) .	Background	train
We have shown elsewhere ( <CITED HERE> ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .	Extends	train
( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; <CITED HERE> ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .	CompareOrContrast	train
There has been some controversy , at least for simple stemmers ( <CITED HERE> ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .	Background	train
<CITED HERE> reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .	CompareOrContrast	train
Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( <CITED HERE> ; Gooi and Allan , 2004 ) .	Background	train
More recently , <CITED HERE> have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .	Background	train
With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a <CITED HERE> style analysis .	Background	train
For some adjectives , including the ones that <CITED HERE> called evaluative ( as opposed to dimensional ) , this is clearly inadequate .	Background	train
WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( <CITED HERE>b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .	Extends	train
The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( <CITED HERE> ) , with the resolution process as described here .	Future	train
They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( <CITED HERE> ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .	Motivation	train
While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; <CITED HERE> ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms .	CompareOrContrast	train
While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( <CITED HERE> ) .	Background	train
The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; <CITED HERE> ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .	Background	train
In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( <CITED HERE> ) .	Background	train
More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; <CITED HERE> ) could be applied to extract semantic classes from the corpus itself .	Future	train
The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; <CITED HERE> ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .	Background	train
<CITED HERE> replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .	Background	train
â¢ Only an automatic evaluation was performed , which relied on having model responses ( <CITED HERE> ; Berger et al. 2000 ) .	CompareOrContrast	train
Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; <CITED HERE> ) as discussed in ( Gotz and Meurers , 1997a ) and ( Meurers and Minnen , 1997 ) .	Background	train
In <CITED HERE>a ) we identified several systems that resemble ours in that they provide answers to queries .	Background	train
The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , <CITED HERE> , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .	CompareOrContrast	train
A variety of such lists for many languages are already available ( e.g. , <CITED HERE> ) .	Background	train
In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( <CITED HERE> ) .	Motivation	train
Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and <CITED HERE>a ) all belong to the syntactic transformation category .	Background	train
We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( <CITED HERE> ) .	Uses	train
<CITED HERE> observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .	Background	train
[ The current system should be distinguished from an earlier voice system ( VNLC , <CITED HERE> ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]	CompareOrContrast	train
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; <CITED HERE> ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	train
He was a grammarian who analysed Sanskrit ( <CITED HERE> ) .	Background	train
After much exploration , <CITED HERE> discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .	Background	train
The necessity of this kind of merging of arguments has been recognized before : <CITED HERE> call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .	Background	train
The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( <CITED HERE> ) , contains 3,145 annotated documents .	Uses	train
and <CITED HERE> , as described below .	CompareOrContrast	train
We collect substring rationales for a sentiment classification task ( <CITED HERE> ) and use them to obtain significant accuracy improvements for each annotator .	Uses	train
We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( <CITED HERE> ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .	Uses	train
Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; <CITED HERE> ; McCallum and Wellner , 2004 ) .	Background	train
More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( <CITED HERE> ) .	Background	train
The expectation parser uses an ATN-like representation for its grammar ( <CITED HERE> ) .	Uses	train
â¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; <CITED HERE> ) .	CompareOrContrast	train
The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( <CITED HERE> : 0.28 % vs. 0.20 % error rate ) .	CompareOrContrast	train
We follow our previous work ( <CITED HERE>b ) and restrict bridging to non-coreferential cases .	Extends	train
In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( <CITED HERE> ; Wallace 2005 ) .	Uses	train
This is in line with our previous findings from ( <CITED HERE> ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .	CompareOrContrast	train
As has been previously observed and exploited in the NLP literature ( <CITED HERE> ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .	CompareOrContrast	train
It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; <CITED HERE> ; Chen , 1996 ) .	Background	train
It provides a fine grained NE recognition covering 100 different NE types ( <CITED HERE> ) .	Uses	train
Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( <CITED HERE> ) .	Background	train
Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; <CITED HERE> ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .	Background	train
In FAQs , <CITED HERE> employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .	Background	train
The statistical significance test is performed by the re-sampling approach ( <CITED HERE> ) .	Uses	train
With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by <CITED HERE> using a Hale and Keyser ( 1993 ) style analysis .	Background	train
Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; <CITED HERE> ) .	CompareOrContrast	train
We used the revised experimental setup ( <CITED HERE> ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .	Uses	train
<CITED HERE> ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .	CompareOrContrast	train
Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( <CITED HERE> ) .3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into TIT definite clauses	Background	train
In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( <CITED HERE> ) .	Background	train
This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( <CITED HERE> ) ) are incorporated into our system .	Future	train
( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; <CITED HERE> ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .	CompareOrContrast	train
raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; <CITED HERE> ) and create multiple features for length using a decision tree ( J48 ) .	Extends	train
Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and <CITED HERE> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .	Background	train
In some systems such dependencies are learned from labeled examples ( <CITED HERE> ) .	Background	train
CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( <CITED HERE> , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .	Background	train
Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see <CITED HERE> , for further discussion ) .	Background	train
While wikis have spread from a detailed design ( <CITED HERE> ) , unfortunately blogs have not been designed under a model .	Background	train
In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and <CITED HERE> .	CompareOrContrast	train
notation of <CITED HERE> is more sophisticated , and may be considered another possibility .	CompareOrContrast	train
The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; <CITED HERE> ; Zue et al. , 2000 ; Walker et al. , 2000 ) .	Background	train
Most approaches rely on VerbNet ( <CITED HERE> ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .	Background	train
<CITED HERE> aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .	Background	train
We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( <CITED HERE> ) .	Uses	train
Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , <CITED HERE> .	Future	train
Most DOP models , such as in Bod ( 1993 ) , <CITED HERE> , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .	Background	train
Some previous works ( <CITED HERE> ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .	Background	train
Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( <CITED HERE> ) .	Background	train
Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and <CITED HERE> ;	Uses	train
Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; <CITED HERE> ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .	Future	train
In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see <CITED HERE>a ) for a brief overview .	Background	train
However , <CITED HERE> claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .	Motivation	train
In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( <CITED HERE> ) .	Uses	train
This alignment is done on the basis of both length ( Gale and Church <CITED HERE> ) and a notion of cognateness ( Simard [ 16 ] ) .	Uses	train
Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; <CITED HERE> ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .	Extends	train
A more detailed discussion of the various available Arabic tag sets can be found in <CITED HERE> .	Background	train
The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from <CITED HERE> .	Uses	train
The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , <CITED HERE> ) .	Background	train
In our previous papers ( <CITED HERE> ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .	CompareOrContrast	train
A number of speech understanding systems have been developed during the past fifteen years ( <CITED HERE> , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .	CompareOrContrast	train
Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; <CITED HERE> ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	Background	train
In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( <CITED HERE> ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .	Background	train
A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; <CITED HERE> ) .	Background	train
( <CITED HERE> contains further description and discussion of LDOCE . )	Background	train
<CITED HERE> , 1997 ) assumes that words ending in - ed are verbs .	CompareOrContrast	train
For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( <CITED HERE> ) .	Uses	train
We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( <CITED HERE> ) .	Uses	train
The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; <CITED HERE> ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .	Background	train
There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; <CITED HERE> ; Hull , 1996 ) .	Background	train
We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( <CITED HERE> ) .	Future	train
Generally speaking , we find that the personal public diary metaphor behind blogs ( <CITED HERE> ) may bring to an unsatisfactory representation of the context .	Background	train
Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by <CITED HERE>b ) .	Background	train
A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; <CITED HERE> ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .	Background	train
The significance testing is performed by paired bootstrap re-sampling ( <CITED HERE> ) .	Uses	train
The system is implemented based on ( <CITED HERE> ) and ( Marcu et al. 2006 ) .	Uses	train
IGEN uses standard chart generation techniques ( <CITED HERE> ) in its base generator to efficiently produce generation candidates .	Background	train
Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from <CITED HERE> .	Uses	train
The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and <CITED HERE> , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .	Background	train
A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( <CITED HERE>a , 2006b ) .	Background	train
<CITED HERE> reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .	Background	train
<CITED HERE> describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .	Background	train
The contextual interpreter then uses a reference resolution approach similar to <CITED HERE> , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output .	CompareOrContrast	train
<CITED HERE> maintains a survey of this area .	Background	train
The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( <CITED HERE> ) .	Background	train
In the context of word alignment , <CITED HERE> use a state-duration HMM in order to model word-to-phrase translations .	Background	train
With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( <CITED HERE> ) .	CompareOrContrast	train
We could also introduce new variables , e.g. , nonterminal refinements ( <CITED HERE> ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .	Future	train
Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( <CITED HERE> ; Roberto et al. , 2007 ) .	Background	train
Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; <CITED HERE> ) .	Background	train
Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; <CITED HERE>a ) .	Background	train
The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( <CITED HERE> ) .	Background	train
For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( <CITED HERE> ) .	Future	train
The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) <CITED HERE> , and	Uses	train
We take some core ideas from our previous work on mining script information ( <CITED HERE> ) .	Extends	train
While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; <CITED HERE> ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .	Background	train
Typical examples are Bulgarian ( <CITED HERE> ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .	Background	train
â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( <CITED HERE> ; Leuski et al. 2006 ) .	CompareOrContrast	train
Our work on the prosodic phrase status of clause final prepositional phrases , which we discuss below , suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase .3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in <CITED HERE> , which `` are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when Computational Linguistics Volume 16 , Number 3 , September 1990 157 J. Bachenko and E. Fitzpatrick Discourse-Neutral Prosodic Phrasing in English there is no good reason to take some other option '' ( p. 251 ) .	CompareOrContrast	train
Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; <CITED HERE> ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	Background	train
When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , <CITED HERE> ) .	Background	train
Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , <CITED HERE> and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .	Background	train
While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; <CITED HERE> ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .	Background	train
Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase <CITED HERE> , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .	Uses	train
The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( <CITED HERE> ) .	Background	train
The features can be easily obtained by modifying the TAT extraction algorithm described in ( <CITED HERE> ) .	Extends	train
<CITED HERE> present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .	Background	train
The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( <CITED HERE> ) .	Background	train
ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( <CITED HERE> ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â ) of the elements one	Background	train
<CITED HERE> built a corpus by iteratively searching Google for a small set of seed terms .	Background	train
Expanding on a suggestion of <CITED HERE> , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .	Extends	train
This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , <CITED HERE> , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .	CompareOrContrast	train
For more information on CATiB , see <CITED HERE> and Habash , Faraj , and Roth ( 2009 ) .	Background	train
With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( <CITED HERE> ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; Melamed , 1995 ) .	Background	train
Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; <CITED HERE> ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .	Extends	train
Our approach to extract and classify social events builds on our previous work ( <CITED HERE> ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .	Extends	train
Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; <CITED HERE> ; Goldberg and Zhu , 2006 ) or explicit	Background	train
For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( <CITED HERE> ; Nakano and Shimazu , 1999 ) .	Future	train
<CITED HERE> employed a Bayesian method to learn discontinuous SCFG rules .	CompareOrContrast	train
Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( <CITED HERE> ) .	Uses	train
We use the same splits as <CITED HERE> .	Uses	train
These results are slightly worse than those obtained in previous studies using the same annotation scheme ( <CITED HERE> ) , but are still sat -	CompareOrContrast	train
In <CITED HERE> , this flattening process is not part of the grammar .	Background	train
MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( <CITED HERE> ; De Groote and Dorsch 2003 ) .	Background	train
There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; <CITED HERE> ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .	Background	train
He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition '' <CITED HERE>:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .	Background	train
For this research , we used a coreference resolution system ( ( <CITED HERE> ) ) that implements different sets of heuristics corresponding to various forms of coreference .	Uses	train
The work that is most similar to ours is that of <CITED HERE> , who introduced the Constraint Driven Learning algorithm ( CODL ) .	CompareOrContrast	train
For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX <CITED HERE> , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11 What to optimize ?	Background	train
<CITED HERE> tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .	CompareOrContrast	train
As shown in ( <CITED HERE> ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .	Motivation	train
Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; <CITED HERE> ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .	Background	train
This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by <CITED HERE>a ) , among many others .	Background	train
<CITED HERE> also presents a similar method for the extraction of a TAG from the Penn Treebank .	Background	train
To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( <CITED HERE> ) by its generator .	Background	train
A previous work along this line is <CITED HERE> , which is based on weighted finite-state transducers ( FSTs ) .	CompareOrContrast	train
Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; <CITED HERE> ; Shi and Mihalcea , 2005 ) .	Background	train
<CITED HERE> describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .	Background	train
As suggested in <CITED HERE> this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .	Motivation	train
32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( <CITED HERE> ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .	Background	train
This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; <CITED HERE> ) .	Extends	train
These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( <CITED HERE> ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) .	Background	train
See , among others , ( <CITED HERE> ) .	Background	train
In Table 2 , lem refers to the LTAG parser ( <CITED HERE> ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .	CompareOrContrast	train
<CITED HERE> and Burkett et al. ( 2010 ) focused on joint parsing and alignment .	CompareOrContrast	train
In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ <CITED HERE> ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	Background	train
For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( <CITED HERE> ) .	Background	train
Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( <CITED HERE> ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .	Background	train
Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( <CITED HERE>a ) .	Background	train
This idea was expanded to include nouns and their modifiers through verb nominalizations ( <CITED HERE> ; Quirk et al. , 1985 ) .	Background	train
For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( <CITED HERE> ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .	Background	train
<CITED HERE> define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .	Background	train
9 We only use the minimal GHKM rules ( <CITED HERE> ) here to reduce the complexity of the sampler .	Uses	train
FBLTAG ( <CITED HERE> ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .	Background	train
In addition to headwords , dictionary search through the pronunciation field is available ; <CITED HERE> has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) .	Uses	train
Research on shallow parsing was inspired by psycholinguistics arguments ( <CITED HERE> ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .	Background	train
Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; <CITED HERE> ; Malik , Subramaniam , and Kaushik 2007 ) .	Background	train
results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( <CITED HERE> ) .	Uses	train
A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( <CITED HERE> ) .	Uses	train
Our baseline coreference system uses the C4 .5 decision tree learner ( <CITED HERE> ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .	Uses	train
This alignment is obtained by following the same set of rules learned from the development dataset as in ( <CITED HERE> ) .	Uses	train
Developed Systems Our developed system is built on the work by <CITED HERE> , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .	Uses	train
The best performance on the Brown corpus , a 0.2 % error rate , was reported by <CITED HERE> , who trained a decision tree classifier on a 25-million-word corpus .	CompareOrContrast	train
Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; <CITED HERE> ) .	Background	train
These include devices such as interleaving the components ( McDonald 1983 ; <CITED HERE> ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) .	Background	train
The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and <CITED HERE> , and became a common testbed .	CompareOrContrast	train
<CITED HERE> was the first scholar who stressed the impact of the digital revolution to the medium of writing .	Background	train
A similar problem is discussed in the psycholinguistics of interpretation ( <CITED HERE> ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .	Background	train
Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( <CITED HERE> ) .	Background	train
In our previous work ( <CITED HERE> ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .	Extends	train
Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; <CITED HERE> ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .	Background	train
the mention sub-type , which is a sub-category of the mention type ( <CITED HERE> ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .	Uses	train
For more details on the proprieties of SSTC , see <CITED HERE> .	Background	train
This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; <CITED HERE> ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .	Background	train
Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( <CITED HERE> ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .	Uses	train
To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( <CITED HERE> ) with dense features .	Uses	train
To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by <CITED HERE> .	Uses	train
The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( <CITED HERE> ) .	Uses	train
In particular , ( <CITED HERE> ) lists the converses of some 3 500 predicative nouns .	Future	train
This is similar to the `` deletion '' strategy employed by <CITED HERE> , but we do it directly in the grammar .	CompareOrContrast	train
Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; <CITED HERE> ) .	CompareOrContrast	train
Many NLP applications require knowledge about semantic relatedness rather than just similarity ( <CITED HERE> ) .	Background	train
Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( <CITED HERE> ; Regneri et al. , 2013 ) .	Background	train
It has been shown ( <CITED HERE> ) that the subcategorization tendencies of verbs vary across linguistic domains .	Motivation	train
In the system , we extract both the minimal GHKM rules ( <CITED HERE> ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .	Uses	train
Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; <CITED HERE> ; Carl 1999 ) .7 As an example , consider the translation into French of the house collapsed .	Background	train
One of the better-known approaches is described in <CITED HERE> , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .	Background	train
Unlike the models proposed by <CITED HERE>b ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .	CompareOrContrast	train
We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; <CITED HERE> ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .	Uses	train
( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; <CITED HERE> ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .	Background	train
Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and <CITED HERE> ) , and LFS ( Iordanskaja et al. , 1992 ) .	Extends	train
Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( <CITED HERE> ) .	Background	train
Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; <CITED HERE> ) ) .	Background	train
Association Norms ( AN ) is a collection of association norms collected by Schulte im <CITED HERE> .	Uses	train
Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( <CITED HERE> ) .	Background	train
The changes made were inspired by those described in <CITED HERE> , page 75 ) .	Motivation	train
The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of <CITED HERE> , 1985 ) .	Extends	train
Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( <CITED HERE> ) .	Background	train
A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITED HERE> .	Background	train
Using the GHKM algorithm ( <CITED HERE> ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .	Uses	train
The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; <CITED HERE> ) .	Background	train
Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , <CITED HERE> , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .	Background	train
Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including `` crummy '' MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( <CITED HERE> ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby .	Background	train
Our HDP extension is also inspired from the Bayesian model proposed by <CITED HERE> .	Motivation	train
The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; <CITED HERE> ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .	Background	train
As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; <CITED HERE> ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .	CompareOrContrast	train
To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( <CITED HERE> ) , using each score as a feature .	Uses	train
Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; <CITED HERE> ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .	Background	train
For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( <CITED HERE> ) .	Background	train
Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( <CITED HERE> ) .	Future	train
For example , 10 million words of the American National Corpus ( <CITED HERE> ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .	Background	train
From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; <CITED HERE> ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .	Background	train
This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in <CITED HERE> and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .	Background	train
Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( <CITED HERE> ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .	Background	train
In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in <CITED HERE> in our experiments .	Uses	train
We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( <CITED HERE> ; Bengtson and Roth , 2008 ) .	Uses	train
Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; <CITED HERE> ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .	Background	train
An alternative representation based on <CITED HERE> is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .	CompareOrContrast	train
For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( <CITED HERE>a ) , while being more than an order of magnitude faster .	CompareOrContrast	train
According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( <CITED HERE> ) .	Background	train
Since then this idea has been applied to several tasks , including word sense disambiguation ( <CITED HERE> ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .	Background	train
â¢ History-based feature models for predicting the next parser action ( <CITED HERE> ) .	Uses	train
For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( <CITED HERE> ) 2 .	Uses	train
However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on <CITED HERE> .	Uses	train
Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; <CITED HERE> ) .	Background	train
The powerful mechanism of lexical rules ( <CITED HERE> ) has been used in many natural language processing systems .	Background	train
The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in <CITED HERE> , 1991 ) , with some differences that are commented on below .	CompareOrContrast	train
<CITED HERE> presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .	CompareOrContrast	train
NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; <CITED HERE> ) : The selected expression should also be felicitous .	Background	train
<CITED HERE> , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â > French and English â > Urdu .	Background	train
This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( <CITED HERE> ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .	CompareOrContrast	train
All experiments have been performed using MaltParser ( <CITED HERE> ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1	Uses	train
One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; <CITED HERE> ) .	Background	train
Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( <CITED HERE> ; Huang , 2008 ) for self training .	Future	train
Some researchers ( <CITED HERE> ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .	Background	train
Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( <CITED HERE> ; Purpura and Hillard , 2006 ) .	Background	train
We chose to follow <CITED HERE> and split the sentences evenly to facilitate further comparison .	Uses	train
That is , a document that contains terms al , a2 and a3 may be ranked higher than a document which contains terms al and b.f. However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( <CITED HERE> ) .	Background	train
The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( <CITED HERE>a ) .	Future	train
Using an accumulator passing technique ( <CITED HERE> ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .	Uses	train
However , most strategies are based on `` internal '' or `` external methods '' ( <CITED HERE> ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .	CompareOrContrast	train
With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; <CITED HERE> ) .	Background	train
A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( <CITED HERE> ) and Boitet & Zaharin ( 1988 ) .	Uses	train
The Web People Search task , as defined in the first WePS evaluation campaign ( <CITED HERE> ) , consists of grouping search results for a given name according to the different people that share it .	Background	train
Although this is only true in cases where y occurs in an upward monotone context ( <CITED HERE> ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .	Motivation	train
We use the same data setting with <CITED HERE> , however a bit different from Xue and Palmer ( 2005 ) .	Uses	train
Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and <CITED HERE>a ) 's information-theoretic metric work best .	Background	train
We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( <CITED HERE> ) .	Extends	train
4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , <CITED HERE> ) .	Background	train
For descriptions of SMT systems see for example ( Germann et al. , 2001 ; <CITED HERE> ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .	Background	train
Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( <CITED HERE> ) and machine translation ( Knight and Al-Onaizan , 1998 ) .	Background	train
It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; <CITED HERE> ) .	CompareOrContrast	train
The ConTroll grammar development system as described in ( <CITED HERE>b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .	Background	train
After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by <CITED HERE> to penalize redundant sentences in cohesive clusters .	Uses	train
If each word 's translation is treated as a sense tag ( <CITED HERE> ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !	Uses	train
The values of a vector correspond to the presence or absence of each ( lemmatized ) corpus word in the document in question ( after removing stop-words and words with very low frequency ) .4 The predictive model is a Decision Graph ( <CITED HERE> ) , which , like Snob , is based on the MML principle .	Uses	train
Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( <CITED HERE> ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .	Uses	train
However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX <CITED HERE> ) .	Background	train
One , the VOYAGER domain ( <CITED HERE> ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .	Uses	train
Our task is closer to the work of <CITED HERE> , who looked at the problem of intellectual attribution in scientific texts .	CompareOrContrast	train
In the seminal work by <CITED HERE> , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .	Background	train
( <CITED HERE> ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .	CompareOrContrast	train
<CITED HERE> , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''	CompareOrContrast	train
See also ( <CITED HERE> ; Naish , 1986 ) .	Background	train
Michiels ( 1982 ) and <CITED HERE> provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .	Background	train
We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( <CITED HERE>b ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .	Uses	train
More specifically , we use LIBSVM ( <CITED HERE> ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .	Uses	train
It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( <CITED HERE> ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .	Background	train
positional features that have been employed by highwe can see , the baseline achieves an F-measure of performing resolvers such as <CITED HERE> 57.0 and a resolution accuracy of 48.4 .	CompareOrContrast	train
Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( <CITED HERE> ) .	Future	train
Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( <CITED HERE> ) dictionaries and adding around 30 frames found by manual inspection .	Background	train
The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; <CITED HERE> ; Fillmore , 1968 ) .	Background	train
It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; <CITED HERE> ) .	Background	train
For example , the suite of LT tools ( Mikheev et al. , 1999 ; <CITED HERE> ) perform tokenization , tagging and chunking on XML marked-up text directly .	Background	train
It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; <CITED HERE> ) .	Uses	train
To retrieve translation examples for a test sentence , ( <CITED HERE> ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :	Uses	train
By using the EM algorithm ( <CITED HERE> ) , they can guarantee convergence towards the globally optimum parameter set .	Background	train
â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by <CITED HERE> , â¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .	Future	train
Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; <CITED HERE> ) .	Extends	train
In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; <CITED HERE> ; Kalashnikov et al. , 2007 ) .	Background	train
To provide the required configurability in the static version of the code we will use policy templates ( <CITED HERE> ) , and for the dynamic version we will use configuration classes .	Uses	train
<CITED HERE> demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .	Uses	train
In informal experiments described elsewhere ( <CITED HERE> ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .	Extends	train
â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( <CITED HERE> ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )	Background	train
A recent study by <CITED HERE> also investigates the task of training parsers to improve MT reordering .	CompareOrContrast	train
Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; <CITED HERE> , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .	Background	train
15 <CITED HERE> show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .	Background	train
Latent Dirichlet Allocation ( <CITED HERE> ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .	Background	train
Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; <CITED HERE> ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .	Future	train
Third , the paradigm of evidence-based medicine ( <CITED HERE> ) provides a task-based model of the clinical information-seeking process .	Background	train
<CITED HERE> introduced factored SMT .	CompareOrContrast	train
Both systems are built around from the maximum-entropy technique ( <CITED HERE> ) .	Uses	train
We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; <CITED HERE> ) .	Uses	train
Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( <CITED HERE> ) .	Background	train
Undesirable consequences of this fact have been termed `` label bias '' ( <CITED HERE> ) .	Background	train
For the full parser , we use the one developed by Michael Collins ( <CITED HERE> ; Collins , 1997 ) -- one of the most accurate full parsers around .	Uses	train
Some researchers , however , including <CITED HERE> , train on predicted feature values instead .	CompareOrContrast	train
Their computational significance arises from the issue of their storage in lexical resources like WordNet ( <CITED HERE> ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .	Background	train
Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by <CITED HERE> , 2002 ) .	Background	train
<CITED HERE> claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .	Background	train
We rephrase the method of <CITED HERE> as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .	Uses	train
6The analysis is reminiscent of the treatment of coordination in the Collins parser ( <CITED HERE> ) .	CompareOrContrast	train
The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( <CITED HERE> ) .	Motivation	train
CornmandTalk ( <CITED HERE> ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .	Background	train
The annotation procedure is dependent on locating the head daughter , for which an amended version of <CITED HERE> is used .	Uses	train
<CITED HERE> ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .	CompareOrContrast	train
For instance , <CITED HERE> recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) .	Background	train
Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , <CITED HERE> and others .	CompareOrContrast	train
We use an in-house statistical tagger ( based on ( <CITED HERE> ) ) to tag the text in which the unknown word occurs .	Uses	train
Based on a computational grammar that associates natural language expressions with both a syntactic and a semantic representation , a paraphrastic gram ` As we shall briefly discuss in section 4 , the grammar is developed with the help of a meta-grammar ( <CITED HERE> ) thus ensuring an additional level of abstraction .	Uses	train
There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; <CITED HERE> ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .	CompareOrContrast	train
Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( <CITED HERE> ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .	CompareOrContrast	train
The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( <CITED HERE> ) .	Background	train
Our own work ( <CITED HERE> ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .	Extends	train
They proved to be useful in a number of NLP applications such as natural language generation ( <CITED HERE> ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .	Motivation	train
Current state-of-the-art statistical parsers ( <CITED HERE> ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .	Background	train
Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of <CITED HERE> .	Background	train
Riehemann 1993 ; Oliva 1994 ; <CITED HERE> ; Opalka 1995 ; Sanfilippo 1995 ) .	CompareOrContrast	train
The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; <CITED HERE> ) .	Background	train
A detailed introduction to the SBD problem can be found in <CITED HERE> .	Background	train
Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and <CITED HERE> ; see Esuli ( 2006 ) for an active bibliography ) .	Background	train
The BEETLE II system architecture is designed to overcome these limitations ( <CITED HERE> ) .	Background	train
For the evaluation of the results we use the BLEU score ( <CITED HERE> ) .	Uses	train
There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; <CITED HERE> ; Malik , Subramaniam , and Kaushik 2007 ) .	CompareOrContrast	train
For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( <CITED HERE> ) or ( Ratnaparkhi , 1997 ) .	Background	train
This design idea was adopted from TANKA ( <CITED HERE>b ) .	Uses	train
The problem of handling ill-formed input has been studied by <CITED HERE> , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .	CompareOrContrast	train
In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( <CITED HERE> ) .	Motivation	train
The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( <CITED HERE> ) .	Background	train
Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( <CITED HERE> ) .	Uses	train
Here , the PET and GR kernel perform similar : this is different from the results of ( <CITED HERE> ) where GR performed much worse than PET for ACE data .	CompareOrContrast	train
In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; <CITED HERE> ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	Background	train
Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; <CITED HERE> ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .	Background	train
This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in <CITED HERE> , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .	CompareOrContrast	train
Our work is inspired by the latent left-linking model in <CITED HERE> and the ILP formulation from Chang et al. ( 2011 ) .	Background	train
For example , <CITED HERE> proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .	Background	train
<CITED HERE> also note that the applicability of paraphrases is strongly influenced by context .	Background	train
Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( <CITED HERE> ) .	Future	train
For descriptions of SMT systems see for example ( <CITED HERE> ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .	Background	train
like information extraction ( <CITED HERE> ) and textual entailment ( Berant et al. , 2010 ) .	Background	train
In a final processing stage , we generalize over the marker lexicon following a process found in <CITED HERE> .	Uses	train
One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( <CITED HERE> , Hepple 1990 , Hendriks 1993 ) .	Background	train
The SPR uses rules automatically learned from training data , using techniques similar to ( <CITED HERE> ; Freund et al. , 1998 ) .	CompareOrContrast	train
<CITED HERE> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .	CompareOrContrast	train
For complementing this database and for converse constructions , the LADL tables ( <CITED HERE> ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .	Future	train
These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; <CITED HERE> ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .	Background	train
Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on <CITED HERE> to permit a limited form of insertion in the translation process .	Uses	train
ment ( Sarkar and Wintner , 1999 ; <CITED HERE> ; Makino et al. , 1998 ) .	Background	train
The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( <CITED HERE> ) , and question answering .	Background	train
Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( <CITED HERE> ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .	Background	train
The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( <CITED HERE> ) or malapropism detection ( Budanitsky and Hirst , 2006 ) .	Background	train
<CITED HERE> argue for application-specific evaluation of similarity measures , because measures are always used for some task .	Background	train
The first direct application of parse forest in translation is our previous work ( <CITED HERE> ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .	Extends	train
4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and <CITED HERE> ; Gerdemann 1995 ) .	Background	train
In their Gaijin system , <CITED HERE> give a result of 63 % accurate translations obtained for English â > German on a test set of 791 sentences from CorelDRAW manuals .	CompareOrContrast	train
This method can be generalized , inspired by <CITED HERE> , who derive N-gram probabilities from stochastic context-free grammars .	Background	train
The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , <CITED HERE> , and Ratnaparkhi ( 1997 ) , and became a common testbed .	CompareOrContrast	train
The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX <CITED HERE> ; Thorisson 1994 , for other plans ) .	Background	train
The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in <CITED HERE> , which exploits a parallel corpus and methods developed for statistical machine translation .	Uses	train
To name a few examples , <CITED HERE> and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .	Background	train
This approach has now gained wide usage , as exemplified by the work of <CITED HERE> , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .	Motivation	train
For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( <CITED HERE> ) , ( Al-Adhaileh & Tang , 1999 ) .	Background	train
Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( <CITED HERE> ) .	Background	train
As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( <CITED HERE> ) .	CompareOrContrast	train
de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( <CITED HERE> ) to operate on those raised elements .	Background	train
Research that is more similar in goal to that outlined in this paper is Vosse ( <CITED HERE> ) .	CompareOrContrast	train
<CITED HERE> pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .	Uses	train
Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( <CITED HERE> ) .	Uses	train
While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( <CITED HERE> ) :	Uses	train
Following <CITED HERE> , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .	Uses	train
From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( <CITED HERE> ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .	Background	train
The Gsearch system ( <CITED HERE> ) also selects sentences by syntactic criteria from large on-line text collections .	Background	train
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; <CITED HERE> ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	train
See also the work of <CITED HERE> , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .	Background	train
The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( <CITED HERE> ) .	Uses	train
The psycholinguistic studies of Martin ( 1970 ) , <CITED HERE> , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .	Background	train
Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( <CITED HERE> ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .	Background	train
Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; <CITED HERE> ; Rooth et al. , 1999 ) .	CompareOrContrast	train
â¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â¢ multilingual document filtering ( e.g. , Oard 1997 ) , â¢ computer-assisted language learning ( e.g. , <CITED HERE> ) , â¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,	Background	train
One would think that the type information ti , which is more specific than that 16 A linguistic example based on the signature given by <CITED HERE> would be a lexical rule deriving predicative signs from nonpredicative ones , i.e. , changing the PRD value of substantive signs from -- to - F , much like the lexical rule for NPs given by Pollard and Sag ( 1994 , p. 360 , fn .	Background	train
â¢ language learning ( Green 1979 ; <CITED HERE> ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )	Background	train
Notable early papers on graph-based semisupervised learning include <CITED HERE> , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .	Background	train
As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see <CITED HERE> ) .	Background	train
An example of psycholinguistically oriented research work can be found in <CITED HERE> .	Background	train
Alternatively , we may think of user-centered comparative studies ( <CITED HERE> ) .	Future	train
<CITED HERE> , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' .	Background	train
However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( <CITED HERE> ) , containing more than 1,000,000 words and 49,000 sentences .	Background	train
The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITED HERE> , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) .	CompareOrContrast	train
It also shows the structural identity to bilingual grammars as used in ( <CITED HERE> ) .	Uses	train
Relationships between the unlabeled items <CITED HERE> consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .	Background	train
100000 word stems of German ( <CITED HERE> ) .	Uses	train
The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and <CITED HERE> , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .	Extends	train
Identical to the standard perceptron proof , e.g. , <CITED HERE> , by inserting in loss-separability for normal separability .	Background	train
The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( <CITED HERE> ) .	CompareOrContrast	train
The simplest strategy for ordering adjectives is what <CITED HERE> call the direct evidence method .	Background	train
The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; <CITED HERE> ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .	Background	train
Such systems extract information from some types of syntactic units ( clauses in ( <CITED HERE> ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .	Background	train
`` petty conversational implicature '' ( <CITED HERE> ) , or the metarules of Section 5.2 ?	Background	train
Unlike other POS taggers , this POS tagger ( <CITED HERE> ) was also trained to disambiguate sentence boundaries .	Uses	train
On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( <CITED HERE> ) .	Background	train
ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( <CITED HERE> ) and called qualia relations ( Bouillon et al. , 2001 ) .	Background	train
People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( <CITED HERE> ) .	Background	train
For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( <CITED HERE> ) .	Background	train
Its significance is reflected both in the growing interest in annotation software for word sense tagging ( <CITED HERE> ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .	Background	train
Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , <CITED HERE> ) .	Background	train
Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and <CITED HERE> .	Background	train
Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by <CITED HERE> .	Uses	train
The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; <CITED HERE> ) .	CompareOrContrast	train
Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( <CITED HERE> ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks	Motivation	train
Whereas <CITED HERE> dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .	CompareOrContrast	train
We also experiment with a CCG parser ( <CITED HERE> ) , requiring that the contexts surrounding the original phrase and paraphrase are assigned	Uses	train
Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( <CITED HERE> ) .	Future	train
Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) <CITED HERE> .	Uses	train
ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( <CITED HERE> ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .	Background	train
include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( <CITED HERE> ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .	Background	train
In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( <CITED HERE> ) .	Background	train
For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( <CITED HERE> ; Lin , 1998 ) .	Future	train
converted to numerical features using the standard technique of binarization , and we split values of the FEATS field into its atomic components .4 For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( <CITED HERE> ) .	Background	train
â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( <CITED HERE> ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )	Background	train
Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( <CITED HERE> ) .	Uses	train
Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; <CITED HERE> ) .	Future	train
For English â > Urdu , <CITED HERE> , page 213 ) notes that `` the system learned the original training corpus ... perfectly and could reproduce it without errors '' ; that is , it scored 100 % accuracy when tested against the training corpus .	Background	train
Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of <CITED HERE> , Yeniterzi and Oflazer ( 2010 ) and others .	CompareOrContrast	train
Recent work ( <CITED HERE> ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .	Background	train
See <CITED HERE> for a variant of this approximation that constructs finite transducers rather than finite automata .	Background	train
Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; <CITED HERE> ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .	Background	train
These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( <CITED HERE>a , 1988c ) .	Background	train
The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; <CITED HERE> ) .	Uses	train
Other approaches use less deep linguistic resources ( e.g. , POS-tags <CITED HERE> ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .	CompareOrContrast	train
Some well-known approaches include rule-based models ( <CITED HERE> ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .	Background	train
One approach to partial parsing was presented by <CITED HERE> , who extended a shallow-parsing technique to partial parsing .	Background	train
<CITED HERE> compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .	CompareOrContrast	train
Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by <CITED HERE> with WoRDNET relations ) .	CompareOrContrast	train
In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ <CITED HERE> ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	Background	train
Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by <CITED HERE> and Paiva ( 1998 ) .	Background	train
Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , <CITED HERE> , and Brown ( 2000 ) , inter alia .	Background	train
The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; <CITED HERE> ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .	Background	train
In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( <CITED HERE> ) .	Background	train
Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , <CITED HERE> , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .	Uses	train
The only disambiguation metric that we used in our previous work ( <CITED HERE>b ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right .	Extends	train
The need for information systems to support physicians at the point of care has been well studied ( <CITED HERE> ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .	Background	train
Every arc always has a definite direction , i.e. arcs are arrows ( <CITED HERE> ) .	Background	train
In the first experiment , we use an induction algorithm ( <CITED HERE>a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .	Uses	train
To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( <CITED HERE> ) respectively .	Uses	train
6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( <CITED HERE> ) , but the simple binary bag-of-lemmas representation yielded similar results .	Uses	train
At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to <CITED HERE> .	Future	train
It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and <CITED HERE> , 1991 ) .	CompareOrContrast	train
The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; <CITED HERE> ; Goodman 1998 ) .	Background	train
Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( <CITED HERE> ) .	Background	train
The example used to illustrate the power of ATNs ( <CITED HERE> ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator .	CompareOrContrast	train
Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; <CITED HERE> ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .	Motivation	train
LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; <CITED HERE> ) .	Background	train
As in ( <CITED HERE> ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .	Uses	train
To address this limitation , our previous work ( <CITED HERE> ) has initiated an investigation on the problem of conversation entailment .	Extends	train
â A brief version of this work , with some additional material , first appeared as ( <CITED HERE>a ) .	Extends	train
They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by <CITED HERE> as follows .	Background	train
Here , I adopt the model proposed by <CITED HERE> and decompose lexical verbs into verbalizing heads and verbal roots .	Uses	train
This heuristic is called soft union ( <CITED HERE> ) .	Uses	train
This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( <CITED HERE> ) and PropBank ( Kingsbury et al. , 2002 ) .	Background	train
Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; <CITED HERE> ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .	Motivation	train
The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( <CITED HERE> ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .	Background	train
This semantics was constructed ( <CITED HERE>a , 1987b ) as a formal framework for default and commonsense reasoning .	Uses	train
There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <CITED HERE> ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .	Background	train
Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of <CITED HERE> .	CompareOrContrast	train
The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; <CITED HERE> ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .	Background	train
Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , <CITED HERE> , Niemann ( 1990 ) , and Young ( 1989 ) .	Background	train
R98 ( , , , , â ) uses a variant of Kozima 's semantic similarity measure ( <CITED HERE> ) to compute block similarity .	Extends	train
<CITED HERE> , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .	Background	train
We used a publicly available tagger ( <CITED HERE> ) to tag the words and then used these in the input to the system .	Uses	train
Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; <CITED HERE> ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .	Background	train
Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( <CITED HERE> ) ) have been created for several languages , with different degrees of coverage .	Background	train
An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( <CITED HERE> , Biermann and Ballard 1980 ) .	Background	train
For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; <CITED HERE> ) , perform in comparison to our approach .	Future	train
But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; <CITED HERE> ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .	Motivation	train
That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( <CITED HERE> ) .	Background	train
Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; <CITED HERE> ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .	Background	train
For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( <CITED HERE> ) .	Background	train
We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( <CITED HERE> ) .	Future	train
Another paper ( <CITED HERE> ) describes the detailed analysis on the factor of the difference of parsing performance .	Background	train
The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( <CITED HERE> ) 6 ( the average length is 6.32 words ) .	Uses	train
The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( <CITED HERE> ) .	Motivation	train
The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as <CITED HERE> ) .	Background	train
19 The paper by <CITED HERE> presents additional , more sophisticated models that we do not use in this article .	CompareOrContrast	train
Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; <CITED HERE> ) .	Background	train
Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , <CITED HERE> ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .	CompareOrContrast	train
( Details of how the average-expert model performs can be found in our prior work ( <CITED HERE> ) . )	Extends	train
Self-training should also benefit other discriminatively trained parsers with latent annotations ( <CITED HERE> ) , although training would be much slower compared to using generative models , as in our case .	Future	train
We introduce here a clearly defined and replicable split of the <CITED HERE> data , so that future investigations can accurately and correctly compare against the results presented here .	Uses	train
In most cases , the accuracy of parsers degrades when run on out-of-domain data ( <CITED HERE> ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .	Background	train
<CITED HERE> relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .	Background	train
To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( <CITED HERE> ) .	Uses	train
One obvious approach to this problem is to employ parser reranking ( <CITED HERE> ) .	Background	train
Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( <CITED HERE>:460 ff . )	Future	train
The table also presents the closest comparable experimental results reported by <CITED HERE> .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .	CompareOrContrast	train
How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( <CITED HERE> ) .	Extends	train
<CITED HERE> furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .	Background	train
A cooccurrence based stemmer ( <CITED HERE> ) was used to stem Spanish words .	Uses	train
The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( <CITED HERE> ) : a 0.5 % error rate .	CompareOrContrast	train
In a number of proposals , lexical generalizations are captured using lexical underspecification ( <CITED HERE> ; Krieger and Nerbonne 1992 ;	CompareOrContrast	train
Although a grid may be more descriptively suitable for some aspects of prosody ( for example , <CITED HERE> use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .	CompareOrContrast	train
The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; <CITED HERE> ; Collins , 2000 ; Bod , 2001 ) .	CompareOrContrast	train
Other factors , such as the role of focus ( Grosz 1977 , 1978 ; <CITED HERE> ) or quantifier scoping ( Webber 1983 ) must play a role , too .	Background	train
linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( <CITED HERE> ) .	CompareOrContrast	train
The second version ( RM ) concerns the Resource Management task ( <CITED HERE> ) that has been popular within the DARPA community in recent years .	Uses	train
Another line of research that is correlated with ours is recognition of agreement/disagreement ( <CITED HERE> ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .	CompareOrContrast	train
The studies presented by <CITED HERE> and Johnson ( 2007 ) differed in the number of states that they used .	CompareOrContrast	train
A number of alignment techniques have been proposed , varying from statistical methods ( <CITED HERE> ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .	Background	train
We would like to use features that look at wide context on the input side , which is inexpensive ( <CITED HERE> ) .	Future	train
Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; <CITED HERE> ) , which was difficult both to represent and to process , and which required considerable human input .	Background	train
Experiments on Chinese SRL ( Xue and Palmer 2005 , <CITED HERE> ) reassured these findings .	Background	train
This is then generalized , following a methodology based on <CITED HERE> , to generate the `` generalized marker lexicon . ''	Uses	train
A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , <CITED HERE> , Walker 1978 , and Wolf and Woods 1980 ) .	CompareOrContrast	train
Our proposed method is based on the automatically acquired paraphrase dictionary described in <CITED HERE> , in which the application of paraphrases from the dictionary encodes secret bits .	Uses	train
For example , frequent words are translated less consistently than rare words ( <CITED HERE> ) .	Background	train
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; <CITED HERE> ) .	Background	train
These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( <CITED HERE> ) .	Extends	train
Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; <CITED HERE> ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	Background	train
Due to advances in statistical syntactic parsing techniques ( <CITED HERE> ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .	Background	train
This is where robust syntactic systems like SATZ ( <CITED HERE> ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .	CompareOrContrast	train
category relationships from the weak supervision : the tag dictionary and raw corpus ( <CITED HERE> ; Garrette et al. , 2015 ) .4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags .	Uses	train
WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( <CITED HERE> ) .	Extends	train
Lexical functional grammar ( <CITED HERE> ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .	Background	train
<CITED HERE> present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic	Background	train
A companion paper describes the evaluation process and results in further detail ( <CITED HERE> ) .	Extends	train
SNoW ( Carleson et al. , 1999 ; <CITED HERE> ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .	Uses	train
Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( <CITED HERE> ) , turns out to be infeasible , at least for German and related languages .	Background	train
It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( <CITED HERE> ; Dagan et al. , 1993 ; Chen , 1996 ) .	CompareOrContrast	train
They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( <CITED HERE> ) .	Motivation	train
The results , which partly confirm those obtained on a smaller dataset in <CITED HERE> , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .	CompareOrContrast	train
We employ the idea of ultraconservative update ( <CITED HERE> ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .	Uses	train
It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( <CITED HERE> ) .	Background	train
<CITED HERE> reported a correlation of r = .69 .	CompareOrContrast	train
A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; <CITED HERE> ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .	Background	train
As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; <CITED HERE> ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .	CompareOrContrast	train
WIT features an incremental understanding method ( <CITED HERE>b ) that makes it possible to build a robust and real-time system .	Uses	train
This choice is inspired by recent work on learning syntactic categories ( <CITED HERE> ) , which successfully utilized such language models to represent word window contexts of target words .	Motivation	train
As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; <CITED HERE> ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .	Background	train
Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; <CITED HERE> ; Miller et al , 1999 .	CompareOrContrast	train
In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( <CITED HERE>b ) .	Uses	train
<CITED HERE> considers the second verb V2 as an aspectual complex comparable to the auxiliaries .	Background	train
2We could just as easily use other symmetric `` association '' measures , such as 02 ( <CITED HERE> ) or the Dice coefficient ( Smadja , 1992 ) .	CompareOrContrast	train
In this situation , <CITED HERE>b , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''	Motivation	train
The use of the web as a corpus for teaching and research on language has been proposed a number of times ( <CITED HERE> ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .	Background	train
â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( <CITED HERE> ) â¢ Localization ( Sch Â¨ aler 1996 )	Background	train
Following our previous work on stance classification ( <CITED HERE>c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .	Extends	train
However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( <CITED HERE> ) .	Background	train
This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; <CITED HERE> ) .	Background	train
We offer a theorem that highlights the broad applicability of these modeling techniques .4 If f ( input , output ) is a weighted regular relation , then the following statements are equivalent : ( 1 ) f is a joint probabilistic relation ; ( 2 ) f can be computed by a Markovian FST that halts with probability 1 ; ( 3 ) f can be expressed as a probabilistic regexp , i.e. , a regexp built up from atomic expressions a : b ( for a E E U -LCB- E -RCB- , b E A U -LCB- E -RCB- ) using concatenation , probabilistic union + p , and probabilistic closure * p. For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( <CITED HERE> ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .	Background	train
We use the <CITED HERE> CCG parser to analyse the sentence before and after paraphrasing .	Uses	train
For example , consider a relational description ( cfXXX , <CITED HERE> ) involving a gradable adjective , as in the dog in the large shed .	Background	train
Previous versions of our work , as described in <CITED HERE> also assume that phrasing is dependent on predicate-argument structure .	Extends	train
like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( <CITED HERE> ) .	Background	train
The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in <CITED HERE> , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .	Background	train
Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; <CITED HERE> consider joint coreference and entity-linking .	Background	train
However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , <CITED HERE> , Markert and Nissim ( 2005 ) ) .	Background	train
This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( <CITED HERE> ; Chang et al. , 2010 ) .	CompareOrContrast	train
The final machine is a trigram language model , specifically a Kneser-Ney ( <CITED HERE> ) based backoff language model .	Uses	train
Hovy has described another text planner that builds similar plans ( <CITED HERE>b ) .	Background	train
In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; <CITED HERE> ; Turney , 2002 ; Dave et al. , 2003 ) .	Background	train
transition-based dependency parsing framework ( <CITED HERE> ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .	Uses	train
Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX <CITED HERE> ) .	Uses	train
As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; <CITED HERE> ) .	CompareOrContrast	train
( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( <CITED HERE> ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .	CompareOrContrast	train
We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; <CITED HERE> ; Germann et al. , 2001 ) , trained on the same parallel corpus .	CompareOrContrast	train
Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( <CITED HERE> ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .	Background	train
Since we are not generating from the model , this does not introduce difficulties ( <CITED HERE> ) .	Motivation	train
The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and <CITED HERE> .	Background	train
As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling '' vs. `` brother , comrade , friend '' ( <CITED HERE> ) .	Uses	train
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; <CITED HERE> ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	Background	train
<CITED HERE> argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .	Motivation	train
Corpus frequency : ( <CITED HERE> ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .	Uses	train
We use an in-house developed hierarchical phrase-based translation ( <CITED HERE> ) as our baseline system , and we denote it as In-Hiero .	Uses	train
The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( <CITED HERE> ) .	Background	train
The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and <CITED HERE> .	CompareOrContrast	train
One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( <CITED HERE> ) .	Background	train
The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( <CITED HERE> ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .	Background	train
<CITED HERE> extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .	Background	train
Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , <CITED HERE> , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .	Background	train
Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( <CITED HERE> ) .	Background	train
Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( <CITED HERE> ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .	Background	train
The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( <CITED HERE>a ) to produce a domain-specific semantic representation of the student 's output .	Uses	train
only the available five relative scopings of the quantifiers are produced ( <CITED HERE> , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â¢ partial scopings are permitted ( see Reyle [ 19961 ) â¢ scoping can be freely interleaved with other types of reference resolution ; â¢ unscoped or partially scoped forms are available for inference or for generation at every stage .	Background	train
In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( <CITED HERE> ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .	Uses	train
Similar approaches are being explored for parsing ( Steedman , <CITED HERE> ; Hwa et al. 2003 ) .	Background	train
In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; <CITED HERE> ) .	Background	train
would be chunked as follows ( Tjong Kim <CITED HERE> ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP	Background	train
The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( <CITED HERE> ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .	Background	train
5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( <CITED HERE> ) , which we leave for future work .	Future	train
Such approaches have been tried recently in restricted cases ( <CITED HERE> ; Eisner , 2001b ; Lafferty et al. , 2001 ) .	Background	train
Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( <CITED HERE>a ) and ( Meurers and Minnen , 1997 ) .	Extends	train
It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; <CITED HERE> ; Delic and Lahaix 1998 ) .	Background	train
A more subtle example is weighted FSAs that approximate PCFGs ( <CITED HERE> ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .	Background	train
ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( <CITED HERE> ; Kouylekov et al. , 2009 ) .	Background	train
The Ruby on <CITED HERE> framework permits us to quickly develop web applications without rewriting common functions and classes .	Uses	train
We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of <CITED HERE> .	Uses	train
Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; <CITED HERE> ; Kwon et al. , 2006 ) .	Background	train
<CITED HERE> has made some preliminary attempt on the idea of hierarchical semantic	Background	train
<CITED HERE> pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .	Background	train
Some of the intuitions we associate with this notion have been very well expressed by <CITED HERE> , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .	Background	train
In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( <CITED HERE> ) .	Background	train
The head words can be automatically extracted using a heuristic table lookup in the manner described by <CITED HERE> .	Uses	train
Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( <CITED HERE> ) .	Background	train
For example , the suite of LT tools ( <CITED HERE> ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .	Background	train
Liu et al. ( 2005 ) , <CITED HERE> , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .	Background	train
Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( <CITED HERE> ; Bergsma et al. , 2008 ) .	Future	train
The RenTAL system is implemented in LiLFeS ( <CITED HERE> ) 2 .	Uses	train
In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( <CITED HERE> ) .	Future	train
Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( <CITED HERE> ) is used to segment the phrasal lexicon into a `` marker lexicon . ''	Uses	train
One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; <CITED HERE> ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) .	Uses	train
Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( <CITED HERE> ; Marcu and Echihabi , 2002 ) .	CompareOrContrast	train
`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; <CITED HERE> ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .	CompareOrContrast	train
Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes <CITED HERE> , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .	Background	train
First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( <CITED HERE> ; Appelt et al. , 1993 ) .	Background	train
In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( <CITED HERE> ) .	Background	train
<CITED HERE> developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last	Extends	train
Riehemann 1993 ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; <CITED HERE> ) .	CompareOrContrast	train
Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( <CITED HERE> ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .	Motivation	train
Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; <CITED HERE> ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .	Background	train
We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in <CITED HERE> and Stockwell et al. ( 1973 ) .	Uses	train
The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( <CITED HERE> ) .	CompareOrContrast	train
Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( <CITED HERE> ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .	Background	train
The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; <CITED HERE> ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .	Background	train
Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by <CITED HERE>a ) .	Background	train
This includes work on generalized expectation ( <CITED HERE> ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .	Background	train
A very similar formulation , for another grammar transformation , is given in <CITED HERE> .	CompareOrContrast	train
We run GIZA + + ( <CITED HERE> ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .	Uses	train
Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( <CITED HERE> ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .	Background	train
Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , <CITED HERE> ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .	Background	train
This observation has led some researchers , e.g. , <CITED HERE> , to claim a direct mapping between the syntactic phrase and the prosodic phrase .	Background	train
These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; <CITED HERE> ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .	Motivation	train
In our previous work ( <CITED HERE> ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .	Extends	train
Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , <CITED HERE> , and Joachims ( 2003 ) .	Background	train
In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( <CITED HERE> ; Misra and Walker , 2013 ) .	Motivation	train
<CITED HERE> compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .	Background	train
Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; <CITED HERE> ) .	Background	train
The dialogue state is represented by a cumulative answer analysis which tracks , over multiple turns , the correct , incorrect , and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well ( <CITED HERE> ) .	Future	train
As noted above , it is well documented ( <CITED HERE> ) that subcategorization frames ( and their frequencies ) vary across domains .	Motivation	train
<CITED HERE> explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .	Background	train
At the same time , we believe our method has advantages over the approach developed initially at IBM ( <CITED HERE> ; Brown et al. 1993 ) for training translation systems automatically .	CompareOrContrast	train
Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( <CITED HERE> ) .	Background	train
The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; <CITED HERE> ) .	Background	train
As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( <CITED HERE> ; Hirschman and Gaizauskas 2001 ) .	CompareOrContrast	train
The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; <CITED HERE> ) .	Background	train
Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX <CITED HERE> ; Frisch 1987 ; Patel-Schneider 1985 ) .	CompareOrContrast	train
As <CITED HERE> points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage , this simple and convenComputational Linguistics , Volume 13 , Numbers 3-4 , July-December 1987 205 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing tional access strategy is perfectly adequate .	Background	train
Other factors , such as the role of focus ( <CITED HERE> , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .	Background	train
This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( <CITED HERE> ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .	Background	train
This is because the binary structure has been verified to be very effective for tree-based translation ( <CITED HERE> ; Zhang et al. , 2011a ) .	Motivation	train
There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; <CITED HERE> ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .	Background	train
measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( <CITED HERE> ) .	Uses	train
The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( <CITED HERE> ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .	Background	train
Accordingly , we convert examples such as ( 27 ) into their generalized equivalents , as in ( 28 ) : ( 28 ) <DET> good man : bon homme That is , where <CITED HERE> substitutes variables for various words in his templates , we replace certain lexical items with their marker tag .	CompareOrContrast	train
How it is done is beyond the scope of this paper but is explained in detail in <CITED HERE> .	Background	train
These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( <CITED HERE> ) .	Uses	train
Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; <CITED HERE> ) .	Background	train
The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , <CITED HERE> ) .	Background	train
<CITED HERE> has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .	Extends	train
An example of psycholinguistically oriented research work can be found in <CITED HERE> .	Background	train
In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; <CITED HERE> ) .	Background	train
Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( <CITED HERE> ) .	Background	train
There is some literature on procedure acquisition such as the LISP synthesis work described in <CITED HERE> and the PROLOG synthesis method of Shapiro ( 1982 ) .	CompareOrContrast	train
Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; <CITED HERE> ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .	Background	train
The list of semantic relations with which we work is based on extensive literature study ( <CITED HERE>a ) .	Uses	train
The research described below is taking place in the context of three collaborative projects ( <CITED HERE> ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .	Background	train
In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; <CITED HERE> ) .	Background	train
This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( <CITED HERE> ) .	Background	train
Following <CITED HERE> , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .	CompareOrContrast	train
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; <CITED HERE> ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	train
Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( <CITED HERE> , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .	Background	train
11 <CITED HERE> reports that non-projective and pseudo-projective algorithms outperform the `` eager '' projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .	CompareOrContrast	train
In particular , boosting ( Schapire , 1999 ; <CITED HERE> ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .	Future	train
For MT the most commonly used heuristic is called grow diagonal final ( <CITED HERE> ) .	CompareOrContrast	train
ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( <CITED HERE> ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .	Uses	train
In order to obtain semantic representations of each word , we apply our previous strategy ( <CITED HERE> ) .	Extends	train
<CITED HERE> annotated a larger set of word pairs ( 353 ) , too .	Background	train
A more flexible approach is used by <CITED HERE> , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .	Background	train
Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( <CITED HERE> ) , were automatically logged , derived , or manually annotated .	Uses	train
<CITED HERE> propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .	Background	train
We evaluated our translations with IBM 's BLEU evaluation metric ( <CITED HERE> ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) .	Uses	train
Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( <CITED HERE> ) .	Background	train
PR is closely related to the work of <CITED HERE> , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .	Background	train
The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( <CITED HERE> ) .	Uses	train
de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( <CITED HERE> ) or the Complement Cliticization Lexical Rule ( Miller and Sag 1993 ) to operate on those raised elements .	Background	train
A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; <CITED HERE> , 1998b ) .	Background	train
ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( <CITED HERE> ) .	Background	train
mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; <CITED HERE>a ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .	Extends	train
The description of the EAGLE workbench for linguistic engineering ( <CITED HERE> ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .	CompareOrContrast	train
An exception is <CITED HERE> , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .	Background	train
Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( <CITED HERE>b ) .	CompareOrContrast	train
The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( <CITED HERE> ) .	Uses	train
Many statistical parsers ( Ratnaparkhi , 1999 ; <CITED HERE> ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .	Background	train
In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by <CITED HERE> slightly outperforms 02 .	Background	train
For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( <CITED HERE> ) .	Background	train
This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( <CITED HERE> , p. 329 ) .	Motivation	train
This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( <CITED HERE> ) .	Background	train
In <CITED HERE> and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .	Background	train
For example , the forward-backward algorithm ( <CITED HERE> ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .	Background	train
ment ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; <CITED HERE> ) .	Background	train
<CITED HERE> argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .	Motivation	train
Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( <CITED HERE> ) .	Background	train
Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( <CITED HERE> ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .	Background	train
Burkett and Klein ( 2008 ) and <CITED HERE> focused on joint parsing and alignment .	CompareOrContrast	train
But the general outlines are reasonably clear , and we can adapt some of the UDRS ( <CITED HERE> ) work to our own framework .	Uses	train
KUbler , McDonald , and <CITED HERE> describe a `` typical '' MaltParser model configuration of attributes and features .13 Starting with it , in a series of initial controlled experiments , we settled on using buf [ 0-1 ] + stk [ 0-1 ] for word-forms , and buf [ 0-3 ] + stk [ 0-2 ] for POS tags .	Uses	train
For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in <CITED HERE> .	Uses	train
The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; <CITED HERE> ; Ely et al. , 2005 ) .	Background	train
This approach is taken in computational syntactic grammars ( e.g. <CITED HERE> ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .	Background	train
Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , <CITED HERE> and Xue ( 2008 ) .	Background	train
For example , the interaction of lexical rules is explored at run-time , even though the possible interaction can be determined at compile-time given the information available in the lexical rules and the base lexical entries .2 Based on the research results reported in <CITED HERE> , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these shortcomings and results in a more efficient processing of lexical rules as used in HPSG .	Motivation	train
`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( <CITED HERE> ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .	CompareOrContrast	train
On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in <CITED HERE> .	CompareOrContrast	train
In addition , the advantages of using linguistically annotated data over raw data are well documented ( <CITED HERE> ; Granger and Rayson , 1998 ) .	Background	train
For the A * algorithm ( <CITED HERE> ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .	Uses	train
Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by <CITED HERE> .	Uses	train
The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and <CITED HERE> ) .	Extends	train
Nevertheless , <CITED HERE> , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''	Background	train
Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and <CITED HERE> , inter alia .	Background	train
( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; <CITED HERE> ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .	CompareOrContrast	train
<CITED HERE> utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .	CompareOrContrast	train
Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and <CITED HERE> ] , which is still considered one of the best smoothing methods for n-gram language models .	Uses	train
Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( <CITED HERE> ) with gold constituency parsing information and gold named entity information .	Uses	train
Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( <CITED HERE> ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .	CompareOrContrast	train
We have presented an ensemble approach to word sense disambiguation ( <CITED HERE> ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .	Background	train
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; <CITED HERE> ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	Background	train
We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by <CITED HERE> .	Extends	train
Nevertheless , the full document text is present in most systems , sometimes as the only feature ( <CITED HERE> ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .	Background	train
The recognizer for these systems is the SUMMIT system ( <CITED HERE> ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .	Uses	train
Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( <CITED HERE> ; Krahmer and Theune 2002 ) .	CompareOrContrast	train
To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by <CITED HERE> that samples entire parse trees .	Uses	train
Semantic construction proceeds from the derived tree ( <CITED HERE> ) rather than -- as is more common in TAG -- from the derivation tree .	Background	train
It allows the construction of a non-TAL ( <CITED HERE> ) , ( Harbusch & Poller , 2000 ) .	Background	train
Following previous work ( e.g. , <CITED HERE> and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .	Uses	train
TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( <CITED HERE> ) .	Background	train
The gap mechanism resembles the Hold register idea of ATNs ( <CITED HERE> ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . )	CompareOrContrast	train
To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; <CITED HERE> ) for our models .	Uses	train
Typical examples are Bulgarian ( Simov et al. , 2005 ; <CITED HERE> ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .	Background	val
This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in <CITED HERE> .	CompareOrContrast	val
These observations and this line of reasoning has not escaped the attention of theoretical linguists : <CITED HERE> propose that argument structure is , in fact , encoded syntactically .	Background	val
inter-document references in the form of hyperlinks ( <CITED HERE> ) .	Background	val
While IA is generally thought to be consistent with findings on human language production ( <CITED HERE> ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .	Background	val
Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; <CITED HERE> ) .	Background	val
Secondly , as ( <CITED HERE> ) show , marginalizing out the different segmentations during decoding leads to improved performance .	Future	val
Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( <CITED HERE> ) , trained on the PADT .	Background	val
More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( <CITED HERE> ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .	Background	val
The grammar conversion from LTAG to HPSG ( <CITED HERE> ) is the core portion of the RenTAL system .	Background	val
This includes work on question answering ( <CITED HERE> ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .	Background	val
Our knowledge extractors rely extensively on MetaMap ( <CITED HERE> ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .	Uses	val
We study the cases where a 9Recall that even the <CITED HERE> system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .	CompareOrContrast	val
Inspired by ( Blunsom et al. , 2009 ) and ( <CITED HERE> ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .	Motivation	val
Inspired by ( <CITED HERE> ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .	Motivation	val
The TNT POS tagger ( <CITED HERE> ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .	Background	val
Some previous works ( Bannard and Callison-Burch , 2005 ; <CITED HERE> ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .	Background	val
Similarly , <CITED HERE> report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .	Background	val
To solve these scaling issues , we implement Online Variational Bayesian Inference ( <CITED HERE> ; Hoffman et al. , 2012 ) for our models .	Uses	val
Other studies which view lR as a query generation process include <CITED HERE> ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .	CompareOrContrast	val
As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( <CITED HERE> ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .	Background	val
There have been many studies on parsing techniques ( <CITED HERE> ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environ -	Background	val
An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( <CITED HERE> ) .	Background	val
For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( <CITED HERE> ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .	Background	val
Later works , such as <CITED HERE>a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .	Background	val
results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( <CITED HERE> ) .	Uses	val
We work with a semi-technical text on meteorological phenomena ( <CITED HERE> ) , meant for primary school students .	Uses	val
This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , <CITED HERE> , and implicitly or explicitly by almost all researchers in computational linguistics .	CompareOrContrast	val
The candidate feature templates include : Voice from <CITED HERE> .	Uses	val
Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; <CITED HERE>a ) .	Background	val
The PICO framework ( <CITED HERE> ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .	Background	val
The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( <CITED HERE> ) .	Uses	val
Furthermore , manually selected word pairs are often biased towards highly related pairs ( <CITED HERE> ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .	Background	val
Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( <CITED HERE> ) .	Uses	val
Our strategy is based on the approach presented by <CITED HERE> .	Uses	val
Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( <CITED HERE> ) for a review ) , these patterns allow :	Background	val
A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( <CITED HERE> , pages 10 -- 12 ) .	Background	val
The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della <CITED HERE> ; Chen and	Uses	val
Both kinds of annotation were carried out using ANVIL ( <CITED HERE> ) .	Uses	val
The article classifier is a discriminative model that draws on the state-of-the-art approach described in <CITED HERE> .	Uses	val
The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( <CITED HERE> ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .	Background	val
<CITED HERE> , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''	Background	val
We follow the notation convention of <CITED HERE> .	Uses	val
Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and <CITED HERE>b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .	Background	val
<CITED HERE> did not report inter-subject correlation for their larger dataset .	CompareOrContrast	val
This is a similar conclusion to our previous work in <CITED HERE> .	CompareOrContrast	val
We then use the program Snob ( <CITED HERE> ; Wallace 2005 ) to cluster these experiences .	Uses	val
The priorities are used for disambiguating interpretation in the incremental understanding method ( <CITED HERE>b ) .	Uses	val
A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; <CITED HERE> ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .	Background	val
<CITED HERE> present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .	Background	val
We use the open-source Moses toolkit ( <CITED HERE> ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .	Uses	val
This approach is taken , for example , in LKB ( <CITED HERE> ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .	CompareOrContrast	val
This confirms that although Kozima 's approach ( <CITED HERE> ) is computationally expensive , it does produce more precise segmentation .	CompareOrContrast	val
<CITED HERE> recently described a hybrid method for finding abbreviations and their definitions .	Background	val
More specifically , the notion of the phrasal lexicon ( used first by <CITED HERE> ) has been used successfully in a number of areas :	Background	val
<CITED HERE> run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .	Background	val
This section , which elaborates on preliminary results reported in <CITED HERE> , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .	Extends	val
It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX <CITED HERE>b ) .	CompareOrContrast	val
<CITED HERE> asked subjects to identify the target of a vague description in a visual scene .	Background	val
Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( <CITED HERE> ) , despite being a single generative PCFG .	CompareOrContrast	val
Our recovery policy is modeled on the TargetedHelp ( <CITED HERE> ) policy used in task-oriented dialogue .	Extends	val
It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( <CITED HERE> ) .	CompareOrContrast	val
` See ( <CITED HERE> ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .	Background	val
Specifically , we used Decision Graphs ( <CITED HERE> ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .	Uses	val
There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; <CITED HERE> ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .	Background	val
ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( <CITED HERE> ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .	Background	val
In the latter case , we can also take care of transferring the value of z. However , as discussed by <CITED HERE> , creating several instances of lexical rules can be avoided .	Motivation	val
It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( <CITED HERE> ) .	Background	val
A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and <CITED HERE> .	Uses	val
We found that the oldest system ( <CITED HERE> ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .	Background	val
Other definitions of predicates may be found in ( <CITED HERE> ) .	Background	val
For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( <CITED HERE> ) .	CompareOrContrast	val
The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or <CITED HERE> are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .	Background	val
Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( <CITED HERE> ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .	Extends	val
If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( <CITED HERE> ) .6 Pairs containing such words are not suitable for evaluation .	Background	val
The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , <CITED HERE> ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .	Background	val
Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( <CITED HERE> ) in the prediction of association norms .	Background	val
Although not the first to employ a generative approach to directly model content , the seminal work of <CITED HERE> is a noteworthy point of reference and comparison .	CompareOrContrast	val
Others include selectional preferences , transitivity ( <CITED HERE> ) , mutual exclusion , symmetry , etc. .	Background	val
â¢ cross-language information retrieval ( e.g. , <CITED HERE> ) , â¢ multilingual document filtering ( e.g. , Oard 1997 ) , â¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,	Background	val
For example , ( <CITED HERE> ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .	Background	val
As stated before , the experiments are run in the ACE '04 framework ( <CITED HERE> ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .	Uses	val
Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( <CITED HERE> ) .	Background	val
or quotation of messages in emails or postings ( see <CITED HERE> but cfXXX Agrawal et al. ( 2003 ) ) .	Background	val
The first work to do this with topic models is <CITED HERE>b ) .	Background	val
The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; <CITED HERE> ) .	CompareOrContrast	val
Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( <CITED HERE> ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .	Background	val
For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( <CITED HERE> ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .	Future	val
The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by <CITED HERE> that the G2 statistic is better suited for use in corpus-based NLP .	CompareOrContrast	val
Provided with the candidate fragment elements , we previously ( <CITED HERE> ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .	Extends	val
There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; <CITED HERE> ) .	Background	val
This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( <CITED HERE>b ) .	CompareOrContrast	val
One important example is the constituentcontext model ( CCM ) of <CITED HERE> , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .	Background	val
Japanese ( <CITED HERE> ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .	CompareOrContrast	val
7 We ignore the rare `` false idafa '' construction ( <CITED HERE> , p. 102 ) .	Background	val
Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( <CITED HERE> ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .	Background	val
Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( <CITED HERE> ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .	Background	val
We follow <CITED HERE> in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .	Uses	val
( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see <CITED HERE>a ) ) .	Motivation	val
Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in <CITED HERE> ( henceforth G&G ) .	CompareOrContrast	val
Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( <CITED HERE> ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .	CompareOrContrast	val
For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( <CITED HERE> , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers	Uses	val
Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; <CITED HERE> ; Mitkov , Belguith , and Stys 1998 ) .	Background	val
We use the same data setting with Xue ( 2008 ) , however a bit different from <CITED HERE> .	CompareOrContrast	val
They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( <CITED HERE> ) , and TE ( Dinu and Wang , 2009 ) .	Motivation	val
Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( <CITED HERE> ) .	Background	val
Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( <CITED HERE>a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .	Future	val
Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; <CITED HERE> ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	Background	val
Future research should apply the work of Blunsom et al. ( 2008 ) and <CITED HERE> , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .	Future	val
We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( <CITED HERE> ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .	Uses	val
<CITED HERE>	Background	val
OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of `` simple constraints '' ( e.g. , <CITED HERE>b ) is of course an empirical question .	Background	val
Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of <CITED HERE> , ch .	Background	val
This result is consistent with other works using this model with these features ( <CITED HERE> ; Silberer and Lapata , 2012 ) .	CompareOrContrast	val
<CITED HERE> reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .	CompareOrContrast	test
Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; <CITED HERE> ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .	Background	test
But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( <CITED HERE> ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .	Motivation	test
Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; <CITED HERE> ) or explicit	Background	test
However , the method we are currently using in the ATIS domain ( <CITED HERE> ) represents our most promising approach to this problem .	Uses	test
Henceforth the collaborative traits of blogs and wikis ( <CITED HERE> ) emphasize annotation , comment , and strong editing .	Background	test
The ICA system ( <CITED HERE> ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .	Background	test
To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; <CITED HERE> ) .	Background	test
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; <CITED HERE> ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	test
Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , <CITED HERE> , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .	Background	test
Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( <CITED HERE> ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .	Background	test
Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( <CITED HERE> ) .	CompareOrContrast	test
The advantage of tuning similarity to the application of interest has been shown previously by <CITED HERE> .	CompareOrContrast	test
Although there are other discussions of the paragraph as a central element of discourse ( e.g. <CITED HERE> , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .	CompareOrContrast	test
Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; <CITED HERE> ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	Background	test
We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( <CITED HERE> ) and NE classification ( Collins and Singer , 1999 ) .	Motivation	test
A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( <CITED HERE> ; Knight and Graehl , 1998 ) .	Background	test
We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; <CITED HERE> ; Zaidan et al. , 2007 ) .	Uses	test
Our classification framework , directly inspired by <CITED HERE> , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .	Uses	test
As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; <CITED HERE> ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .	Background	test
For instance , <CITED HERE> report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .	CompareOrContrast	test
One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( <CITED HERE> ) .	Future	test
Later works , such as Atallah et al. ( 2001a ) , <CITED HERE> , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .	Background	test
A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , <CITED HERE> , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .	CompareOrContrast	test
The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; <CITED HERE> ; Bod , 2001 ) .	CompareOrContrast	test
The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( <CITED HERE> ; Clark et al. , 2003 ) .	Background	test
This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; <CITED HERE> ) .	Background	test
Training was done on the Penn Treebank ( <CITED HERE> ) Wall Street Journal data , sections 02-21 .	Uses	test
We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( <CITED HERE> ) to measure the relatedness between words in the dataset .	Uses	test
For example , our previous work ( <CITED HERE> ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .	CompareOrContrast	test
Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( <CITED HERE> ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .	Background	test
Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; <CITED HERE>a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	Background	test
<CITED HERE> replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .	Background	test
One approach to this more general problem , taken by the ` Nitrogen ' generator ( <CITED HERE>a ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .	Uses	test
where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( <CITED HERE> ) .	Uses	test
13 We also employed sequence-based measures using the ROUGE tool set ( <CITED HERE> ) , with similar results to those obtained with the word-by-word measures .	Uses	test
Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( <CITED HERE> ) .	Background	test
In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( <CITED HERE> ) .	Uses	test
The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( <CITED HERE> ) , possibly arranged in an inheritance hierarchy .	CompareOrContrast	test
Later , <CITED HERE> , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .	Background	test
Another technique is automatic discovery of translations from parallel or non-parallel corpora ( <CITED HERE> ) .	Background	test
ASARES is presented in detail in ( <CITED HERE> ) .	Uses	test
Opposition ( called `` adversative '' or `` contrary-to-expectation '' by <CITED HERE> ; cfXXX also Quirk et al. 1972 , p. 672 ) .	Background	test
A number of applications have relied on distributional analysis ( <CITED HERE> ) in order to build classes of semantically related terms .	Background	test
Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; <CITED HERE> ) .	CompareOrContrast	test
Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by <CITED HERE> , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .	CompareOrContrast	test
Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; <CITED HERE> ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000 ;	CompareOrContrast	test
The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; <CITED HERE> ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .	Background	test
In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( <CITED HERE> ) 5 .	CompareOrContrast	test
There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research <CITED HERE> ) .	Background	test
Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of <CITED HERE> , who report large speed-ups from the elimination of disjunction processing during unification .	CompareOrContrast	test
For the task of unsupervised dependency parsing , <CITED HERE> add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .	Background	test
The speech and language processing architecture is based on that of the SRI CommandTalk system ( <CITED HERE> ; Stent et a. , 1999 ) .	Uses	test
Second , in line with the findings of ( <CITED HERE> ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .	CompareOrContrast	test
Therefore , inter-subject correlation is lower than the results obtained by <CITED HERE> .	CompareOrContrast	test
There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport <CITED HERE> ) .	Background	test
For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( <CITED HERE> ) .	Background	test
The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( <CITED HERE> ) .	CompareOrContrast	test
successfully parses , or until a quitting criterion is reached , such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( <CITED HERE> ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .	Uses	test
<CITED HERE> substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .	CompareOrContrast	test
Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( <CITED HERE> ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .	Background	test
The paradigm is `` write many , read many '' ( <CITED HERE> ) .	Background	test
The Praat tool was used ( <CITED HERE> ) .	Uses	test
2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( <CITED HERE> ; also Section 8.1 of the present article ) .	Background	test
The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( <CITED HERE> ; Clark et al. , 2003 ) .	Motivation	test
Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; <CITED HERE> ) .	Future	test
Our work is inspired by the latent left-linking model in <CITED HERE> and the ILP formulation from Chang et al. ( 2011 ) .	Uses	test
Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( <CITED HERE> ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .	Background	test
The names given to the components vary ; they have been called `` strategic '' and `` tactical '' components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , `` planning '' and `` realization '' ( e.g. , McDonald 1983 ; <CITED HERE>a ) , or simply `` what to say '' versus `` how to say it '' ( e.g. , Danlos 1987 ; Reithinger 1990 ) .	Background	test
Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; <CITED HERE> ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	Background	test
We use the TRIPS dialogue parser ( <CITED HERE> ) to parse the utterances .	Uses	test
In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( <CITED HERE>a ) .	Uses	test
The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( <CITED HERE>b ) , which is an integrated parsing and discourse processing method .	Uses	test
We applied our system to the XTAG English grammar ( The XTAG Research <CITED HERE> ) 3 , which is a large-scale FB-LTAG grammar for English .	Uses	test
After the extraction , pruning techniques ( <CITED HERE> ) can be applied to increase the precision of the extracted paraphrases .	Background	test
In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , <CITED HERE> ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .	Background	test
GATE goes beyond earlier systems by using a component-based infrastructure ( <CITED HERE> ) which the GUI is built on top of .	Background	test
Since sentences can refer to events described by other sentences , we may need also a quotation operator ; <CITED HERE> describes how first order logic can be augmented with such an operator .	Background	test
The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; <CITED HERE> ) to represent the state of the world .	Uses	test
A possible future direction would be to compare the query string to retrieved results using a method similar to that of <CITED HERE> .	Future	test
description-level lexical rules ( DLRs ; <CITED HERE> ) .5 2.2.1 Meta-Level Lexical Rules .	Background	test
All EBMT systems , from the initial proposal by <CITED HERE> to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .	Background	test
The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , <CITED HERE> , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .	Background	test
In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; <CITED HERE> ;	CompareOrContrast	test
These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , <CITED HERE> ) .	Motivation	test
Other psycholing-uistic studies that confirm the validity of paragraph units can be found in <CITED HERE> and Haberlandt et al. ( 1980 ) .	Background	test
The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; <CITED HERE> ) .	CompareOrContrast	test
Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; <CITED HERE> ) - .	Background	test
In a similar vain to <CITED HERE> and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .	Future	test
As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , <CITED HERE> ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .	Background	test
We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( <CITED HERE> ) and the perceptron POS tagging model from Collins ( 2002 ) .	Extends	test
Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( <CITED HERE> ) .	Future	test
Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( <CITED HERE> ) .	Background	test
Previously ( <CITED HERE> ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .	Extends	test
To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of <CITED HERE> .	Uses	test
For instance , <CITED HERE> , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''	Motivation	test
Semantic Role labeling ( SRL ) was first defined in <CITED HERE> .	Background	test
AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( <CITED HERE> ) .	Background	test
The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and <CITED HERE> .	Background	test
The most detailed evaluation of link tokens to date was performed by ( <CITED HERE> ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .	CompareOrContrast	test
Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( <CITED HERE> ) .	CompareOrContrast	test
While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( <CITED HERE> ) :	Background	test
Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( <CITED HERE> ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .	Background	test
For shuffling paraphrases , french alternations are partially described in ( <CITED HERE> ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .	Background	test
A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( <CITED HERE> ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )	Background	test
<CITED HERE> combines lexical and dependency mappings to form his generalizations .	Background	test
Thus for instance , ( Copestake and Flickinger , 2000 ; <CITED HERE> ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .	Background	test
The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; <CITED HERE> ) .	Extends	test
And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in <CITED HERE> who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .	Motivation	test
In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; <CITED HERE> ) .	Background	test
criteria and data used in our experiments are based on the work of <CITED HERE> .	Uses	test
We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( <CITED HERE> ) .	Uses	test
â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; <CITED HERE> ) .	CompareOrContrast	test
And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( <CITED HERE> ) .	Background	test
â¢ Support vector machines for mapping histories to parser actions ( <CITED HERE> ) .	Uses	test
<CITED HERE> , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .	Background	test
<CITED HERE> avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .	Background	test
<CITED HERE> have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .	Background	test
Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; <CITED HERE>b ; Lafferty et al. , 2001 ) .	Background	test
The relation between discourse and prosodic phrasing has been examined in some detail by <CITED HERE> , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .	Background	test
By contrast , Turkish ( Oflazer et al. , 2003 ; <CITED HERE> ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .	CompareOrContrast	test
The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; <CITED HERE> ) .	Background	test
Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( <CITED HERE> ) .	Uses	test
Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and <CITED HERE> .	Background	test
Our rules for phonological word formation are adopted , for the most part , from G & G , <CITED HERE> , and the account of monosyllabic destressing in Selkirk ( 1984 ) .	Uses	test
As a generalization , <CITED HERE> notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .	Background	test
Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; <CITED HERE> ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .	Background	test
Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; <CITED HERE> ) or distributional ( Weeds and Weir , 2005 ) .	Background	test
Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; <CITED HERE> ) .	Background	test
The question answering system developed by <CITED HERE> belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .	CompareOrContrast	test
More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( <CITED HERE> ) , and ( Lewis et al. , 2004 ) .	Background	test
Discriminant analysis has been employed by researchers in automatic text genre detection ( <CITED HERE>b ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .	Background	test
This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( <CITED HERE> ; Silberer and Lapata , 2012 ) .	Extends	test
In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( <CITED HERE> ; see our Section 2 ) .	Background	test
Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ <CITED HERE> ; Chiang et al. 2005 ] ) as well as for	Background	test
Following <CITED HERE> , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .	Uses	test
In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; <CITED HERE> ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	Background	test
We have shown elsewhere ( Jensen and Binot 1988 ; <CITED HERE>a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .	Extends	test
Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to <CITED HERE> 's conditional probability scores for pseudodisambiguation of ( v , n , n â² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .	Motivation	test
