{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/scratch/petros/anaconda3/envs/testenv/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd;\n",
    "# from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# import torchtext\n",
    "import random;\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from transformers import *\n",
    "import spacy\n",
    "from ipywidgets import widgets\n",
    "import torchtext\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch;\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_file = \"/export/scratch/saurav/kim/oct_20_new_data/dec_12_annotations.tsv\"\n",
    "output_folder = \"/export/scratch/saurav/kim/scibert/jan_10/\"\n",
    "token_fite = \"/export/scratch/petros/tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = pd.read_csv(annotated_file, sep='\\t');\n",
    "annotated_data = annotated_data.replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not sure 379\n"
     ]
    }
   ],
   "source": [
    "indices = [];\n",
    "labels = [];\n",
    "cited_title_list = [];\n",
    "citing_title_list = [];\n",
    "cited_abstract_list = [];\n",
    "citing_abstract_list = [];\n",
    "citation_context_list = [];\n",
    "for i in range(len(annotated_data.index)):\n",
    "    if annotated_data['new annotation'][i] is None:\n",
    "        if annotated_data['previous annotation'][i] is None:\n",
    "            print(\"None\", i)\n",
    "            continue;\n",
    "        label = str(annotated_data['previous annotation'][i]).strip().lower();\n",
    "    else:\n",
    "        label = str(annotated_data['new annotation'][i]).strip().lower();\n",
    "    if label == 'used' or label == 'not used' or label == 'extended':\n",
    "        labels.append(label);\n",
    "        indices.append(i);\n",
    "        cited_title_list.append(str(annotated_data.loc[i, 'cited title']))\n",
    "        citing_title_list.append(str(annotated_data.loc[i, 'citing title']))\n",
    "        cited_abstract_list.append(str(annotated_data.loc[i, 'cited abstract']))\n",
    "        citing_abstract_list.append(str(annotated_data.loc[i, 'citing abstract']))\n",
    "        # Since there multiple citation contexts, I've separated the with ---(j)--- in the spreadsheet, should ideally use regex to split them, here is a random way\n",
    "        citing_contexts = str(annotated_data.loc[i, 'citation context']).split('---(1)---')[1].strip();\n",
    "        temp_citation_context_list = [];\n",
    "        j = 2;\n",
    "        while True:\n",
    "            citing_contexts = citing_contexts.split('---('+str(j)+')---');\n",
    "            temp_citation_context_list.append(citing_contexts[0].strip());\n",
    "            if len(citing_contexts) == 1:\n",
    "                break;\n",
    "            citing_contexts = citing_contexts[1].strip();\n",
    "            j += 1;\n",
    "        citation_context_list.append(temp_citation_context_list);\n",
    "            \n",
    "    else:\n",
    "        print(label, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = labels\n",
    "labels = [0]*len(text_labels);\n",
    "for i in range(len(text_labels)):\n",
    "    if text_labels[i]=='used':\n",
    "        labels[i] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_sci_lg', disable=[\"tagger\", \"parser\", \"textcat\", \"ner\", \"lemmatizer\"])\n",
    "#Note that we use r, gule-based segmenter, first I tried 'combined_rule_sentence_segmenter', \n",
    "# but that gave some errors, so followed the suggestions here: https://github.com/allenai/scispacy/issues/207\n",
    "nlp.add_pipe('sentencizer')\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'), first=True)\n",
    "\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "config = BertConfig.from_pretrained('allenai/scibert_scivocab_uncased', output_hidden_states=True)\n",
    "bert_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=config).to(device)\n",
    "\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(tokenizer, string):\n",
    "    x = nlp(string);\n",
    "    tokens = tokenizer(string ,return_tensors=\"pt\", max_length=512)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readout_index(tokens):\n",
    "    cited_here_tokens = torch.tensor([962, 8412, 1530, 1374])\n",
    "    readout_index = []\n",
    "    \n",
    "    l = tokens['input_ids'].size(1)\n",
    "    for i in range(1, l - 4):\n",
    "        if torch.equal(tokens['input_ids'][0, i:i+4], cited_here_tokens):\n",
    "            readout_index.append(torch.arange(i, i+4))\n",
    "    return torch.cat(readout_index)\n",
    "\n",
    "def get_readout_mask(tokens, readout_index):\n",
    "    mask = torch.zeros_like(tokens['input_ids'], dtype=torch.bool)\n",
    "    mask[0, readout_index] = True\n",
    "    return mask\n",
    "\n",
    "def get_context_tokens(tokenizer, string):\n",
    "    # get the tokens\n",
    "    tokens = tokenizer(string ,return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    tokens['readout_mask'] = get_readout_mask(tokens, get_readout_index(tokens))\n",
    "    return tokens\n",
    "\n",
    "def readout(bert_result, readout_type, readout_mask=None, pooling=None):\n",
    "    bert_dims = bert_result.last_hidden_state.size(-1)\n",
    "    \n",
    "    if readout_type == 'cls':\n",
    "        return bert_result.last_hidden_state[0, 0]\n",
    "    elif readout_type == 'mean':\n",
    "        return bert_result.last_hidden_state[0].mean(0)\n",
    "    elif readout_type == 'ch':\n",
    "        if readout_mask is not None:\n",
    "            result = bert_result.last_hidden_state[readout_mask].view(-1, 4, bert_dims).mean(1)\n",
    "            if pooling == 'max':\n",
    "                return result.max(0).values\n",
    "            elif pooling == 'mean':\n",
    "                return result.mean(0)\n",
    "            elif pooling == 'sum':\n",
    "                return result.sum(0)\n",
    "            else:\n",
    "                raise ValueError('Pooling {} is not supported.'.format(pooling))\n",
    "        else:\n",
    "            raise ValueError('Cited Here readout requires readout_index.')\n",
    "    else:\n",
    "        raise ValueError('Readout type {} is not supported.'.format(readout_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cited_title_bert = torch.zeros(len(cited_title_list), 768).float().to(device);\n",
    "# # cited_abstract_bert = torch.zeros(len(cited_abstract_list), 768).float().to(device);\n",
    "# # citing_title_bert = torch.zeros(len(citing_title_list), 768).float().to(device);\n",
    "# # citing_abstract_bert = torch.zeros(len(citing_abstract_list), 768).float().to(device);\n",
    "# # citation_context_bert = torch.zeros(len(context_mapping), 768).float().to(device);\n",
    "# cited_title_token = []\n",
    "# cited_abstract_token = []\n",
    "# citing_title_token = []\n",
    "# citing_abstract_token = []\n",
    "# citation_context_token = []\n",
    "# for i in range(len(cited_title_list)):\n",
    "#     cited_title_token.append(get_tokens(tokenizer, cited_title_list[i]));\n",
    "# for i in range(len(cited_abstract_list)):\n",
    "#     cited_abstract_token.append(get_tokens(tokenizer, cited_abstract_list[i]));\n",
    "# for i in range(len(citing_title_list)):\n",
    "#     citing_title_token.append(get_tokens(tokenizer, citing_title_list[i]));\n",
    "# for i in range(len(citing_abstract_list)):\n",
    "#     citing_abstract_token.append(get_tokens(tokenizer, citing_abstract_list[i]));\n",
    "# for i in range(len(citation_context_list)):\n",
    "#     temp = []\n",
    "#     for j in range(len(citation_context_list[i])):\n",
    "#         temp.append(get_tokens(tokenizer, citation_context_list[i][j]));\n",
    "#     citation_context_token.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(tokens, p=1.0):\n",
    "    probability_matrix = torch.full(tokens['input_ids'].shape, p)\n",
    "    mask = torch.bernoulli(probability_matrix).bool()\n",
    "    \n",
    "    # set special tokens to be True\n",
    "    mask[0,0] = True\n",
    "    mask[0,-1] = True\n",
    "    if 'readout_mask' in tokens:\n",
    "        mask[tokens['readout_mask']] = True\n",
    "    \n",
    "    tokens2 = dict()\n",
    "    tokens2['input_ids'] = tokens['input_ids'][mask].unsqueeze(0) \n",
    "    tokens2['token_type_ids'] = tokens['token_type_ids'][mask].unsqueeze(0)\n",
    "    tokens2['attention_mask'] = tokens['attention_mask'][mask].unsqueeze(0)\n",
    "    if 'readout_mask' in tokens:\n",
    "        tokens2['readout_mask'] = tokens['readout_mask'][mask].unsqueeze(0)\n",
    "    \n",
    "    return tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "cited_title_token = []\n",
    "cited_abstract_token = []\n",
    "citing_title_token = []\n",
    "citing_abstract_token = []\n",
    "citation_context_token = []\n",
    "for i in range(len(cited_title_list)):\n",
    "    cited_title_token.append(get_tokens(tokenizer, cited_title_list[i]));\n",
    "for i in range(len(cited_abstract_list)):\n",
    "    cited_abstract_token.append(get_tokens(tokenizer, cited_abstract_list[i]));\n",
    "for i in range(len(citing_title_list)):\n",
    "    citing_title_token.append(get_tokens(tokenizer, citing_title_list[i]));\n",
    "for i in range(len(citing_abstract_list)):\n",
    "    citing_abstract_token.append(get_tokens(tokenizer, citing_abstract_list[i]));\n",
    "for i in range(len(citation_context_list)):\n",
    "    temp = []\n",
    "    for j in range(len(citation_context_list[i])):\n",
    "        temp.append(get_context_tokens(tokenizer, citation_context_list[i][j]));\n",
    "    citation_context_token.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(cited_title_bert, os.path.join(output_folder, 'cited_title_tokens.tns'))\n",
    "# torch.save(cited_abstract_bert, os.path.join(output_folder, 'cited_abstract_tokens.tns'))\n",
    "# torch.save(citing_title_bert, os.path.join(output_folder, 'citing_title_tokens.tns'))\n",
    "# torch.save(citing_abstract_bert, os.path.join(output_folder, 'citing_abstract_tokens.tns'))\n",
    "# torch.save(citation_context_bert, os.path.join(output_folder, 'citation_context_tokens.tns'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1209384752;\n",
    "# seed = 1209384756\n",
    "\"\"\" 1209384756\"\"\"\n",
    "frac_list = [0.05*i for i in range(1, 20)];\n",
    "num_runs = 100;\n",
    "learning_rate = 5e-5;\n",
    "lr1 = 0.0001;\n",
    "lr2 = 0.0001;\n",
    "lr3 = 0.0001;\n",
    "lr4 = 0;\n",
    "l2_regularization = 0.0001;\n",
    "num_epochs = 100;\n",
    "patience = 5;\n",
    "batch_size = 8;\n",
    "verbose = False;\n",
    "hidden_dim = 4;\n",
    "save_postfix = \"mlp\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(labels_list, train_seed=1029384756, train_fraction=0.7, test_seed=0):\n",
    "    unique_labels = list(set(labels_list))\n",
    "    indices_list = [];\n",
    "    random.seed(test_seed);\n",
    "    for i in range(len(unique_labels)):\n",
    "        indices_list.append(np.array([_ for _ in range(len(labels_list)) if labels_list[_]==unique_labels[i]]));\n",
    "        random.shuffle(indices_list[-1])\n",
    "    \n",
    "    train_indices, valid_indices, test_indices = [], [], [];\n",
    "    \n",
    "    for i in range(len(indices_list)):\n",
    "        test_count = int(0.1 * len(indices_list[i]));\n",
    "        test_indices.append(indices_list[i][:test_count]);\n",
    "#         random.shuffle(indices_list[i][test_count:])\n",
    "     \n",
    "    \n",
    "    random.seed(train_seed);\n",
    "    for i in range(len(indices_list)):\n",
    "        random.shuffle(indices_list[i][test_count:])\n",
    "    \n",
    "    for i in range(len(indices_list)):\n",
    "        test_count = int(0.1* len(indices_list[i]));\n",
    "        train_count = int(train_fraction*(len(indices_list[i]) - test_count));\n",
    "        train_indices.append(indices_list[i][test_count:test_count+train_count]);\n",
    "        valid_indices.append(indices_list[i][test_count+train_count:])\n",
    "    \n",
    "    train_indices = np.concatenate(train_indices);\n",
    "    valid_indices = np.concatenate(valid_indices);\n",
    "    test_indices = np.concatenate(test_indices);\n",
    "    return train_indices, valid_indices, test_indices;\n",
    "\n",
    "def get_majority_votes(scores, num_classifiers):\n",
    "    final_scores = []\n",
    "    off_by = []\n",
    "    non_votes = 0\n",
    "    for i in range(len(scores[0])):\n",
    "        votes = [0,0]   \n",
    "        for j in range(num_classifiers):\n",
    "            if scores[j][i] >= 0.5:\n",
    "                votes[0] += 1;\n",
    "            elif scores[j][i] <= 0.5:\n",
    "                votes[1] += 1;\n",
    "            else:\n",
    "                non_votes += 1;\n",
    "                \n",
    "        final_scores.append(np.argmax(votes));\n",
    "        off_by.append(num_classifiers - votes[final_scores[-1]])\n",
    "        \n",
    "    return final_scores, off_by, non_votes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The sub-atmospheric pressure CO 2 adsorption isotherm at 273 K for SiC-DC1073 1 Âµm is predicted using the FWT model and the comparison between the Ar PSD-based predictions (excess theoretical adsorption isotherm) and the experimental data is shown in Figure 7 . For the predictions, the microstructure of the investigated SiC-DC samples is assumed to be rigid under this low pressure condition. CO 2 is treated as an effective Lennard-Jones (LJ) sphere for which the LJ parameters for the CO 2 -CO 2 interactions are taken from elsewhere <CITED HERE> . adsorption, the binary parameter value of k cf = 0 is used since the one-center assumption for CO 2 molecules is suitable at sub-atmospheric pressures [18] . It can be observed that the FWT model provides reasonably good agreement between the excess theoretical and experimental adsorption isotherm for the synthesized sample, indicating good approximation of the slitpore model for even very disordered carbons such as the SiC-DC. This has been also shown to satisfactorily predict the sub-atmospheric CO 2 isotherm of nano-sized SiC-DC in previous work of this laboratory [18] . The under-prediction which is observed at very', 'size of 0.32 nm, for the activation barrier of 40 kJ/mole found by them for methane diffusion in Takeda result in support is that the lower activation energies of 15.6 and 14.2 kJ/mol found here for the larger particle-scale micropores, are close to the value of 11.7 kJ/mole found by Prasetyo et al. [76] , suggesting that these pores are of a similar size range as the micropores in traditional activated carbon. This is indeed evident from the PSDs in Figure 6 , showing the pore size range to be similar to that of many commercial activated carbons [36, <CITED HERE> . The very slightly higher activation energy in the larger micropores, of 15.6 and 14.2 kJ/mol, in comparison to that obtained from simulation, of 11.1 kJ/mol, seen in Figure 14 , does indicate the possibility of some structural constrictions and internal barriers that affect macroscopic transport, but are not captured at the small length scale of simulation (~10 nm). We note here that a slightly lower activation energy from simulation, of about 7.05 kJ/mole, has been reported in our recent article [66] , but that is for a pressure of 400 mmHg and corresponds to the higher loading of']\n"
     ]
    }
   ],
   "source": [
    "print(citation_context_list[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_cuda(tokens, device):\n",
    "#     device = 'cuda'\n",
    "    dictionary = {}\n",
    "    for key, value in tokens.items():\n",
    "        dictionary[key] = value.to(device)\n",
    "    return dictionary\n",
    "\n",
    "def get_batched_token(batch_indices, device, p=1.0 ):\n",
    "    batched_cited_title = tokens_to_cuda(mask_tokens(cited_title_token[batch_indices],p), device)\n",
    "    batched_cited_abstract = tokens_to_cuda(mask_tokens(cited_abstract_token[batch_indices],p), device)\n",
    "    batched_citing_title = tokens_to_cuda(mask_tokens(citing_title_token[batch_indices],p), device)\n",
    "    batched_citing_abstract = tokens_to_cuda(mask_tokens(citing_abstract_token[batch_indices],p), device)\n",
    "#     batched_citation_context = tokens_to_cuda(citation_context_token[batch_indices], device)\n",
    "    batched_citation_context = []\n",
    "    for i in range(len(citation_context_token[batch_indices])):\n",
    "        batched_citation_context.append(tokens_to_cuda(mask_tokens(citation_context_token[batch_indices][i],p), device))\n",
    "    batched_labels = torch.LongTensor(labels)[batch_indices].unsqueeze(0).to(device);\n",
    "    \n",
    "    \n",
    "    return (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, bert, emb_dim, hidden_list, dropout=0.5, flag='mean',readout='ch', pooling='max'):\n",
    "        super().__init__()        \n",
    "        self.flag = flag\n",
    "        self.n_classes = n_classes  \n",
    "        self.n_layers = len(hidden_list)\n",
    "        self.hidden_layers = nn.ModuleList([])\n",
    "        self.batch_norm = nn.ModuleList([])\n",
    "        input_ = emb_dim*5\n",
    "        for i in range(len(hidden_list)):\n",
    "            self.hidden_layers.append(nn.Linear(input_, hidden_list[i]));\n",
    "            self.batch_norm.append(nn.BatchNorm1d(hidden_list[i]));\n",
    "            input_ = hidden_list[i];\n",
    "        self.top_layer = nn.Linear(hidden_list[-1], n_classes)\n",
    "        self.dropout = nn.Dropout(dropout);\n",
    "        self.scibert = bert;\n",
    "        self.readout = readout;\n",
    "        self.pooling = pooling;\n",
    "    \n",
    "    \n",
    "    def forward(self, cited_title_token, cited_abstract_token, citing_title_token, citing_abstract_token, citation_context_token):\n",
    "#         tuple\n",
    "        cited_tilte = self.scibert(**cited_title_token)\n",
    "        cited_abstract = self.scibert(**cited_abstract_token)\n",
    "        citing_tilte = self.scibert(**citing_title_token)\n",
    "        citing_abstract = self.scibert(**citing_abstract_token)\n",
    "       \n",
    "        citation_context = []\n",
    "        for i in range(len(citation_context_token)):\n",
    "            readout_index = citation_context_token[i].pop('readout_mask')\n",
    "            citation_context.append(readout(self.scibert(**citation_context_token[i]), readout_type=self.readout, readout_mask=readout_index, pooling=self.pooling))\n",
    "\n",
    "#             citation_context.append(self.scibert(**citation_context_token[i]).last_hidden_state[:,0])\n",
    "\n",
    "        citation_context = torch.stack(citation_context, dim=0)\n",
    "        if self.flag=='sum':\n",
    "            citation_context = citation_context.sum(dim=0,keepdims=True)\n",
    "        elif self.flag =='max':\n",
    "            citation_context = citation_context.max(dim=0, keepdims=True)\n",
    "        elif self.flag == 'mean':\n",
    "            citation_context = citation_context.mean(dim=0, keepdims=True)    \n",
    "        elif self.flag == 'topk':\n",
    "            topk = citation_context.topk(10,dim=0, keepdims=True)    \n",
    "            citation_context = topk[0].mean(dim=0, keepdims=True)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        input_emb = torch.cat([cited_abstract.last_hidden_state[:,0], citing_abstract.last_hidden_state[:,0], cited_abstract.last_hidden_state[:,0]*citation_context, citing_abstract.last_hidden_state[:,0]*citation_context, citation_context], dim=1)\n",
    "        hidden = self.dropout(torch.tanh(self.hidden_layers[0](input_emb)))\n",
    "\n",
    "        \n",
    "        for i in range(self.n_layers - 1):\n",
    "            hidden = self.dropout(torch.tanh(self.hidden_layers[i+1](hidden)))\n",
    "        scores = self.top_layer(hidden)\n",
    "        \n",
    "    \n",
    "        return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[774  17 531  60 653 505 662 353 241 438  92 318 369 532 368  16  21 312\n",
      "  43 320  30 597 480 244 330 627 537 545  40 217 794 111  36 482 740 630\n",
      " 328  13 643 530 177 125 510 785   3 617 693 154 357 789 333 766 515 519\n",
      " 275 282 205 703  96 467 623  24   9 687 304 521 240 298 578 400 358 114\n",
      " 620 606 680 748 444 575 195]\n",
      "[152 405 560  15 759 245 634 140 371 549 130  90 461 490 796 460 761 168\n",
      "  65 611 319 324 669 317 226 415 603 619 166 350 640 413   1 123   6 383\n",
      "  91  59 685 335 745 726 414 689 172 478 227 308 412  73 590 473 277 112\n",
      " 556 550 425 186 202 752 782 229 758 151 484 145 148 672 741 450 652 544\n",
      " 419 601  86 212 322 155 486 316 508 156 407 539 533 564 276 646 628 329\n",
      " 171 377  10 701 216 792 750 128 778 469 538 349 665 518 389 270 481 801\n",
      " 393   5 570 602 791 607 547   2 582 310 113 705 548 314 541 185 732 479\n",
      " 509 498 108 656 692 250  66 101 554 720 751 296 605 673 299 770 586 100\n",
      " 683 618 725 142 292 728 222 221 704 636 117 795  32 211  58 401 334 581\n",
      " 440 246 638 210  82 121  22 307  11 403 427 472  61 625 451 777  37 775\n",
      " 566 418 259 160  44 422 667 209 303 654  72 284 325 522 373 635 390  31\n",
      " 235 768 351 367 496 520 302 463 661 702 144  39 410 565 776 106 182 362\n",
      " 301  93]\n",
      "[610 576 684 223 465 220 474 208  53  78 287 174 437 261 138 331 355 719\n",
      "  77  80 730 512  34 136 248 800 492 476 471 342 384 747 527 272 736 372\n",
      " 757 132 442 468 734 454 696 651 737 430  62 139  88 712 137 466 713 445\n",
      "  74 213  89 760 184 378 109 487 708 494 219 286 234 258 435 574 382  14\n",
      " 678 572 273 551 256 198 266 408 178 375 253 257 237 772 396 321 633 290\n",
      " 268 279 352  63 731 399 311 536 569 446 561 797 639 540 374 143 562  45\n",
      " 354 187 105 165 391 552 200  33 339 260 753 157 432 394 459 269 517 723\n",
      " 153 738 600 204 598  28 252 615 397 411 595 206 457 134 164 691 706 596\n",
      " 126 264  64 107 278 779 754 674 694 503 511 529 528 624 231 201 385 718\n",
      " 793 267 489 426 424 306 315 398 337 647 207 724 215 340 470 239  85 746\n",
      " 176 553 697  54 475  25 534 263 632 120 546 344 735 336 695 629   0 755\n",
      " 637  27 291 762  35 364 300 224 700 343  48  50 462 571 214 589  99  97\n",
      " 557 361 599 431 386 727 499 441  83  26 613  52  38 488 420 659 388 309\n",
      " 305 122 657 119 338 686 591 699 558 421 455 392 131 650 236 436 729  29\n",
      " 347 676 716  19 341 147 608 110 786 327 416 265 631 417 395 744 773 626\n",
      " 614 670 167 458 196 288 194 514 709 323 404 243 423 251 271 104 129 743\n",
      " 655 280 360 376 583  46 543 733 645 493 175 682 127 573 739 345 771 124\n",
      " 675 135 409 366 592   8 648 453 197  84 621 380 159 588 587 622 429 326\n",
      " 141 434 568 707 149 649 658 781 502 443 555  79 664 668 247 181 452 233\n",
      "   7 381 402 579 483 516 348 189 765 742 722 559  51 491 501 767 363 170\n",
      " 433  12  70 133  41 115 497  47 788 289 203 593 784 477 584 542 525 103\n",
      " 218 118 183 594 504 283  20 677 577 671 163 313  69   4 604  76  98 225\n",
      " 644 485 116 567 721  67 523 616  56 295 506 679 563 715 749 285  87 356\n",
      " 790 513  81 448 228 612 359 642 158 150  75 780 193 169 783 161  57 387\n",
      " 585 464 249  18 191 188 769 526 495  55 173 763 802 332 232 711 756 254\n",
      "  42 500 230 456 346 698  71 293 262 255 379 663 609 294 190 146  49 102\n",
      " 365 524 690 660 688  68 798 717 535 162 242 710 764 370 180 803 447 428\n",
      " 787  94 799 507  95 281  23 274 192 580 238 714 406 449 666 297 199 641\n",
      " 179 439 681]\n",
      "0.47936482084690557 0.5154569892473118 0.5158562367864694\n",
      "using learning rate [0.0001, 0]\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0\n",
      "End of epoch loss 0.7065783012658358\n",
      "0.791970684039088 0.7184139784946236 0.7526427061310781\n",
      "End of epoch loss 0.6785555076785386\n",
      "0.8077198697068405 0.7513440860215054 0.7621564482029598\n",
      "End of epoch loss 0.6344991982914507\n",
      "0.823257328990228 0.7513440860215054 0.7744009866102889\n",
      "End of epoch loss 0.6362858470529318\n",
      "0.8341368078175896 0.7741935483870968 0.782152924594785\n",
      "End of epoch loss 0.5989134865812957\n",
      "0.8493485342019543 0.793010752688172 0.8002995066948555\n",
      "End of epoch loss 0.5922462665475905\n",
      "0.866628664495114 0.8111559139784946 0.8217054263565892\n",
      "End of epoch loss 0.5574197364039719\n",
      "0.8760912052117265 0.8198924731182796 0.8248766737138831\n",
      "End of epoch loss 0.5359044240321964\n",
      "0.8865309446254073 0.8346774193548386 0.8353594080338266\n",
      "End of epoch loss 0.5256090201437473\n",
      "0.89685667752443 0.8528225806451613 0.8449612403100775\n",
      "End of epoch loss 0.48019128548912704\n",
      "0.9009934853420196 0.8649193548387096 0.8480443974630021\n",
      "End of epoch loss 0.5255864127539098\n",
      "0.9089739413680783 0.8729838709677419 0.851568005637773\n",
      "End of epoch loss 0.4942069002427161\n",
      "0.9132084690553747 0.8655913978494624 0.8556201550387597\n",
      "End of epoch loss 0.47381454473361373\n",
      "0.919071661237785 0.8729838709677419 0.8564129668780831\n",
      "End of epoch loss 0.4643065785057843\n",
      "0.9252931596091205 0.8729838709677418 0.8593199436222692\n",
      "End of epoch loss 0.4342063933145255\n",
      "0.9264820846905538 0.8635752688172043 0.8550916138125441\n",
      "End of epoch loss 0.45088513917289674\n",
      "0.9276058631921823 0.8528225806451613 0.8569415081042988\n",
      "End of epoch loss 0.4300280886236578\n",
      "0.9340390879478827 0.8716397849462365 0.8595842142353771\n",
      "End of epoch loss 0.42026243545114994\n",
      "0.9371986970684039 0.8716397849462366 0.8593199436222692\n",
      "End of epoch loss 0.4088993630139157\n",
      "0.9413355048859935 0.875 0.8606412966878083\n",
      "End of epoch loss 0.41822224925272167\n",
      "0.9462214983713354 0.8830645161290323 0.8634601832276252\n",
      "End of epoch loss 0.4110790682025254\n",
      "0.9491368078175896 0.8803763440860215 0.8651338971106413\n",
      "End of epoch loss 0.40556363482028246\n",
      "0.9531921824104235 0.8864247311827957 0.8658386187455954\n",
      "End of epoch loss 0.3882494497811422\n",
      "0.9540879478827361 0.89247311827957 0.8636363636363636\n",
      "End of epoch loss 0.3596964367898181\n",
      "0.9576384364820847 0.8958333333333334 0.8728858350951374\n",
      "End of epoch loss 0.36214275390375406\n",
      "0.9598045602605864 0.8958333333333334 0.8742071881606766\n",
      "using learning rate [2e-05, 2e-05]\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "End of epoch loss 0.5804420177591965\n",
      "0.9368648208469055 0.8602150537634409 0.842847075405215\n",
      "End of epoch loss 0.4468374091666192\n",
      "0.971514657980456 0.9361559139784946 0.9065362931642001\n",
      "End of epoch loss 0.30252481682691723\n",
      "0.9863680781758957 0.92002688172043 0.8954369274136715\n",
      "End of epoch loss 0.2262036483734846\n",
      "0.9865635179153095 0.928763440860215 0.8859231853417899\n",
      "End of epoch loss 0.21772901073563844\n",
      "0.990114006514658 0.9408602150537634 0.8844256518675123\n",
      "End of epoch loss 0.19194489775691181\n",
      "0.9864169381107492 0.9334677419354839 0.8907681465821\n",
      "End of epoch loss 0.1644137983676046\n",
      "0.9879315960912053 0.928763440860215 0.8989605355884427\n",
      "End of epoch loss 0.13594387681223452\n",
      "0.9885993485342018 0.9307795698924731 0.9112050739957717\n",
      "End of epoch loss 0.11148388212313876\n",
      "0.9887459283387622 0.9327956989247311 0.897198731501057\n",
      "End of epoch loss 0.11871081462595612\n",
      "0.9888273615635179 0.9368279569892474 0.900369978858351\n",
      "End of epoch loss 0.09956014461931773\n",
      "0.9888925081433225 0.9395161290322581 0.9000176180408739\n",
      "End of epoch loss 0.10068273296928965\n",
      "0.9889413680781759 0.9401881720430108 0.9007223396758282\n",
      "End of epoch loss 0.10827379915281199\n",
      "0.9892508143322475 0.9448924731182796 0.9027484143763213\n",
      "End of epoch loss 0.11197343474486843\n",
      "0.9896254071661238 0.9469086021505376 0.9022198731501057\n",
      "End of epoch loss 0.09862728393636644\n",
      "0.9901628664495113 0.946236559139785 0.9028365045806908\n",
      "End of epoch loss 0.09586019461858086\n",
      "0.9906188925081433 0.946236559139785 0.9029245947850597\n",
      "End of epoch loss 0.10156489408109337\n",
      "0.9906514657980457 0.9469086021505376 0.9037174066243833\n",
      "End of epoch loss 0.1023942946922034\n",
      "0.9908957654723126 0.948252688172043 0.9034531360112755\n",
      "End of epoch loss 0.10834216355578974\n",
      "0.9911726384364821 0.9489247311827957 0.9037174066243834\n",
      "End of epoch loss 0.09520149262971245\n",
      "0.9913843648208469 0.948252688172043 0.9040697674418605\n",
      "End of epoch loss 0.09517191461054608\n",
      "0.9915146579804559 0.9489247311827956 0.9035412262156448\n",
      "End of epoch loss 0.09369627840351313\n",
      "0.9914169381107492 0.9489247311827956 0.9031007751937984\n",
      "End of epoch loss 0.10594348143786192\n",
      "0.9912540716612378 0.950268817204301 0.9037174066243834\n",
      "End of epoch loss 0.0969095577893313\n",
      "0.9914169381107493 0.950268817204301 0.9037174066243833\n",
      "End of epoch loss 0.10134032860514708\n",
      "0.9914495114006515 0.950268817204301 0.9037174066243834\n",
      "[312 473 475 650 493 541  48 113 556 797  14 674 423 426 613 272 187 626\n",
      " 166 629 119 792 726 227 223  17  92 640 323  43 108 653 224 263 145  59\n",
      " 445  97 107 691 124 481 532 684 512 669  30 441 362 363 787  82 554 161\n",
      "  61 783 443 641 768 728 472 149  12  79 688  42 410 636 205 788 622 523\n",
      "  11 679 781 648 449  94 522]\n",
      "[355 634 171 607 462 736 167 306 608 531 239 257 617 637 287 155  90 639\n",
      " 750 709 177 184 389 405 678 488 675 250 552 261 407 487 164 600 672 310\n",
      " 391 371 574 154 416 208 741 529 537 336 498 219  36 699 398 624 492 735\n",
      " 601 753  83 705 330 706  29 432 779 331 337 628 290  73 510 204 153 744\n",
      " 494 415 396 732  77 615 152 388 686 175 482 543 758 280 132 610 652 518\n",
      " 561 659 633 723 533 774 530 322 258 457 793 411 476 256 185 383 339 385\n",
      " 397 643 186 176 314 386 128 413 436 212 734 460 368 801  54  99 270 474\n",
      "  52 375 214 317 708 470 199 535  75 192 565 189 401 451 751 721 506 581\n",
      " 275 254 387 203 584 507  68 764 409 710 559 346  70 802  96 664 188 357\n",
      " 101 577 575  98  84 748 262 578 803 477 351 555  39 118 567 242 348 181\n",
      " 197  76 790 390 588 609  31 711  81  41 292 769 356 621  23 776 444  55\n",
      " 524 303 190 668  18 763 501 666 681 230 284 644 725 568 102 780 158 100\n",
      " 402 294]\n",
      "[696 104 786 538 207 421 747 217 589 216 597 125 656  25 536 733   3 342\n",
      "  15 308 127 267 737 549 743 647 446 794 126 614 759 685 540 111 746 546\n",
      " 771 236 408 420 335  28 264 268 693 395 539 123 469 269 105 517 772 564\n",
      " 340 414 471 278 265 785 724 553 562 234 727 376 320 490 361 243  80  88\n",
      " 480 372  40 311 611 253 558 718 499 459 338  63 156 800 676  91 198 200\n",
      " 364 382 437 461 341 157 394 571 662 782 627 324 651 327 569  89 547  50\n",
      " 300 692  46   0 761 347 730 760 478 286 682  38 360 147 738 557 632 134\n",
      " 393 349 657  64 143 573 509 442 479 755 352 136 576 419 796 598 220 689\n",
      " 130 745 528 739 740 695 345  65 206 201 582 354 288 353 762  53 425 560\n",
      " 245 378  35 138 570 237 244 139 655   1  62 129 279 646 417 215 110   2\n",
      " 458 716 291 450 503  10 196 315 527 109 596  13 424 260 455 438 505   5\n",
      "  85 603 399 486 248 343 466 231 590 226  74 454 131 174 550 465 213  26\n",
      " 251 273 719 731 630 241 122 344 404 318 112 534 700 645 548 551 430 514\n",
      " 791 694 670  16 572 316 120 435 595 165 431 305 712 697 773 619  21 484\n",
      " 374 544 511 328 137 271 384 508  33 778 277 468  60 319 713 392 631 665\n",
      " 178 412 591   6 752 194 252 377 583 489 309 229  45  78 701 545 757 329\n",
      " 754 266 140  27  34 168 321 350 202  86 172 602  19 151 599 369 276 148\n",
      " 729 159 135 400 521  57 428 717 146   8 293 299 333 563 114 403 638 616\n",
      " 142   4 620 240 642  67  56   9 704 586 222  58 301 173 690 274 433 163\n",
      " 297 798 658 367 366 302 307 191 784 447 238 144 283 485 439 210  69  95\n",
      " 281 504 789 452 594 162 106 777  32 525 654 606 228 249 169 453 756 373\n",
      " 667 742 456 381  72 221 749 434 464 141 246 496  71 677  24 623 502 422\n",
      " 209 218 491 660 233  20 580 618 133 418 649 380 542 235 358 225  37 592\n",
      " 304 483  49 289 313 765 448 326 182 467 587 513 282 795 160 605 193 463\n",
      " 359 702 495 179 195 707 799 150 298 698 671 247 703 440 427 715 520 566\n",
      "  44 722 515 117 232 770 604 714 115 116 365 295 180 379   7  22 683 516\n",
      " 103 635 255 612 526 593 325 406  87 767 673  47 625  93 720 296 170 285\n",
      " 259  51 334 661 121 766 370 497 663 519 332 585 211 500 775 579 429 183\n",
      " 680  66 687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6650977198697069 0.7486559139784946 0.648784355179704\n",
      "using learning rate [0.0001, 0]\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0\n",
      "End of epoch loss 0.4606029652059078\n",
      "0.9743159609120521 0.9798387096774193 0.9726920366455251\n",
      "End of epoch loss 0.33009943016804755\n",
      "0.9764983713355049 0.9818548387096774 0.9756871035940803\n",
      "End of epoch loss 0.3000197554938495\n",
      "0.9766123778501627 0.9838709677419355 0.977184637068358\n",
      "End of epoch loss 0.30329311557579786\n",
      "0.976400651465798 0.9838709677419355 0.9783298097251585\n",
      "End of epoch loss 0.2671852378407493\n",
      "0.976416938110749 0.9865591397849462 0.9797392529950669\n",
      "End of epoch loss 0.2602258522529155\n",
      "0.9781107491856678 0.9858870967741935 0.9804439746300211\n",
      "End of epoch loss 0.24799839581828564\n",
      "0.9789413680781759 0.9858870967741935 0.9792107117688513\n",
      "End of epoch loss 0.20618834916967899\n",
      "0.979201954397394 0.9845430107526882 0.9787702607470049\n",
      "End of epoch loss 0.20627541246358305\n",
      "0.9793648208469056 0.9858870967741935 0.9775369978858351\n",
      "End of epoch loss 0.2062939964234829\n",
      "0.9798371335504885 0.9852150537634409 0.9795630725863284\n",
      "End of epoch loss 0.20483537041582167\n",
      "0.978973941368078 0.9818548387096775 0.9795630725863284\n",
      "End of epoch loss 0.1789989378885366\n",
      "0.9793973941368078 0.9818548387096774 0.9781536293164199\n",
      "End of epoch loss 0.17198614333756268\n",
      "0.9806026058631921 0.9818548387096775 0.9784178999295279\n",
      "End of epoch loss 0.16859191318508238\n",
      "0.9814495114006515 0.9778225806451613 0.9788583509513741\n",
      "End of epoch loss 0.15183025546139106\n",
      "0.9811400651465798 0.978494623655914 0.9784178999295278\n",
      "End of epoch loss 0.1622004618984647\n",
      "0.9794951140065147 0.9811827956989247 0.9777131782945737\n",
      "End of epoch loss 0.14055784803349525\n",
      "0.9801465798045602 0.9825268817204302 0.97815362931642\n",
      "End of epoch loss 0.1384292325237766\n",
      "0.9803094462540716 0.9818548387096774 0.97815362931642\n",
      "End of epoch loss 0.12295893087866716\n",
      "0.9798697068403908 0.9838709677419355 0.9776250880902043\n",
      "End of epoch loss 0.15505201532505453\n",
      "0.9807817589576546 0.9818548387096774 0.9776250880902043\n",
      "End of epoch loss 0.11309755453839898\n",
      "0.980928338762215 0.9811827956989247 0.977536997885835\n",
      "End of epoch loss 0.12730293220374733\n",
      "0.9799674267100977 0.9831989247311828 0.9782417195207893\n",
      "End of epoch loss 0.13806453367578797\n",
      "0.9802117263843647 0.9831989247311828 0.9789464411557435\n",
      "End of epoch loss 0.11863349625491537\n",
      "0.9799511400651465 0.980510752688172 0.9783298097251586\n",
      "End of epoch loss 0.12399548781104386\n",
      "0.9794299674267101 0.9798387096774193 0.9768322762508809\n",
      "using learning rate [2e-05, 2e-05]\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "End of epoch loss 1.1005890147353057\n",
      "0.5220114006514658 0.447244623655914 0.539552501761804\n",
      "End of epoch loss 1.4640630795620382\n",
      "0.6707899022801302 0.6488575268817204 0.6496652572233969\n",
      "End of epoch loss 1.238492829957977\n",
      "0.6761319218241042 0.654233870967742 0.6560958421423538\n",
      "End of epoch loss 1.1844471753574908\n",
      "0.6810016286644951 0.6528897849462365 0.6601039464411558\n",
      "End of epoch loss 1.159855655161664\n",
      "0.680944625407166 0.6535618279569891 0.6720842142353771\n",
      "End of epoch loss 1.123681854805909\n",
      "0.6800651465798045 0.6565860215053764 0.6713794926004228\n",
      "End of epoch loss 1.1333034839481115\n",
      "0.6774999999999999 0.6639784946236559 0.6703664552501762\n",
      "End of epoch loss 1.1497654309496284\n",
      "0.6835504885993487 0.6720430107526882 0.6736698379140241\n",
      "End of epoch loss 1.1322203266900033\n",
      "0.6786074918566776 0.6740591397849462 0.667107117688513\n",
      "End of epoch loss 1.0966389840468764\n",
      "0.6754315960912052 0.6646505376344085 0.6592670894996476\n",
      "End of epoch loss 1.0895916312001646\n",
      "0.6699837133550489 0.6498655913978495 0.6497973925299507\n",
      "End of epoch loss 1.1921043095644563\n",
      "0.47302117263843646 0.4852150537634409 0.49348132487667373\n",
      "End of epoch loss 0.9974289969541132\n",
      "0.6972801302931596 0.6653225806451613 0.6889094432699083\n",
      "End of epoch loss 0.7713635032996535\n",
      "0.7505618892508144 0.6995967741935484 0.7303558844256519\n",
      "End of epoch loss 0.7691434430889785\n",
      "0.8126954397394135 0.7137096774193549 0.8054968287526427\n",
      "End of epoch loss 0.7222722806036472\n",
      "0.8060830618892509 0.7204301075268816 0.7884073291050034\n",
      "End of epoch loss 0.7584570348262787\n",
      "0.8057817589576547 0.7345430107526882 0.7919749823819591\n",
      "End of epoch loss 0.7608485077507794\n",
      "0.8013192182410422 0.7439516129032258 0.7849718111346018\n",
      "End of epoch loss 0.7576431720517576\n",
      "0.7934771986970685 0.7432795698924731 0.7744009866102889\n",
      "End of epoch loss 0.7467725519090891\n",
      "0.8256514657980455 0.7533602150537634 0.8023696264975334\n",
      "End of epoch loss 0.723204156383872\n",
      "0.847915309446254 0.7809139784946236 0.8234672304439746\n",
      "End of epoch loss 0.7554112332873046\n",
      "0.8622068403908794 0.7943548387096775 0.8139534883720929\n",
      "End of epoch loss 0.6736593595705926\n",
      "0.8854071661237785 0.8111559139784946 0.8283121916842847\n",
      "End of epoch loss 0.7519412175752223\n",
      "0.8501954397394137 0.7620967741935484 0.788231148696265\n",
      "End of epoch loss 0.6261939983814955\n",
      "0.8734039087947882 0.8051075268817204 0.8062015503875969\n",
      "[531 219 629 744 214  10 481  16 550 576 459 337   1 730 417 248 208 545\n",
      " 318 407   3 762 223 320 237  38 153 416 466 538  86  91 470 408 414 322\n",
      " 329 562 241  88  19 186 344 372 393 637  89 343 301 225 535 332 707 491\n",
      "  98 209 495  95 381  93 472  81 592 433 218 636 195 559 769 228 777 585\n",
      " 284 254 162 565 584 150 523]\n",
      "[398 794 494 573 112 475 530 352 257 220 498 286 386 147  17 156 779 468\n",
      " 478 488 659 353  46 113 126 154 754 354 137 471 574 595 212 171 229  80\n",
      " 590 800 529 546 600 139 469 537  43 184 736 172 108 678 785 300 455 305\n",
      " 340 607 436 670 723 684 534 385 582 368 598 324 261   6 528 384  35 202\n",
      " 712 317 759 311 655 131 750 140 462 271 713 288 206 694   0 541 129  48\n",
      " 686 490  74  40 512 174 510 509 119 166 260  97 801 360 315  13 474 327\n",
      " 514 706 772 273 437 272 597  77 201 239 479 243 505 349 656 252 450  85\n",
      " 771 345 631 430 674 696 688 210 648 357 501 159  31 703 183 193 720 240\n",
      " 485 542 690 189 121 242 563 580 370 444 103 275 687 618 233 401 367 803\n",
      " 115   8 620 717 295 400 789   9 586 526 513 504 763 519  11 593 100 784\n",
      " 680  44 577 588 447 728 661  47 117 742 464  84 173  32 609 262 711  58\n",
      " 587  76 180 477 787  12 722 521 780  61 795 333 179 191 766 704 255 749\n",
      " 525 756]\n",
      "[278 460 536 371 484 120  45 123 558 734 552 244 457 148 632 396 331 435\n",
      " 145 518 732 335 532 454 611 662 718 308 280 786 130 492 657 733 276  14\n",
      " 652 465 503 708  65 425 105 350 599  60 432 319 415  83 747 651 740 572\n",
      " 569 729 735 676  27 341 624 290 761 207 125 782 791 539 404 124 323 374\n",
      " 628 589 731 752 314 591 633 634 391 321 392 709 270 167 234 560 411  59\n",
      " 388 697 258 489 675 256 745 306 328 682 596 544 442 175 122 755 231 741\n",
      " 215 639 705 446 310 330 268 517 482 695 128 549 608 603 355 420 361 378\n",
      " 511 394 309 499 473  99  73 157 553 760  64  53 383  30 551 419 508 224\n",
      " 421 382 793 412 693 650 543 701 277 527 339 613 753 194 143 547 236 279\n",
      " 557 151 792 287 619 204 438 347 739 168 227 493 109 653 177 187 757 540\n",
      "  25 264 646 213 692 647 424 665 104 672 743 107 245  78 643 570 716 431\n",
      " 480 110  90 626 476 152 685 601 561 746  36 602 266  92 458 226  33 397\n",
      " 669 564 583 338 291 486 253  52 216  21 377 176 737 250 316  54  29 312\n",
      " 165 132 724  28 136 198 691  50 699 369  34 217 389 796 556 426 138   5\n",
      "  62 548 441 164 196 610 111 773 627 487 423 269 263 617 251 727 774 342\n",
      "  63 700 395 719 155 461 200  26 364 778 758 399 738   2 445 127 185 413\n",
      " 640 178 376 336 134 571 267 726 630 405 614 375 645 797 689 615 533 265\n",
      "  15 359 578  68 799 325 247 606 259 515 197 451 373 199 365 579 116 625\n",
      "  57  82 346 149 203 281 102 721 671 169 182 304 293 144 246 101 677 667\n",
      "   7 751 428 181 334 141 406 222 506 496 502 230 767 714 802  79 190 658\n",
      " 161  24 520 427 362 516 363  67  75 524 467 673 249 638 298 554  51 387\n",
      " 294 654 644 456 679  20  66  37 507 768 555 274 299 163 463 366 443 605\n",
      " 285 170  39 379 135  56 621 765 616 238 483  72 235 594 449 668 642 221\n",
      " 710 788 649   4 192 211  55 453 439 205 114 776 146 434 781 566 452 188\n",
      " 664  41 764 403 296 289 390 623  42 448  71 783 666 351 663 581 497 725\n",
      " 160 612 118 348  87 232 422  96  94 303 158 715  69 440 106 604 142 660\n",
      " 410 326 133 698 567 402 683 568  18  49 380 282 748  70 770 307 302 681\n",
      " 575 418 409 635 500 429 313 283 292 622 790  23 798 522 297 702 358 356\n",
      " 775  22 641]\n",
      "0.16348534201954396 0.16263440860215053 0.18287526427061312\n",
      "using learning rate [0.0001, 0]\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch loss 0.5450345892459154\n",
      "0.875700325732899 0.883736559139785 0.8359760394644116\n",
      "End of epoch loss 0.49537073844112456\n",
      "0.876400651465798 0.8850806451612904 0.8365045806906272\n",
      "End of epoch loss 0.5358426226302981\n",
      "0.8774755700325733 0.8938172043010753 0.8335976039464411\n",
      "End of epoch loss 0.5140512380748987\n",
      "0.8778338762214983 0.885752688172043 0.8360641296687809\n",
      "End of epoch loss 0.49222090491093695\n",
      "0.8766449511400651 0.8844086021505375 0.8363284002818887\n",
      "End of epoch loss 0.5146526608150452\n",
      "0.8781596091205213 0.8918010752688172 0.8350070472163496\n",
      "End of epoch loss 0.5100394459441304\n",
      "0.8778175895765472 0.894489247311828 0.8343023255813953\n",
      "End of epoch loss 0.4942961379420012\n",
      "0.8777687296416937 0.8877688172043011 0.8355355884425651\n",
      "End of epoch loss 0.5128543158061802\n",
      "0.8775244299674267 0.8891129032258065 0.8335976039464411\n",
      "End of epoch loss 0.5170431898441166\n",
      "0.8759609120521172 0.8797043010752688 0.8376497533474278\n",
      "End of epoch loss 0.5030624938663095\n",
      "0.8757491856677525 0.8810483870967741 0.8369450317124736\n",
      "End of epoch loss 0.4985406664200127\n",
      "0.8777198697068405 0.887768817204301 0.8365926708949964\n",
      "End of epoch loss 0.5082710308488458\n",
      "0.8775732899022801 0.8864247311827956 0.835799859055673\n",
      "End of epoch loss 0.4893039492890239\n",
      "0.8782084690553745 0.885752688172043 0.8358879492600422\n",
      "End of epoch loss 0.4956884905695915\n",
      "0.8783061889250814 0.8978494623655915 0.8343904157857646\n",
      "End of epoch loss 0.48592762695625424\n",
      "0.8786644951140066 0.8911290322580645 0.8350070472163496\n",
      "End of epoch loss 0.49001847789622843\n",
      "0.8786807817589577 0.9005376344086021 0.8352713178294574\n",
      "End of epoch loss 0.4925863929092884\n",
      "0.8785504885993486 0.9012096774193549 0.8342142353770261\n",
      "End of epoch loss 0.499520315323025\n",
      "0.8779478827361563 0.8985215053763441 0.8343023255813953\n",
      "End of epoch loss 0.4877922497689724\n",
      "0.8785016286644952 0.898521505376344 0.8349189570119803\n",
      "End of epoch loss 0.48866507527418435\n",
      "0.877654723127036 0.9012096774193549 0.8343904157857647\n",
      "End of epoch loss 0.47001056699082255\n",
      "0.8784690553745929 0.8985215053763441 0.8354474982381959\n",
      "End of epoch loss 0.47862519696354866\n",
      "0.8786644951140066 0.896505376344086 0.8352713178294573\n",
      "End of epoch loss 0.48720711818896234\n",
      "0.878599348534202 0.8951612903225805 0.8343023255813954\n",
      "End of epoch loss 0.500154681969434\n",
      "0.8783550488599349 0.894489247311828 0.833861874559549\n",
      "using learning rate [2e-05, 2e-05]\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "End of epoch loss 0.5118869990110397\n",
      "0.8961074918566775 0.8615591397849461 0.8446969696969697\n",
      "End of epoch loss 0.47258886648342013\n",
      "0.9362052117263843 0.9368279569892473 0.8902396053558844\n",
      "End of epoch loss 0.43458502751309425\n",
      "0.9637622149837134 0.9543010752688172 0.9185165609584214\n",
      "End of epoch loss 0.33313124731648713\n",
      "0.9761237785016287 0.9556451612903226 0.906800563777308\n",
      "End of epoch loss 0.3177595908055082\n",
      "0.9748697068403909 0.9583333333333334 0.8989605355884426\n",
      "End of epoch loss 0.3942755184834823\n",
      "0.9709446254071662 0.9025537634408601 0.897815362931642\n",
      "End of epoch loss 0.4936981904320419\n",
      "0.9563192182410424 0.9301075268817205 0.888477801268499\n",
      "End of epoch loss 0.34207689110189676\n",
      "0.9653257328990228 0.9294354838709677 0.8977272727272727\n",
      "End of epoch loss 0.43502061266917735\n",
      "0.9839250814332248 0.9361559139784946 0.8779950669485553\n",
      "End of epoch loss 0.30551101616583765\n",
      "0.982915309446254 0.9334677419354839 0.8694503171247357\n",
      "End of epoch loss 0.2369165262207389\n",
      "0.9850651465798046 0.9200268817204301 0.8713883016208598\n",
      "End of epoch loss 0.20986663666553795\n",
      "0.9877524429967427 0.9240591397849462 0.8703312191684284\n",
      "End of epoch loss 0.17105429677758366\n",
      "0.9893159609120521 0.9529569892473118 0.8905919661733616\n",
      "End of epoch loss 0.21242085087578744\n",
      "0.9878501628664496 0.9032258064516129 0.8758809020436928\n",
      "End of epoch loss 0.2679376824526116\n",
      "0.9882410423452769 0.9274193548387096 0.8433756166314306\n",
      "End of epoch loss 0.1761613094713539\n",
      "0.9918892508143323 0.9327956989247312 0.8633720930232558\n",
      "End of epoch loss 0.15906022244598716\n",
      "0.9893892508143323 0.9375 0.8691860465116279\n",
      "End of epoch loss 0.15676869312301278\n",
      "0.9898697068403908 0.9294354838709677 0.8585711768851303\n",
      "End of epoch loss 0.1482436687219888\n",
      "0.9906677524429967 0.9351478494623656 0.8593199436222693\n",
      "End of epoch loss 0.14867677353322506\n",
      "0.9897964169381107 0.9287634408602151 0.8628435517970401\n",
      "End of epoch loss 0.14696410909527913\n",
      "0.9898371335504886 0.9307795698924731 0.858615221987315\n",
      "End of epoch loss 0.14914996403967962\n",
      "0.9899348534201955 0.9294354838709677 0.8583509513742072\n",
      "End of epoch loss 0.13855974387843162\n",
      "0.9900081433224757 0.9301075268817204 0.8576462297392531\n",
      "End of epoch loss 0.14398421783698723\n",
      "0.9895928338762215 0.928763440860215 0.8556201550387597\n",
      "End of epoch loss 0.12908461969345808\n",
      "0.9895602605863193 0.9301075268817205 0.8564129668780831\n",
      "[772 450 512 484 288 104 129 727 290 257 723 127 416 474 678  10 202   3\n",
      " 322 187 693 316 487 599 126 152 392 476 614 610 393 244 493 651 420 549\n",
      " 498 505 778 276  35 139 527 437 108 340 419 105 183 802 197 233 232 767\n",
      " 524 577 242  39 581  47 135 790 195 698 313 588 472  12 641 163 444 720\n",
      "  75 798 497 642  87  51  11]\n",
      "[354  21 219 194 556 633 760  99 389  19 719 731 270 543 344 557 445 708\n",
      " 198 730  80 640 291 752  33 482 196 733 347  17 369 534  13  89 709 645\n",
      " 200   1  92 672 793 157 750 404 469 241 220 119 431  83 286 166  60 130\n",
      " 423 140  36 572 700 560 424  27 131 574  43 628 305 113 773 759 319 613\n",
      " 558 382 547   2 460  90 397 120 531 178 792 167 701 552  73 499 375 287\n",
      " 212 329 383 215 154  50   5 250 436 185 300 489 454 538 229 168 779 441\n",
      "  30  25 761 177 603 342 256 691 432 268 738 647 569 570 674  38 175 315\n",
      " 184 147 107 475 309  45 464  44  95 210 304 346 115  93 710  20 106 428\n",
      " 751 766 193 495 292 449 787  70 401 283 161 587  41 448 410 362 182 365\n",
      " 521 781 502 162 491 500 188 117 282 116 169 664 504 297 749 380 677  94\n",
      " 520 728 592   7 159 296 501 440 580 222  18 400 568 158 452 717  72 648\n",
      " 240 403 519 390 418 228  98 199 575  96 658  61 707 225   4 769 249 205\n",
      " 667 679]\n",
      "[371 551 639 399 438 243 273 171 537 352  46 124 306 737  29 155 533  86\n",
      " 174 213 743  52 692 746  64 637 386 553 165 459 774 361 473 626 589 718\n",
      " 583 311  14 753 514 694 634 384  74 545 712 546 368 696 398 736 123  53\n",
      " 364 408 479  54 411 430 598 734 650 132  34 716 134 724 548 143 136 345\n",
      " 122 310 417 330 480 662 595 550 339 224 611 632 735 630 670 478 223 415\n",
      " 653 374 486 471 492 669 267 207 686 457  62  28 530 350 312 771 619 646\n",
      " 280 413 624 204 227 744 248 353 468 757 470 385 697 263 564 490 617 376\n",
      " 741 343 172 745 488 597 643  65 186  48 414 231 252 544 336 800 786  78\n",
      " 355 629 314 591 110 682 517 151 796 421 435 335 425 684 396 260  63 602\n",
      " 689 705 528  97  26 601 713  59 214 801 596 272 278  91 277 532 237 657\n",
      " 337 582 395 739 785 341   6 656 111 794 462 529 125 349 659 676 372  16\n",
      " 732 675 206 508 706 109 405 426 308 747 590 112 562  77 695 511 271 755\n",
      " 503 539 573 740 754 208 699 461 264 608 391 536 466 758 217 164 510 509\n",
      " 561 388 269 494 317 791 631 782   0 455 615 412 156 226 176 261 324 458\n",
      " 258 137 323 481 148 236 253 576 265 407 128 726 360 245 251 266 729 331\n",
      "  88 328 541 234 320 318 571 446 685 600 338 279 518 394  85 442 327 153\n",
      " 652 607 540 145 465 797 216 762  15 655 665 321 201 138 627 239  40 377\n",
      " 378  71 485  22 506 668 303  66 218 788 621 181 623 554 289  76 247 566\n",
      " 604  42 555 763 513 433 795 434 649 259 160 334 357 585 742 579 103 173\n",
      " 326 101 559 332 456 803  57 170  82 294 274 146 367 144 325 209 443 118\n",
      " 748 671 467  58 149 636 507 715 422 189 348 681 654 262 211 477 673 687\n",
      " 142 203 351 702 180 725 285 275 293 356 483 463 644 606 663 525 515 616\n",
      " 776 333  84 447  79 522 299 284 429 238 714 370 777 221 141 799 230 409\n",
      " 770 373  32 114 298 535 381 711  67 366 439  56 387 780 586   8 722 775\n",
      " 121  31 721 402 625 150 302 523 133 690  68 783 768 358 622 789 618 427\n",
      " 379 704   9 565 301 255 660 192 638 453 307 563  69 578 516 680 612 764\n",
      " 703 584 567 100 235 179  55 102 542  37 661  81 295 620 765 451 594 593\n",
      " 526 683 191 784 190 756  49 281 605  23 496 254 609 359 635 666 406 363\n",
      "  24 246 688]\n",
      "0.1559609120521173 0.21505376344086022 0.1700140944326991\n",
      "using learning rate [0.0001, 0]\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0\n",
      "End of epoch loss 0.3889053431339562\n",
      "0.9638599348534201 0.9139784946236559 0.9626497533474279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch loss 0.34483049553819\n",
      "0.9639820846905538 0.915994623655914 0.9645877378435519\n",
      "End of epoch loss 0.3233070974238217\n",
      "0.9654234527687297 0.9186827956989247 0.966261451726568\n",
      "End of epoch loss 0.34089432284235954\n",
      "0.9662540716612378 0.9193548387096775 0.9687279774489077\n",
      "End of epoch loss 0.3100471047218889\n",
      "0.9663843648208469 0.9173387096774194 0.9686398872445384\n",
      "End of epoch loss 0.3070137567119673\n",
      "0.9661807817589577 0.9274193548387096 0.9678470754052149\n",
      "End of epoch loss 0.3138631535694003\n",
      "0.9659609120521172 0.9206989247311828 0.9679351656095843\n",
      "End of epoch loss 0.3000952817965299\n",
      "0.965399022801303 0.9227150537634409 0.9677149400986611\n",
      "End of epoch loss 0.30190419079735875\n",
      "0.9672964169381106 0.9180107526881719 0.9668780831571528\n",
      "End of epoch loss 0.2798076926264912\n",
      "0.9663192182410423 0.9213709677419354 0.9677589852008457\n",
      "End of epoch loss 0.2847747174091637\n",
      "0.9669136807817589 0.9227150537634409 0.9656448202959831\n",
      "End of epoch loss 0.27368956967256963\n",
      "0.966628664495114 0.9193548387096774 0.9628259337561662\n",
      "End of epoch loss 0.2771440850337967\n",
      "0.9668729641693811 0.9173387096774193 0.9644115574348132\n",
      "End of epoch loss 0.2798034093575552\n",
      "0.9664169381107491 0.9166666666666667 0.9646758280479211\n",
      "End of epoch loss 0.2884639063850045\n",
      "0.9674267100977199 0.918010752688172 0.9639711064129668\n",
      "End of epoch loss 0.25453085207846016\n",
      "0.9688925081433224 0.9173387096774194 0.9642353770260748\n",
      "End of epoch loss 0.26301108056213707\n",
      "0.9684201954397395 0.92002688172043 0.9630902043692742\n",
      "End of epoch loss 0.24467215512413532\n",
      "0.9683387622149837 0.9206989247311828 0.9644115574348133\n",
      "End of epoch loss 0.2343636435107328\n",
      "0.968485342019544 0.9186827956989247 0.9652043692741368\n",
      "End of epoch loss 0.24273746402468532\n",
      "0.9686644951140065 0.9166666666666666 0.9634425651867512\n",
      "End of epoch loss 0.24712586257373914\n",
      "0.969543973941368 0.9200268817204301 0.9616807610993657\n",
      "End of epoch loss 0.26323318551294506\n",
      "0.9696254071661238 0.9206989247311828 0.9631782945736433\n",
      "End of epoch loss 0.2377368665765971\n",
      "0.9693322475570032 0.9213709677419354 0.9620331219168428\n",
      "End of epoch loss 0.2459151460789144\n",
      "0.9707654723127036 0.9213709677419355 0.9603594080338267\n",
      "End of epoch loss 0.236476831138134\n",
      "0.9707980456026059 0.9206989247311826 0.9589499647639183\n",
      "using learning rate [2e-05, 2e-05]\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "End of epoch loss 0.25636508234310895\n",
      "0.9765472312703584 0.9112903225806451 0.9575405214940097\n",
      "End of epoch loss 0.2599860389600508\n",
      "0.9814006514657981 0.9314516129032259 0.9466173361522199\n",
      "End of epoch loss 0.25785030715633184\n",
      "0.9718078175895766 0.9435483870967741 0.9491719520789288\n",
      "End of epoch loss 0.2540879736188799\n",
      "0.9714983713355049 0.9381720430107526 0.9226568005637774\n",
      "End of epoch loss 0.3275343648856506\n",
      "0.9574918566775243 0.8991935483870968 0.9018675123326284\n",
      "End of epoch loss 0.38735817710403353\n",
      "0.9768241042345277 0.918010752688172 0.934945384073291\n",
      "End of epoch loss 0.22669225942809135\n",
      "0.9794951140065147 0.9173387096774194 0.9341966173361522\n",
      "End of epoch loss 0.20157584198750556\n",
      "0.9861563517915308 0.9270833333333333 0.9288231148696265\n",
      "End of epoch loss 0.18213305273093283\n",
      "0.9881107491856679 0.9260752688172043 0.930584918957012\n",
      "End of epoch loss 0.16995233373017982\n",
      "0.9903094462540717 0.898521505376344 0.911777660324172\n",
      "End of epoch loss 0.15959388361079618\n",
      "0.9897964169381108 0.8971774193548386 0.9101479915433404\n",
      "End of epoch loss 0.14277913339901716\n",
      "0.9913355048859934 0.9025537634408602 0.9441948555320648\n",
      "End of epoch loss 0.1413665729924105\n",
      "0.9899267100977199 0.8951612903225806 0.9261804087385483\n",
      "End of epoch loss 0.15071590605657548\n",
      "0.9895358306188926 0.9025537634408602 0.8703312191684285\n",
      "End of epoch loss 0.15753321815282106\n",
      "0.9902605863192183 0.898521505376344 0.92961592670895\n",
      "End of epoch loss 0.14448119309963658\n",
      "0.9904641693811075 0.9018817204301075 0.9104122621564481\n",
      "End of epoch loss 0.13192269223509356\n",
      "0.9909120521172639 0.9055779569892473 0.9144644115574349\n",
      "End of epoch loss 0.13801412098109722\n",
      "0.99121335504886 0.8948252688172044 0.9036293164200141\n",
      "End of epoch loss 0.17931604839395732\n",
      "0.947614006514658 0.9200268817204301 0.8499823819591261\n",
      "End of epoch loss 0.16057688725413755\n",
      "0.9901547231270359 0.9139784946236559 0.8912966878083157\n",
      "End of epoch loss 0.143236854695715\n",
      "0.9913192182410423 0.9049059139784946 0.9236257928118392\n",
      "End of epoch loss 0.13612735687638633\n",
      "0.9814169381107491 0.9227150537634409 0.8809901338971106\n",
      "End of epoch loss 0.18850879833917134\n",
      "0.9905618892508143 0.9059139784946235 0.8888742071881605\n",
      "End of epoch loss 0.12834133493015543\n",
      "0.9905048859934853 0.9075940860215054 0.890283650458069\n",
      "End of epoch loss 0.13354791287565604\n",
      "0.991970684039088 0.9139784946236559 0.8677766032417196\n",
      "[601 657 226 548  99 263 245 156 321 231 217 762 536  85 614  86 510 177\n",
      " 713 347 476 475 110 278 185 734 175 392 164 291 438 396 611 656 352 132\n",
      "  30 760 700 498 596 469 201 252 684 393 372 771 654   7 188 500 103 293\n",
      " 483 115 722 683 472  79 209 680 649 525 565 135 170 668 169 303 373 559\n",
      " 775 121 795 433 410 158  94]\n",
      "[168 157 466 562  48 314 543 375 315 493 200 136 480 484 796 573 111 107\n",
      " 391 528 747 105  52 462 196 153 726 613 377 394  78  64 215 343 732 342\n",
      " 354 266  59   3 243  10 558 202  21 752 701 779 631 186   2 549 674  89\n",
      " 598  65  62 337  33 143 479 151 269  43 738 383 413 801  92 251 276   5\n",
      " 608  13 123 514 595 350 454 735 239 640 253 312 290 746 465 509 470 431\n",
      " 602 258 134 244 793 397 574 227 108 478  97 241 260  63 692 610 774 791\n",
      " 300 271 122 607 176 471 223 759 327 691 693 450 647  80  27 634 583 184\n",
      " 473 369 570 178 696 630 661  37 802 667 180 555 506 304 118 612 616 567\n",
      " 501 141 240 790 748 642 358 133 163 789  55 444 161 380 452 221 777 334\n",
      " 238  61 497 427 183 702  49 203 401  44 679 274 769 302 688 623 434 406\n",
      "  20 463  69  72 142  67 781 366 357 159  66 418 579 297 281 150 299 477\n",
      " 447 275 524 402 588  18 247  68 742  22 403 515 620 228 440 749 294 767\n",
      " 205 504]\n",
      "[482 517 386  14   6 627 126 237 187 120 341 628 194 539 207 124 214 398\n",
      " 441 645 727 736 778 541 782 127 755 167 675 652 460 665 145 706 624 265\n",
      " 206 695 800 109 599 486 505 272 530 659 773  88 750 208 468 404 737 219\n",
      " 442 678 753 329 280 650 617 489 792  74 311 643 419 626 556 531 544 305\n",
      " 651 458 140 147 248 405 426  60 385 371 155 408 676 420  28 432 723 589\n",
      "  26 569  53 619 319  16 754 270 532 499 213 709 745  83 551 600  54 119\n",
      " 416 267 339 317 353 739 324 364 743 138 445 137 537 306 572 130 591 481\n",
      " 320 797 165 154 459 529 545 708 561  40 534 224 172 328 733 395 335 699\n",
      " 424 461 212 417 744 152  36 250 355 662 345 794 388 518 437 318 286 474\n",
      "  38   1  45 731  77 633 730 758 166 629 492 457 384 741 216 113 632 330\n",
      " 615  46 128 338 712 368 508 310 547 139 740 761 553 487 488 171 511 538\n",
      " 309 316 360 597 389  25 646  15 689 550 407 323 378 670 349 256  17 273\n",
      " 399 198 112 411 436 382 322  35 308 257 490 331 729 261 540 129 268 131\n",
      " 786 716   0 672 435 653 220 376 557 655 669 344  19 336 148 724 236 412\n",
      "  73 694 414 685 374 686  29 340 533 279 415 705 697 288 571 718 430 582\n",
      " 590 772 527 785 425 423  34 682 503 204 174  91 287 264 455 446 277 552\n",
      " 757  50 719 234 512 421 546 639 603 229 104  90 564 125 361 637 576 560\n",
      " 494 390 381 193 664 429  81 711 282 641 776 584 690 255 766 189 787 644\n",
      " 449 714 117  41  76 439 485 332 784 728 362  96   9 295 580 313  23 351\n",
      " 586 663 618 210 359 190 173 182 677 292  12 346 625  47 770 578 254 592\n",
      " 703 577 542  57  56 451 526  84 681 783 409 262 707 301  39 259 365 191\n",
      " 491 307 467 379 666 788 636 456 502 242 587 721  58 605 100 453 751 522\n",
      "  11 179 246 635 230 803 671 799 326 496 704 162 233  75 507  71 648 604\n",
      " 363   4 698 325 146 658 622 225 235 400 102 333 521 763 519  98 106 594\n",
      " 289 367 710 181 585 720 211 566 581 687 621   8 348  82 160 717 116 370\n",
      " 218 798 249 428 464  42 197 296 780 387 764 285 638 606  32 513 448  51\n",
      " 356 575 149 593 715  31  87 535 101 192 660 563 443 298 232 144 516 725\n",
      " 609  93  70 520 284 199 195 422 554 495 283 768 568  24  95 114 765 523\n",
      " 756 673 222]\n",
      "0.44874592833876226 0.38239247311827956 0.5040521494009866\n",
      "using learning rate [0.0001, 0]\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0\n",
      "End of epoch loss 0.3463183694984764\n",
      "0.9752442996742672 0.9395161290322581 0.9569238900634249\n",
      "End of epoch loss 0.28355705295689404\n",
      "0.9784039087947882 0.9509408602150536 0.9596546863988724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch loss 0.25925662624649704\n",
      "0.9699674267100977 0.9408602150537635 0.9567036645525018\n",
      "End of epoch loss 0.24512271815910935\n",
      "0.969299674267101 0.9435483870967742 0.9556465821000706\n",
      "End of epoch loss 0.24010701361112297\n",
      "0.9783550488599349 0.9448924731182795 0.957892882311487\n",
      "End of epoch loss 0.23089396837167442\n",
      "0.9769218241042344 0.9411962365591398 0.9556906272022552\n",
      "End of epoch loss 0.2344013131223619\n",
      "0.9794136807817589 0.9583333333333334 0.9585095137420718\n",
      "End of epoch loss 0.2359592256252654\n",
      "0.9810097719869706 0.9405241935483871 0.9547656800563777\n",
      "End of epoch loss 0.22933859814656898\n",
      "0.9819218241042345 0.9536290322580645 0.9581131078224102\n",
      "End of epoch loss 0.2281720244209282\n",
      "0.9844381107491856 0.9462365591397849 0.9521670190274842\n",
      "End of epoch loss 0.23428590642288327\n",
      "0.9835342019543974 0.9455645161290323 0.9541050035236083\n",
      "End of epoch loss 0.2543717559892684\n",
      "0.9774104234527687 0.9516129032258064 0.9596546863988724\n",
      "End of epoch loss 0.2138609791873023\n",
      "0.9818403908794788 0.9415322580645161 0.9574524312896405\n",
      "End of epoch loss 0.21409235161263496\n",
      "0.9801465798045603 0.9455645161290321 0.9571000704721635\n",
      "End of epoch loss 0.2384294252260588\n",
      "0.9794869706840391 0.9435483870967741 0.9596106412966878\n",
      "End of epoch loss 0.2307754712528549\n",
      "0.9813192182410423 0.9469086021505376 0.9579809725158562\n",
      "End of epoch loss 0.2101140640443191\n",
      "0.9811889250814332 0.9415322580645161 0.953884778012685\n",
      "End of epoch loss 0.217311481887009\n",
      "0.984755700325733 0.9388440860215054 0.9523872445384072\n",
      "End of epoch loss 0.22319938999135047\n",
      "0.9850651465798045 0.956989247311828 0.9567036645525017\n",
      "End of epoch loss 0.20393611438339576\n",
      "0.9833957654723128 0.948252688172043 0.9581131078224101\n",
      "End of epoch loss 0.20760033011902124\n",
      "0.9807573289902279 0.9422043010752688 0.9596106412966877\n",
      "End of epoch loss 0.20543932769214734\n",
      "0.979698697068404 0.9381720430107526 0.9568798449612403\n",
      "End of epoch loss 0.2179832558031194\n",
      "0.9832980456026059 0.9314516129032258 0.9555144467935165\n",
      "End of epoch loss 0.21853406715672463\n",
      "0.9798534201954398 0.9368279569892473 0.9589499647639183\n",
      "End of epoch loss 0.207990025926847\n",
      "0.9825977198697068 0.9368279569892474 0.9585095137420718\n",
      "using learning rate [2e-05, 2e-05]\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "2e-05\n",
      "End of epoch loss 0.2097791470005177\n",
      "0.9828501628664496 0.965725806451613 0.9619450317124736\n",
      "End of epoch loss 0.22325346118304878\n",
      "0.9870602605863192 0.9663978494623655 0.9604034531360113\n",
      "End of epoch loss 0.2041741549037397\n",
      "0.9886400651465798 0.9556451612903226 0.962869978858351\n",
      "End of epoch loss 0.2906863939133473\n",
      "0.9852117263843648 0.9546370967741936 0.9586416490486258\n",
      "End of epoch loss 0.18458034069044515\n",
      "0.9904234527687296 0.9583333333333334 0.9613284002818888\n",
      "End of epoch loss 0.2676498044747859\n",
      "0.9236889250814332 0.8776881720430108 0.8926180408738549\n",
      "End of epoch loss 0.3090562470606528\n",
      "0.9783306188925082 0.9348118279569891 0.9396582100070472\n",
      "End of epoch loss 0.2204197938553989\n",
      "0.987956026058632 0.9469086021505376 0.9575405214940098\n",
      "End of epoch loss 0.20415055446210317\n",
      "0.9894788273615636 0.9368279569892473 0.9536645525017619\n",
      "End of epoch loss 0.7007222628453746\n",
      "0.9750162866449512 0.8575268817204301 0.94053911205074\n",
      "End of epoch loss 0.3156555782770738\n",
      "0.9778501628664494 0.853494623655914 0.9395701198026779\n",
      "End of epoch loss 0.17711776358191855\n",
      "0.9880944625407168 0.8776881720430108 0.9179439746300211\n",
      "End of epoch loss 1.4958716020919383\n",
      "0.35510586319218235 0.39314516129032256 0.3790521494009866\n",
      "End of epoch loss 1.3641242871526629\n",
      "0.49359120521172634 0.4825268817204301 0.48493657505285415\n",
      "End of epoch loss 0.6712393001653254\n",
      "0.8269218241042344 0.6834677419354838 0.797568710359408\n",
      "End of epoch loss 0.5599653859389946\n",
      "0.8436482084690554 0.7681451612903225 0.7107117688513037\n",
      "End of epoch loss 0.4087981746997684\n",
      "0.9413029315960912 0.8487903225806451 0.8886099365750528\n",
      "End of epoch loss 0.32608426723163575\n",
      "0.9598371335504886 0.8978494623655914 0.9265327695560254\n",
      "End of epoch loss 0.2957456229487434\n",
      "0.9741042345276875 0.8924731182795699 0.9408914728682171\n",
      "End of epoch loss 0.26770072273211554\n",
      "0.9651465798045602 0.8931451612903225 0.9373678646934461\n",
      "End of epoch loss 0.25092027650680393\n",
      "0.9709446254071661 0.9489247311827956 0.9229210711768853\n",
      "End of epoch loss 0.2648998327786103\n",
      "0.9783061889250814 0.9422043010752688 0.9230091613812544\n",
      "End of epoch loss 0.2419069900061004\n",
      "0.9798045602605863 0.9334677419354839 0.9217758985200845\n",
      "End of epoch loss 0.23424622061429545\n",
      "0.9805211726384365 0.9301075268817204 0.9325669485553207\n",
      "End of epoch loss 0.2508867966243997\n",
      "0.9881596091205211 0.9596774193548386 0.9380725863284003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_ensemble_scores = []\n",
    "val_ensemble_scores = []\n",
    "scores_for_roc = []\n",
    "val_cutoffs = []\n",
    "\n",
    "learning_rate = 0.0001;\n",
    "hidden_list = [1024, 128, 8]\n",
    "# hidden_list = [100]\n",
    "\n",
    "test_roc_list = [];\n",
    "test_loss_list = [];\n",
    "\n",
    "train_loss_info = [];\n",
    "train_roc_info = [];\n",
    "\n",
    "valid_loss_info = [];\n",
    "valid_roc_info = [];\n",
    "\n",
    "test_loss_info = [];\n",
    "test_roc_info = [];\n",
    "\n",
    "val_plots = []\n",
    "test_plots = []\n",
    "train_plots = []\n",
    "phase_brk = -0.5\n",
    "\n",
    "\n",
    "for run in range(5):\n",
    "    test_indices = []\n",
    "    test_labels = []\n",
    "    val_indices = []\n",
    "    val_labels = []\n",
    "    test_seed = np.random.randint(100) #1029384756\n",
    "    train_indices, valid_indices, test_indices = get_splits(labels, run, 0.7, test_seed)\n",
    "    print(test_indices)\n",
    "    print(valid_indices)\n",
    "    print(train_indices)\n",
    "    # Declare the model. We'll use the shallow CBoW classifier or the one that has one hidden layer.\n",
    "    model = BertClassifier(n_classes=2, bert=bert_model, emb_dim=768, hidden_list=hidden_list, dropout=0.5)   \n",
    "\n",
    "    val_plot = []\n",
    "    test_plot = []\n",
    "    train_plot = []\n",
    "\n",
    "    # Put the model on the device.\n",
    "    model.to(device)\n",
    "    class_weight = torch.FloatTensor([np.count_nonzero(np.array(labels) == _)+0.0 for _ in range(2)]).to(device)\n",
    "    class_weight = (torch.max(class_weight)/class_weight)**1.0\n",
    "\n",
    "    # Cross-entropy loss as usual, since we have a classification problem.\n",
    "    loss_function = torch.nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "    # Adam optimizer. We can try to tune the learning rate to get a fast convergence while avoiding instability.\n",
    "    #  optimizer = torch.optim.Adam([{'params' :  model.top_layer.parameters(), 'lr' : lr1}, {'params' :  model.hidden_layers.parameters(), 'lr' : lr2}, {'params' :  model.batch_norm.parameters(), 'lr' : lr3}, {'params' :  model.scibert.parameters(), 'lr' : lr4}])\n",
    "\n",
    "\n",
    "    # We'll keep track of some indicators and plot them in the end.\n",
    "    history = defaultdict(list)\n",
    " \n",
    "    lrs = [[0.0001,0], [2e-5,2e-5]]\n",
    "    repeats = [1,1]\n",
    "    epochs = 25\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{'params' :  model.top_layer.parameters(), 'lr' : 0}, {'params' :  model.hidden_layers.parameters(), 'lr' : 0}, {'params' :  model.batch_norm.parameters(), 'lr' : 0}, {'params' :  model.scibert.parameters(), 'lr' : 0}])\n",
    "\n",
    "    for lr_i in range(len(lrs)):\n",
    "        lr = lrs[lr_i]\n",
    "        if lr_i == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_scores = [];\n",
    "                val_labels = [];\n",
    "                for batch in valid_indices:\n",
    "                    (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels) = get_batched_token(batch, device);\n",
    "                    scores = model(batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context)\n",
    "                    val_scores.extend(F.softmax(scores, dim=1).detach().cpu().numpy().tolist())\n",
    "                    val_labels.extend(batched_labels.cpu().numpy().tolist())\n",
    "                val_loss = loss_function(torch.FloatTensor(val_scores).to(device), torch.LongTensor(val_labels).to(device)).item()\n",
    "                val_macro_roc = roc_auc_score(np.array(val_labels), np.array(val_scores)[:,1])\n",
    "                val_plot.append(val_macro_roc)\n",
    "\n",
    "                train_scores = [];\n",
    "                train_labels = [];\n",
    "                for batch in train_indices:\n",
    "                    (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels) = get_batched_token(batch, device);\n",
    "                    scores = model(batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context)\n",
    "                    train_scores.extend(F.softmax(scores, dim=1).detach().cpu().numpy().tolist())\n",
    "                    train_labels.extend(batched_labels.cpu().numpy().tolist())\n",
    "                train_loss = loss_function(torch.FloatTensor(train_scores).to(device), torch.LongTensor(train_labels).to(device)).item()\n",
    "                train_macro_roc = roc_auc_score(np.array(train_labels), np.array(train_scores)[:,1])\n",
    "                train_plot.append(train_macro_roc)\n",
    "\n",
    "\n",
    "                test_scores = [];\n",
    "                test_labels = [];\n",
    "                for batch in test_indices:\n",
    "                    (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels) = get_batched_token(batch, device);\n",
    "                    scores = model(batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context)\n",
    "                    test_scores.extend(F.softmax(scores, dim=1).detach().cpu().numpy().tolist())\n",
    "                    test_labels.extend(batched_labels.cpu().numpy().tolist())\n",
    "                test_loss = loss_function(torch.FloatTensor(test_scores).to(device), torch.LongTensor(test_labels).to(device)).item()\n",
    "                test_macro_roc = roc_auc_score(np.array(test_labels), np.array(test_scores)[:,1])\n",
    "                test_plot.append(test_macro_roc)\n",
    "\n",
    "                print(train_macro_roc, test_macro_roc, val_macro_roc)\n",
    "\n",
    "\n",
    "        print(\"using learning rate\", lr)\n",
    "        \n",
    "#       optimizer = torch.optim.Adam([{'params' :  model.top_layer.parameters(), 'lr' : lr[0]}, {'params' :  model.hidden_layers.parameters(), 'lr' : lr[0]}, {'params' :  model.batch_norm.parameters(), 'lr' : lr[0]}, {'params' :  model.scibert.parameters(), 'lr' : lr[1]}])\n",
    "    \n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr[0]\n",
    "    \n",
    "        optimizer.param_groups[-1]['lr'] = lr[1]\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=verbose, save_path='/export/scratch/trash/bert_finetuning_checkpoint_{}.pt'.format(save_postfix))\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            print(g['lr'])\n",
    "\n",
    "        \n",
    "        for i in range(epochs):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            # Calling model.train() will enable the dropout layers.\n",
    "            model.train()\n",
    "            # We iterate through the batches created by torchtext.\n",
    "            # For each batch, we can access the text part and the output label part separately.\n",
    "\n",
    "            train_scores = [];\n",
    "            train_labels = [];\n",
    "\n",
    "            np.random.shuffle(train_indices)\n",
    "            for d in range(repeats[lr_i]):\n",
    "                for offset in range(0,len(train_indices),batch_size):\n",
    "                    loss = 0\n",
    "                    bs = 0\n",
    "                    for idx in range(batch_size):\n",
    "                        b = offset+idx\n",
    "                        if(b < len(train_indices)):\n",
    "                            # Compute the output scores.\n",
    "                            if d == 0:\n",
    "                                (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels) = get_batched_token(train_indices[b], device);\n",
    "                            else:    \n",
    "                                (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels) = get_batched_token(train_indices[b], device, p= 1.0);\n",
    "                            scores = model(batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context)\n",
    "                            # Then the loss function.\n",
    "                            loss += loss_function(scores, batched_labels)\n",
    "                            bs += 1\n",
    "\n",
    "                    loss = loss / (bs)\n",
    "\n",
    "                    # Compute the gradient with respect to the loss, and update the parameters of the model.\n",
    "                    optimizer.zero_grad()            \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    loss_sum += loss.item()\n",
    "                    n_batches += 1\n",
    "\n",
    "                    train_scores.extend(F.softmax(scores, dim=1).detach().cpu().numpy().tolist())\n",
    "                    train_labels.extend(batched_labels.cpu().numpy().tolist())\n",
    "                    \n",
    "#             train_pred_labs = [(round(s[1])) for s in train_scores]       \n",
    "#             print(train_pred_labs)\n",
    "            train_loss = loss_sum / n_batches\n",
    "            print(\"End of epoch loss\", train_loss)\n",
    "            train_macro_roc = roc_auc_score(np.array(train_labels), np.array(train_scores)[:,1])\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_macro_roc'].append(train_loss)\n",
    "\n",
    "            # After each training epoch, we'll compute the loss and accuracy on the validation set.\n",
    "            n_correct = 0\n",
    "            n_valid = len(valid_indices)\n",
    "            loss_sum = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            # Calling model.eval() will disable the dropout layers.\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_scores = [];\n",
    "                val_labels = [];\n",
    "                for batch in valid_indices:\n",
    "                    (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels) = get_batched_token(batch, device);\n",
    "                    scores = model(batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context)\n",
    "                    val_scores.extend(F.softmax(scores, dim=1).detach().cpu().numpy().tolist())\n",
    "                    val_labels.extend(batched_labels.cpu().numpy().tolist())\n",
    "                val_loss = loss_function(torch.FloatTensor(val_scores).to(device), torch.LongTensor(val_labels).to(device)).item()\n",
    "                val_macro_roc = roc_auc_score(np.array(val_labels), np.array(val_scores)[:,1])\n",
    "                val_plot.append(val_macro_roc)\n",
    "\n",
    "                train_scores = [];\n",
    "                train_labels = [];\n",
    "                for batch in train_indices:\n",
    "                    (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels) = get_batched_token(batch, device);\n",
    "                    scores = model(batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context)\n",
    "                    train_scores.extend(F.softmax(scores, dim=1).detach().cpu().numpy().tolist())\n",
    "                    train_labels.extend(batched_labels.cpu().numpy().tolist())\n",
    "                train_loss = loss_function(torch.FloatTensor(train_scores).to(device), torch.LongTensor(train_labels).to(device)).item()\n",
    "                train_macro_roc = roc_auc_score(np.array(train_labels), np.array(train_scores)[:,1])\n",
    "                train_plot.append(train_macro_roc)\n",
    "\n",
    "\n",
    "                test_scores = [];\n",
    "                test_labels = [];\n",
    "                for batch in test_indices:\n",
    "                    (batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context, batched_labels) = get_batched_token(batch, device);\n",
    "                    scores = model(batched_cited_title, batched_cited_abstract, batched_citing_title, batched_citing_abstract, batched_citation_context)\n",
    "                    test_scores.extend(F.softmax(scores, dim=1).detach().cpu().numpy().tolist())\n",
    "                    test_labels.extend(batched_labels.cpu().numpy().tolist())\n",
    "                test_loss = loss_function(torch.FloatTensor(test_scores).to(device), torch.LongTensor(test_labels).to(device)).item()\n",
    "                test_macro_roc = roc_auc_score(np.array(test_labels), np.array(test_scores)[:,1])\n",
    "                test_plot.append(test_macro_roc)\n",
    "                print(train_macro_roc, test_macro_roc, val_macro_roc)\n",
    "\n",
    "#             if i > 0:\n",
    "#                 early_stopping(-val_macro_roc, model)\n",
    "#                 if early_stopping.early_stop:\n",
    "#                     if verbose:\n",
    "#                         print('Early stopping!')\n",
    "#                     break\n",
    "    val_plots.append(val_plot) \n",
    "    test_plots.append(test_plot) \n",
    "    train_plots.append(train_plot) \n",
    "# torch.save(model.state_dict(), \"/export/scratch/petros/test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '1x training size with deletion rate 7.5% and CLS')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "\n",
    "# plt.vlines(25,0,1)\n",
    "plt.errorbar(x=range(len(test_plots[0])), y=np.mean(np.array(val_plots), axis=0)   , yerr=np.std(np.array(val_plots),axis=0) , label='validation')\n",
    "plt.errorbar(x=range(len(test_plots[0])), y=np.mean(np.array(train_plots), axis=0), yerr=np.std(np.array(train_plots),axis=0) , label='train')\n",
    "plt.errorbar(x=range(len(test_plots[0])), y=np.mean(np.array(test_plots), axis=0), yerr=np.std(np.array(test_plots),axis=0) , label='test')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "# ax.set_xticks(np.arange(0, 1, 0.))\n",
    "ax.set_yticks(np.arange(0, 1., 0.05))\n",
    "\n",
    "plt.grid()\n",
    "plt.title(\"1x training size with deletion rate 7.5% and CLS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axes = plt.gca()\n",
    "# axes.set_ylim([0,1])\n",
    "\n",
    "# plt.vlines(phase_brk,0,1)\n",
    "# plt.plot(val_plot, label='validation')\n",
    "# plt.plot(train_plot,label='train')\n",
    "# plt.plot(test_plot,label='test')\n",
    "# plt.legend()\n",
    "# print(len(val_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"/export/scratch/petros/test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = BertClassifier(n_classes=2, bert=bert_model, emb_dim=768, hidden_list=hidden_list, dropout=0.5)   \n",
    "# model2.load_state_dict(torch.load('/export/scratch/petros/test.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
